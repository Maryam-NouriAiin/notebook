# Homeworks {-}

## Homework #1 {-}

### The workflow for Bio 381 {-}

The goal of this first exercise is to get comfortable with the workflow of this course:

* Build a project in Rstudio
* Link your project to a repository that is stored at Github
* Build a webpage `index.html` that is associated with this repository
* Add text, links, and web pages to your repository
* Frequently commit your changes and sync your repo with GitHub
* If your code stops working and you can't repair it, know how to revert changes back to the most recent commit

The detailed instructions for these activities can be found in the notes from [Lecture #1](https://gotellilab.github.io/Bio381/Lectures/Lecture01.html). To get used to the workflow, try the following sequence, creating a project directory on your desktop 

### Exercises and practice for getting comfortable with the workflow  {-}
1. Create a project in R studio, and set it up in GitHub with just the index page. Were you able to publish the page? If so, delete the repository on GitHub and delete the local project on your desktop and try it a few more times.

2. Now continue by adding text, and links to your index page. Each page you add should have a commit to store it. Again, when this all works for you, delete the repository and the local project and practice it again.

3. Now that you are getting good at this, try adding a new file and making changes to your index file. Commit those changes and then go to the latest version and use the revert button. Your new file should disappear and the repo should be reset to the previous stage. Again, destroy the repo and the local copy and repeat until you are comfortable with this sequence.

### Building your website for your portfolio  {-}

When steps 1 - 3 are very comfortable, it is time to make the repository you will keep and use to build your portfolio of materials for this class. 

* If you use DropBox, set up your Rstudio Project folder inside DropBox. If you do not use DropBox, just put it on your desktop or somewhere else that is convenient^[Using GitHub means you never have to worry about losing your files, even without DropBox. If you ever delete or corrupt the local repository, just throw it away and clone a fresh copy of the repo from GitHub!].

* Name your RStudio project `LastnameBio381`, with no spaces or other characters. For example, if I was building this project for myself, the name would be `Bio381Gotelli`
* All you need to do is to set up your index page with the text "Index page successfully built". We will change that later

### What to turn in  {-}
 At the end of the day, email to me (ngotelli@uvm.edu) the following information 
 
* The full website address for your GitHub repository
* The full website address for the homepage of your Bio381 repository
* A one-sentence "fun fact"-- something memorable about you that everyone will always remember when they see you again. Perhaps a famous person you have met, a notable accomplishment, something interesting from your past...
    
Once I have all of this information, I will create a webpage that has links to all of these pages for the entire class. That means you can study the work of others as you do your own coding.    


## Homework #2 {-}

1. Take a broad, self-critical look at your entire thesis/honors project. What are the big questions you are asking, and what are the specific hypotheses and mechanisms that your experiments and sampling are trying to address? How will you interpret the different possible results from your activities? Be explicit and detailed; do not hide behind jargon or vague statements.  

2. Translate your answers to these questions into any 2 of the 4 diagrams we discussed in class: a) cause-and-effect diagram; b) pre-graph; c) DIPSwitch table c) logic tree. You may have to try several versions to get a satisfying result.

3. Lauren will illustrate a simple programmming language embedded within Typora that will allow you to create a flowchart diagram of your figure. Alternatively, you could also make a neat pen and ink drawing of your figure, photograph it with your iphone, and send it to your computer.

4. After an hour or devoted to working on these diagrams, Lauren will ask you to volunteer to bring your diagram up to the overhead projector. Show it to the class, and explain the mechanisms and links you are working with. The class should ask questions and give you feedback for improving the figure.

5. At your index page, build a link to a page called `Homework02`.

6. Create `Homework02.Rmd` as a new Markdown file.

7. Insert each of the images into the file and compile as an `.html`.

8. Use GitHub to make frequent commits, and sync your repo to update your webpages.

9. Add Markdown titles, formatting, and text, and develop a narrative to explain each figure, the hypotheses they elaborate, and the expected outcomes of your experiments and observations. If you hit a wall in developing your hypotheses, try to write some text explaining what the issues are. 

10. As your project progresses, you may want to return to this webpage to update (and hopefully improve!) your diagrams and interpretations.

### Building a Flowchart in Typora {-}

1. In Typora, go to `preferences` (under the `file` or `edit` pull-down menu) and check the `Diagrams` option.

2. In Typora help, go to `More Topics...` and `Draw Diagrams With Markdown`

3. Scroll down to `Flowchart` and copy the code snippet into a fresh Typora file.

4. Click outside the box and you should see this code rendered as a nice flow chart.

5. Now study the code to understand how it works, and then modify it to recreate your flow chart or logic tree diagram from last week. 

6. Add some text and pick a nice theme, and then render the Typora `.md` file to `.html` and push the changes to your repo.

## Homeowrk #3 {-}

#### Creating a Beamer Presentation {-}

1. In this lab, you will learn how to create a set of slides in a pdf file that will look much more professional than a typical powerpoint presentation. 

2. Following the notes at the bottom of this page, create a short `Beamer` presentation (6 to 12 slides). Note that you will need to save your images to `.tiff`. or `.jpeg` or other digital formats and put them in a folder that can be linked to from your Beamer code.

2. Spend a bit of time exploring the different themes, colors, and fonts available in Beamer, and set up something that looks nice to you.

3. Your presentation should include the following:
- sequential bullet points
- simultaneous bullet points
- colored text
- images
- images with captions
- images with a caption overlaid (you may need to change the text color for better contrast)

4. The only real challenge will be to to get your images and captions properly sized and placed in the Bamer slides. I have posted some code for you to use on the [R Scripts](https://gotellilab.github.io/Bio381/Scripts/RScripts.html) page. 

Modify this code to adjust the size of the image, its placement on the page, and the size and spacing of the legend. You will have to carefully experiment with the default numerical settings to understand what each of them do. Make some good notes in your markdown file so you will have a reference handy when you want to create slides with images that are properly sized and aligned.



#### Beamer Slides - A Preview {-}
- Better fonts and styles
- Avoids corporate odor of Microsoft
- Frequency dependent selection
- Cachet of "hard science" LaTeX users (computer science, physics, chemistry, stats)
- Completely stable and reliable appearance from pdf
- No need to bring your own computer

#### Creating Basic Beamer Slides {-}
- always show bullets incrementally
- choose among styles, colors, and fonts
- show `Madrid`, `beaver`, `bold` on fonts
- use two `##` to denote a new slide
- use `{r, echo=FALSE}` to just show your graphs

#### Demonstrate Colors And Bullets {-}
- Use `> - ` to override the incremental bullets
- Use `\color{gray}` to color a bullet point
- Illustrate building of red/gray bullet points in a list

#### Demonstrate Adjustment of Text Placement {-}
- Illustrate use of `\bigskip` to adjust spacing of words

#### Dealing with images {-}
- must be placed in folder with .Rmd file
- use image code that is posted 
- adjust settings for image size, placement, and caption

## Homework #4 {-}

Some of these problems are adapted from:

> Jones, O., R. Maillardet, and A. Robinson. 2009. Scientific Programming and Simulation Using R. CRC Press, Boca Raton.

1. Suppose *x* = 1.1, *a* = 2.2, and *b* = 3.3. Assign each expression to the value of the variable *z*and print the value stored in *z*.

    a) $x^{a^b}$
    b) $(x^{a})^b$
    c) $3x^3 + 2x^2 + 1$

2. Using the `rep` and `seq` functions, create the following vectors:

    a) $(1,2,3,4,5,6,7,8,7,6,5,4,3,2,1)$
    b) $(1,2,2,3,3,3,4,4,4,4,5,5,5,5,5)$
    c) $(5,4,4,3,3,3,2,2,2,2,1,1,1,1,1)$
    
3. Create a vector of two random uniform numbers. In a spatial map, these can be interpreted as *x* and *y* coordinates that give the location of an individual (such as a marked forest tree in a plot that has been mapped). Using one of R's inverse trigonometry functions (`asin()`, `acos()`, or `atan()`), convert these numbers into polar coordinates (If you don't know what polar coordinates are, read about them on the web or in your calculus textbook).

4. Suppose that `queue <- c("sheep", "fox", "owl", "ant")` and that `queue` represents the animals that are lined up to enter Noah's Ark, with the sheep at the front of the line. Using R expressions, update the queue successively as 

    a) the serpent arrives;
    b) the sheep enters the ark;
    c) the donkey arrives and talks his way to the front of the line;
    d) the serpent gets impatient and leaves;
    e) the owl gets bored and leaves;
    f) the aphid arrives and the ant invites him to cut in line.
    g) Finally, determine the position of the aphid in the line.

5. Use R to create a vector of all of the integers from 1 to 100 that are not divisible by 2, 3, or 7.


## Homeowrk #5 {-}

#### Data manipulations using the `dplyr` package  {-}

This homework assignment focus on data manipulation in `R`. Complete these problems using the `dplyr` package, and if you have time, try each of these exercises using the subsetting methods in base `R` that have been covered in lecture. 

1. Examine the structure of the iris dataset. How many observations and variables are in the dataset?   

2. Create a new data frame `iris1` that contains only the species *virginica* and *versicolor* with sepal lengths longer than 6 cm and sepal widths longer than 2.5 cm. How many observations and variables are in the dataset?      

3. Now, create a `iris2` data frame from `iris1` that contains only the columns for Species, Sepal.Length, and Sepal.Width. How many observations and variables are in the dataset?   

4. Create an `iris3` data frame from `iris2` that orders the observations from largest to smallest sepal length. Show the first 6 rows of this dataset.    

5. Create an `iris4` data frame from `iris3` that creates a column with a sepal area (length * width) value for each observation. How many observations and variables are in the dataset?

6. Create `iris5` that calculates the average sepal length, the average sepal width, and the sample size of the entire `iris4` data frame and print `iris5`.

7. Finally, create `iris6` that calculates the average sepal length, the average sepal width, and the sample size *for each species* of in the `iris4` data frame and print `iris6`.

8. In these exercises, you have successively modified different versions of the data frame `iris1 iris1 iris3 iris4 iris5 iris6`. At each stage, the output data frame from one operation serves as the input fro the next. 

A more efficient way to do this is to use the pipe operator `%>%` from the `tidyr` package. See if you can rework all of your previous statements into an extended piping operation that uses `iris` as the input and generates `iris6` as the output.

## Homework #6 {-}
 
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, message=FALSE, warning=FALSE)
```

### Long and wide data formats  {-}

Try converting the `iris` data set into the long format, with a column called "trait" to indicate sepal and petal length and width. 

Once you have converted to the long format, calculate the average for each combination of species and trait.

### Simulating and Fitting Data Distributions

This exercise teaches you how to compare a histogram of continuous (or integer) data to the probability density functions for different statistical distributions.

1. Set up a new `.Rmd` file for this exercise. Copy and paste the code below into different code chunks, and then read the text and run the code chunks one at a time to see what they do. You probably won't understand everything in the code, but this is a good start for seeing some realistic uses of `ggplot`. We will cover most of these details in the next few weeks.

2. Once the code is in and runs, try reading in your own `.csv` file into a data frame with this code chunk:

```{r, eval=FALSE}
z <- read.table("MyDataFile.csv",header=TRUE,sep=",", stringsAsFactors=FALSE)
str(z)
summary(z)
```

Lauren will be able to help you if you are having trouble reading in the data.

3. Once your data are in, go ahead and comment out the "fake data" that are simulated in the chunk below. At that point, if you compile the entire file, it should run all of the code on your own data. Be sure to add comments to the code and commentary to the `.Rmd` file so that you can go back to it later and understand and use the code in your work.

4. Take a look at the second-to-last graph which shows the histogram of your data and 4 probability density curves (normal, uniform, exponential, gamma) that are fit to the data. The `beta` distribution in the final graph is somewhat special. It often fits the data pretty well, but that is because we have assumed the largest data point is the true upper bound, and everything is scaled to that. The fit of the uniform distribution also fixes the upper bound. The other curves (normal, exponential, and gamma) are more realistic because they do not have an upper bound. For most data sets, the gamma will probably fit best, but if you data set is small, it may be very hard to see much of a difference between the curves.

5. Using the best-fitting distribution, go back to the code and get the maximum likelihood parameters. Use those to simulate a new data set, with the same length as your original vector, and plot that in a histogram and add the probability density curve. Right below that, generate a fresh histogram plot of the original data, and also include the probability density curve. 

How do the two histogram profiles compare? Do you think the model is doing a good job of simulating realistic data that match your original measurements? Why or why not?

If you have entered a large data frame with many columns, try running all of the code on a different variable to see how the simulation performs.

Once we get a little bit more R coding under our belts, we will return to the problem of simulating data and use some of this code again.




#### Open libraries  {-}

```{r}
library(ggplot2) # for graphics
library(MASS) # for maximum likelihood estimation
```

#### Read in data vector  {-}

To illustrate, we will generate some fake data here:

```{r}
# quick and dirty, a truncated normal distribution to work on the solution set

z <- rnorm(n=3000,mean=0.2)
z <- data.frame(1:3000,z)
names(z) <- list("ID","myVar")
z <- z[z$myVar>0,]
str(z)
summary(z$myVar)
```

#### Plot histogram of data  {-}

Plot a histogram of the data, using a modification of the code from lecture. Here we are switching from `qplot` to `ggplot` for more graphics options. We are also rescaling the y axis of the histogram from counts to density, so that the area under the histogram equals 1.0.

```{r}
p1 <- ggplot(data=z, aes(x=myVar, y=..density..)) +
  geom_histogram(color="grey60",fill="cornsilk",size=0.2) 
print(p1)
```

#### Add empirical density curve  {-}

Now modify the code to add in a kernel density plot of the data. This is an empirical curve that is fitted to the data. It does not assume any particular probability distribution, but it smooths out the shape of the histogram:

```{r}
p1 <-  p1 +  geom_density(linetype="dotted",size=0.75)
print(p1)
```

#### Get maximum likelihood parameters for `normal`  {-}

Next, fit a normal distribution to your data and grab the maximum likelihood estimators of the two parameters of the normal, the mean and the variance:

```{r}
normPars <- fitdistr(z$myVar,"normal")
print(normPars)
str(normPars)
normPars$estimate["mean"] # note structure of getting a named attribute

```
#### Plot `normal` probability density  {-}

Now let's call the `dnorm` function inside ggplot's `stat_function` to generate the probability density for the normal distribution. Read about `stat_function` in the help system to see how you can use this to add a smooth function to any ggplot. Note that we first get the maximum likelihood parameters for a normal distribution fitted to thse data by calling `fitdistr`. Then we pass those parameters (`meanML` and `sdML` to `stat_function`:

```{r}

meanML <- normPars$estimate["mean"]
sdML <- normPars$estimate["sd"]

xval <- seq(0,max(z$myVar),len=length(z$myVar))

 stat <- stat_function(aes(x = xval, y = ..y..), fun = dnorm, colour="red", n = length(z$myVar), args = list(mean = meanML, sd = sdML))
 p1 + stat
```

Notice that the best-fitting normal distribution (red curve) for these data actually has a biased mean. That is because the data set has no negative values, so the normal distribution (which is symmetric) is not working well.

#### Plot `exponential` probability density  {-}

Now let's use the same template and add in the curve for the exponential:

```{r}
expoPars <- fitdistr(z$myVar,"exponential")
rateML <- expoPars$estimate["rate"]

stat2 <- stat_function(aes(x = xval, y = ..y..), fun = dexp, colour="blue", n = length(z$myVar), args = list(rate=rateML))
 p1 + stat + stat2
```

#### Plot `uniform` probability density  {-}

For the uniform, we don't need to use `fitdistr` because the maximum likelihood estimators of the two parameters are just the minimum and the maximum of the data:

```{r}
stat3 <- stat_function(aes(x = xval, y = ..y..), fun = dunif, colour="darkgreen", n = length(z$myVar), args = list(min=min(z$myVar), max=max(z$myVar)))
 p1 + stat + stat2 + stat3
```


#### Plot `gamma` probability density  {-}


```{r}
gammaPars <- fitdistr(z$myVar,"gamma")
shapeML <- gammaPars$estimate["shape"]
rateML <- gammaPars$estimate["rate"]

stat4 <- stat_function(aes(x = xval, y = ..y..), fun = dgamma, colour="brown", n = length(z$myVar), args = list(shape=shapeML, rate=rateML))
 p1 + stat + stat2 + stat3 + stat4
```


#### Plot `beta` probability density  {-}
This one has to be shown in its own plot because the raw data must be rescaled so they are between 0 and 1, and then they can be compared to the beta.

```{r}

pSpecial <- ggplot(data=z, aes(x=myVar/(max(myVar + 0.1)), y=..density..)) +
  geom_histogram(color="grey60",fill="cornsilk",size=0.2) + 
  xlim(c(0,1)) +
  geom_density(size=0.75,linetype="dotted")

betaPars <- fitdistr(x=z$myVar/max(z$myVar + 0.1),start=list(shape1=1,shape2=2),"beta")
shape1ML <- betaPars$estimate["shape1"]
shape2ML <- betaPars$estimate["shape2"]

statSpecial <- stat_function(aes(x = xval, y = ..y..), fun = dbeta, colour="orchid", n = length(z$myVar), args = list(shape1=shape1ML,shape2=shape2ML))
pSpecial + statSpecial
```

## Homework #7 {-}

### Creating Fake Data Sets To Explore Hypotheses 
{-}

1. Go back to your "thinking on paper" exercise, and decide on a pattern that you might expect in your experiment if a specific hypothesis were true.

2. To start simply, assume that the data in each of your treatment groups follow a normal distribution. Specify the sample sizes, means, and variances for each group that would be reasonable if your hypothesis were true.

3. Using the methods we have covered in class, write code to create a random data set that has these attributes. Organize these data into a data frame or tibble with the appropriate structure.

4. Now write code to analyze the data (probably as an ANOVA or regression analysis, but possibly as a logistic regression or contingency table analysis. Write code to generate a useful graph of the data.

5. Try running your analysis multiple times to get a feeling for how variable the results are with the same parameters, but different sets of random numbers.

6. Now begin adjusting the means of the different groups. Given the sample sizes you have chosen, how small can the differences between the groups be (the "effect size") for you to still detect a significant pattern (p < 0.05)?

7. Alternatively, for the effect sizes you originally hypothesized, what is the minimum sample size you would need in order to detect a statistically significant effect? Again, run the model a few times with the same parameter set to get a feeling for the effect of random variation in the data.

8. Write up your results in a markdown file, organized with headers and different code chunks to show your analysis. Be explicit in your explanation and justification for sample sizes, means, and variances.

9. If you have time, try repeating this exercise with one of the more sophisticated distributions, such as the gamma or negative binomial (depending on the kind of data you have). You will have to spend some time figuring out by trial and error the parameter values you will need to generate appropriate means and variances of the different groups.
