# 2023  {-}


## Command line {-}
### Ecological Genomics Tutorials: Intro to connecting to unix servers and navigating the command-line {-}

 *What is the command-line?*
  
  The command-line, also known as a "terminal" or "shell", is a way of interacting with your local computer or a remote server by means of typing commands or scripts, without using a graphical user interface (GUI).



*Why do I want to be doing this?*
  
  At first, the command-line can seem a little intimidating. But after you get used to typing instead of pointing and clicking to issue your commands, youll realize how powerful it is. For example, its quite easy to copy, move, edit, and search within thousands of files in multiple directories with some simple command-line code. It would take forever to do this by dragging/dropping with a mouse. The command-line also allows you to work with very large data files without uncompressing them fully, or loading the entire files contents into memory…something that standard GUI type applications arent good at.



*So, lets get started…*

- The first step is to open a terminal *shell* on your local computer. For windows users, this would be "PuTTy". For MacOS users, this is called "Terminal".

- Well connect to our remote server running Unix using the secure shell (ssh) protocol. Our servers name is *pbio381* and we can connect to it using our UVM netid username and password (as long as were on-campus)

```
ip0af52fbf:papers srkeller$ ssh srkeller@pbio381.uvm.edu
srkeller@pbio381.uvm.edus password: 
Last login: Tue Jan 31 10:51:15 2017 from ip040027.uvm.edu
[srkeller@pbio381 ~]$ 
```



- The log-in screen tells us some basic info on when we last logged in, and then gives us our current location in the filesystem (~) followed by the $ prompt, that tells us the computer is ready for our next command. 

  - NOTE: The tilda (~) is short-hand for your home directory in UNIX. This is your own personal little corner of the computers hard drive space, and is the location that you should use to create folders and input/output/results files that are specific to your own work. No one has access to any files stored in your home directory but you.

- To see the full path to your current directory, use the **pwd** command:
  
  ```
[srkeller@pbio381 ~]$ pwd
/users/s/r/srkeller
[srkeller@pbio381 ~]$ 
  ```



- The path shows the full directory address up from the "root" of the file structure, which is the most basal level (appealing to all you phylogeneticists here…). The root is symbolized as "/" and each subdirectory is separated by an additional "/". So, the full path to my working directory on the server is */users/s/r/srkeller/*
  
  
  - Lets make a new folder (aka, directory) using the **mkdir** command. Lets name this folder "mydata"

```
[srkeller@pbio381 ~]$ mkdir mydata
```

- We can then use the **ll** command to show the current contents of any folders and files in our current location:
  
  ```
[srkeller@pbio381 ~]$ ll
total 0
drwxr-xr-x. 6 srkeller users 82 Jan 31 17:21 archive
drwxr-xr-x. 2 srkeller users  6 Jan 31 17:21 mydata
drwxr-xr-x. 2 srkeller users  6 Jan 31 17:15 scripts
[srkeller@pbio381 ~]$ 
  ```

- Youll notice that Ive got some extra folders in my output from previous work, whereas you will probably only see the "scripts" folder you just made. 
- NOTE: Each row shows a file or a folder (in this case, these are all folders) displaying (from right to left) its name, when it was last edited, size, who it belongs to , and who has permission to read (r) write (w) and execute (x) it. More on permissions later...
- Try making your own folder named "scripts" and then use the **ll** command to list the folders again 
- We can change our current location within the directory structure using the **cd** command. Lets use **cd** to move inside the *mydata/* directory and **ll** to list its contents:

```
[srkeller@pbio381 ~]$ cd mydata/
[srkeller@pbio381 mydata]$ ll
total 0
[srkeller@pbio381 mydata]$ 
```

- Hah — nothing in there yet! Lets go get some data!
  - Weve placed the text file containing all the metadata information on the seastar sampling under a shared space on the server. The path to this shared space is: 
    - */data/*   Try using **cd** to navigate over to this location. Then **ll** to show its contents. You should see something like this:

```
drwxr-xr-x.  5 root root       73 Jan 31 17:35 archive
drwxrwxr-x.  2 root pb381adm   40 Nov 30  2015 packages
drwxrwxr-x. 33 root pb381adm 4096 Nov 30  2015 popgen
drwxrwxr-x.  3 root pb381adm   42 Jan 30 09:08 project_data
drwxrwxr-x.  2 root pb381adm    6 Oct  2  2015 scripts
drwxr-xr-x. 18 root root     4096 Sep  2  2015 users
[srkeller@pbio381 data]$ 
```

- Now, **cd** into the folder called "project_data" and **ll**. Do you see this?

```
[srkeller@pbio381 data]$ cd project_data/
[srkeller@pbio381 project_data]$ ll
total 8
drwxr-xr-x. 12 srkeller users 4096 Jan 30 09:06 archive
-rw-r--r--.  1 srkeller users 1255 Jan 30 09:08 ssw_samples.txt
[srkeller@pbio381 project_data]$ 
```

- The file called "ssw_samples.txt" is the one with the seastar metadata. We dont want to open and make changes to this file in the shared space, because we dont want to have our edits affect the rest of the group. So, lets first make a copy of this file over to our home directory and put it inside the "mydata" folder. Use the **cp** command, followed by the filename, and the path to your destination (remember the ~ signals your home directory, and each subdirectory is then separated by a /):
  
  ```
[srkeller@pbio381 project_data]$ cp ssw_samples.txt ~/mydata/
  ```

- **cd** back to your *~/mydata/* directory and look inside. You should see your file...

```
[srkeller@pbio381 project_data]$ cd ~/mydata/
  [srkeller@pbio381 mydata]$ ll
total 4
-rw-r--r--. 1 srkeller users 1255 Jan 31 17:42 ssw_samples.txt
[srkeller@pbio381 mydata]$ 
  ```

- Lets take a peek at this file with the **head** command, which prints the first 10 lines to screen.

```
[srkeller@pbio381 mydata]$ head ssw_samples.txt 
Individual	Trajectory	Location	Day3	Day6	Day9	Day12	Day15
10	HH	INT	10_5-08_H	10_5-11_H	10_5-14_H	10_5-17_H	10_5-20_H
24	HH	INT	24_5-08_H	24_5-11_H	24_5-14_H	24_5-17_H	24_5-20_H
27	HH	INT	27_5-08_H	27_5-11_H	27_5-14_H	27_5-17_H	27_5-20_H
08	HS	INT	08_5-08_H	08_5-11_S	08_5-14_S	08_5-17_S	08_5-20_S
09	HS	INT	09_5-08_H		09_5-14_S	09_5-17_S	09_5-20_S
15	HS	INT	15_5-08_H	15_5-11_H	15_5-14_H	15_5-17_S	15_5-20_S
19	HS	INT		19_5-11_H	19_5-14_H	19_5-17_H	19_5-20_S
20	HS	INT	20_5-08_H	20_5-11_H	20_5-14_H	20_5-17_H	20_5-20_S
03	SS	INT	03_5-08_S	03_5-11_S
```

- The **tail** command provides similar functionality, but prints just the last lines in the file. These features may not seem a big deal right now, but when youre dealing with files that are 20 Gb compressed, and feature hundreds of millions of lines of data, you and your computer will be happy to have tools to peek inside without having to open the whole file!
  - What if we want to extract just the rows of data that correspond to Healthy (HH) individuals? We can use the search tool **grep** to search for a target query. Any line matching our search string will be printed to screen.

```
[srkeller@pbio381 mydata]$ grep 'HH' ssw_samples.txt 
10	HH	INT	10_5-08_H	10_5-11_H	10_5-14_H	10_5-17_H	10_5-20_H
24	HH	INT	24_5-08_H	24_5-11_H	24_5-14_H	24_5-17_H	24_5-20_H
27	HH	INT	27_5-08_H	27_5-11_H	27_5-14_H	27_5-17_H	27_5-20_H
31	HH	SUB	31_6-12_H	31_6-15_H	31_6-18_H	31_6-21_H	31_6-24_H
32	HH	SUB	32_6-12_H	32_6-15_H	32_6-18_H	32_6-21_H	
33	HH	SUB	33_6-12_H	33_6-15_H	33_6-18_H	33_6-21_H	33_6-24_H
34	HH	SUB	34_6-12_H	34_6-15_H	34_6-18_H	34_6-21_H	34_6-24_H
35	HH	SUB	35_6-12_H	35_6-15_H	35_6-18_H	35_6-21_H	
[srkeller@pbio381 mydata]$
  ```

- What if instead of printing it to screen, we want to save the output of our search to a new file? This is easy, just use the ">" symbol to redirect the results of any command to an output file with your choice of name.

```
[srkeller@pbio381 mydata]$ grep 'HH' ssw_samples.txt >ssw_HHonly.txt
[srkeller@pbio381 mydata]$ ll
total 8
-rw-r--r--. 1 srkeller users  462 Jan 31 20:46 ssw_HHonly.txt
-rwxrwxr-x. 1 srkeller users 1255 Jan 31 17:42 ssw_samples.txt
[srkeller@pbio381 mydata]$ 
  ```

- We can do the same routine for the "SS" samples. Heres a trick, when youre doing a similar task as a previous command, hit the up arrow on your keyboard at the $ prompt, and it will recall the last command you issued. Then you just have to switch the HHs for SSs.

```
[srkeller@pbio381 mydata]$ grep 'SS' ssw_samples.txt >ssw_SSonly.txt
[srkeller@pbio381 mydata]$ ll
total 12
-rw-r--r--. 1 srkeller users  462 Jan 31 20:46 ssw_HHonly.txt
-rwxrwxr-x. 1 srkeller users 1255 Jan 31 17:42 ssw_samples.txt
-rw-r--r--. 1 srkeller users  342 Jan 31 20:48 ssw_SSonly.txt
[srkeller@pbio381 mydata]$ 
  ```

- **Grep** is a useful search tool and has many additional features for sorting and output of the results. These kinds of search algorithms are called "regular expressions", or "regexp", and are one of the most powerful tools for wokring with large text files. If you want to learn more about **grep** and its regexp capabilities, you can look at the **"man"** page or manual. In fact, every UNIX command-line program has a built-in **man** page that you can call up to help you. Just type **man** and then the program name and it will give you the manual (small excerpt shown below).

```
[srkeller@pbio381 mydata]$ man grep


GREP(1)  General Commands Manual GREP(1)

NAME
grep, egrep, fgrep - print lines matching a pattern

SYNOPSIS
grep [OPTIONS] PATTERN [FILE...]
grep [OPTIONS] [-e PATTERN | -f FILE] [FILE...]

DESCRIPTION
grep searches the named input FILEs (or standard input if no files are named, or if a
           single hyphen-minus (-) is given as file name) for lines containing a  match  to  the
given PATTERN.  By default, grep prints the matching lines.

In  addition,  two variant programs egrep and fgrep are available.  egrep is the same
as grep -E.  fgrep is the same as grep -F.  Direct  invocation  as  either  egrep  or
fgrep  is  deprecated,  but is provided to allow historical applications that rely on
them to run unmodified.

OPTIONS
Generic Program Information
--help Print a usage message briefly summarizing these command-line options  and  the
bug-reporting address, then exit.

-V, --version
Print  the version number of grep to the standard output stream.  This version
number should be included in all bug reports (see below).

Matcher Selection
-E, --extended-regexp
Interpret PATTERN as an extended regular expression (ERE, see below).  (-E  is
       specified by POSIX.)

-F, --fixed-strings, --fixed-regexp
Interpret  PATTERN  as  a list of fixed strings, separated by newlines, any of
which is to be matched.  (-F is  specified  by  POSIX,  --fixed-regexp  is  an
obsoleted alias, please do not use it in new scripts.)

-G, --basic-regexp
Interpret PATTERN as a basic regular expression (BRE, see below).  This is the
default.

-P, --perl-regexp
Interpret PATTERN as a Perl regular expression.  This is  highly  experimental
and grep -P may warn of unimplemented features.
```

- One of the most useful aspects of UNIX is the ability to take the output from one command and use it as standard input (termed 'stdin') into another command without having to store the intermediate files. Such a workflow is called "piping", and makes use of the pipe character (|) located above the return key to feed data between programs.
- Example: Say we wanted to know how many samples come from the Intertidal. We can use **grep** to do the search, and pipe the results to the command **wc** which will tally up the number of lines, words, and characters in the file…voila!
  
  ```
[srkeller@pbio381 mydata]$ grep 'INT' ssw_samples.txt | wc
16     106     762
[srkeller@pbio381 mydata]$ 
  ```

- Looks like 16 INT samples in the original data. See how quick it was to get a line count on this match, without actully opening a file or printing/saving the outputs? 
  - Now, what if we want to move the files we created with just individuals of a particular disease status. Theres a way to do this quickly using the wildcard character "*". With the wildcard, the "*\*" takes the place of any character, and in fact any length of characters. For example, make a new directory called *samples_by_disease/* inside the *mydata/* folder. Then move all files that contain the word "only" into the new directory using the **mv** command.

```
[srkeller@pbio381 mydata]$ mkdir sample_by_disease/
[srkeller@pbio381 mydata]$ ll
total 12
drwxr-xr-x. 2 srkeller users   10 Jan 31 21:12 sample_by_disease
-rw-r--r--. 1 srkeller users  462 Jan 31 20:46 ssw_HHonly.txt
-rwxrwxr-x. 1 srkeller users 1255 Jan 31 17:42 ssw_samples.txt
-rw-r--r--. 1 srkeller users  342 Jan 31 20:48 ssw_SSonly.txt
[srkeller@pbio381 mydata]$ mv *only* sample_by_disease/
[srkeller@pbio381 mydata]$ ll
total 4
drwxr-xr-x. 2 srkeller users   60 Jan 31 21:12 sample_by_disease
-rwxrwxr-x. 1 srkeller users 1255 Jan 31 17:42 ssw_samples.txt
[srkeller@pbio381 mydata]$ cd sample_by_disease/
[srkeller@pbio381 sample_by_disease]$ ll
total 8
-rw-r--r--. 1 srkeller users 462 Jan 31 20:46 ssw_HHonly.txt
-rw-r--r--. 1 srkeller users 342 Jan 31 20:48 ssw_SSonly.txt
[srkeller@pbio381 sample_by_disease]$ 
```

- OK, what about when we have files we don't want anymore? How do we clean up our workspace? You can remove files and folders with the **rm** command. However, in its default mode, UNIX will not ask if you really mean it before getting rid of it forever(!), so this can be dangerous if you're not paying attention. 
  - As an example, let's use our **grep** command to pull out he seastar samples that started healthy and then became sick. But perhaps we later decide we're not going to work with those samples, so we use **rm** to delete that file:

```
[srkeller@pbio381 mydata]$ ll
total 8
drwxr-xr-x. 2 srkeller users   60 Jan 31 21:12 sample_by_disease
-rw-r--r--. 1 srkeller users  282 Feb  1 05:35 ssw_HSonly.txt
-rwxrwxr-x. 1 srkeller users 1255 Jan 31 17:42 ssw_samples.txt
[srkeller@pbio381 mydata]$ rm ssw_HSonly.txt 
[srkeller@pbio381 mydata]$ ll
total 4
drwxr-xr-x. 2 srkeller users   60 Jan 31 21:12 sample_by_disease
-rwxrwxr-x. 1 srkeller users 1255 Jan 31 17:42 ssw_samples.txt
[srkeller@pbio381 mydata]$
```
- Gone! Forever! If that worries you, you can change your personal settings so that the server asks you to confirm deletion before it acts. To do this, well need to follow a couple of new steps:
  
  
  1.    **cd** to your home directory (~/)
2. list all the files, including "hidden" ones that arent usually shown. To do this, use `ll -a`.
      		3. Look for a file called ".bashrc" — this contains your settings for how you interact with the server when you log in.
      		4. Were going to open this file and edit it to add a setting to request that **rm** confirms deletion with us. To edit text files on the fly in UNIX, you can use the built-in text editor, "vim": `vim .bashrc`
5. You should see something that looks like this:
  
  ```
# .bashrc

# Source global definitions
if [ -f /etc/bashrc ]; then
. /etc/bashrc
fi

# Uncomment the following line if you don't like systemctl's auto-paging feature:
# export SYSTEMD_PAGER=

# User specific aliases and functions

```

6.   Use your arrow key to move your cursor down to the last line, below ""# User specific aliases and functions" — this is where we're going to insert our new function.

7.   By defauly, vim is in read-only mode when it opens files. To go into edit mode, press your "i" key (for "insert"). You are now able to make changes to the file.

8.   Add the following text on a new line directly below the "# User specific…" line:
  
  `alias rm='rm -i'`

9.   Your file should now look like this:
  
  ```
# .bashrc

# Source global definitions
if [ -f /etc/bashrc ]; then
. /etc/bashrc
fi

# Uncomment the following line if you don't like systemctl's auto-paging feature:
# export SYSTEMD_PAGER=

# User specific aliases and functions

alias rm='rm -i'
```

10.    Youre now ready to get out of edit mode (hit the `escape key`), save your changes (type `:w`), and exit vim (type `:q`).

11.    These changes wont take effect until you log out (type `exit` to log out of the server). But from now on, every time you log in, the server will remember that you want a reminder before deleting any of your work.



##Let's review what we've learned so far…##

- Logging in to the server: `ssh netid@pbio381.uvm.edu`
- Finding what directory youre in: `pwd`
- Listing files in your current directory, or changing to a new directory: `ll`, `cd`
- Making a new folder: `mkdir foldername`
- Location of shared space, data, and programs on our class server:

```
[srkeller@pbio381 ~]$ cd /data/
[srkeller@pbio381 data]$ ll
total 8
drwxr-xr-x.  5 root root       73 Jan 31 17:35 archive
drwxrwxr-x.  2 root pb381adm   40 Nov 30  2015 packages
drwxrwxr-x. 33 root pb381adm 4096 Nov 30  2015 popgen
drwxrwxr-x.  3 root pb381adm   42 Jan 30 09:08 project_data
drwxrwxr-x.  2 root pb381adm    6 Oct  2  2015 scripts
drwxr-xr-x. 18 root root     4096 Sep  2  2015 users
[srkeller@pbio381 data]$ 
```

- Copying or moving files from one location to another: `cp filename destinationpath/` or `mv filename destinationpath/` 
- Peeking into the first or last few lines of a file: `head filename`, `tail filename`
- Searching within a file for a match: `grep 'search string' filename`
- Outputing the results of a command to a new file: `grep 'search string' filename >outputfilename`
- Using wildcards to work on multiple files at the same time: `mv *.txt ~/newfolder`
-  Using the "pipe" to send the output of one command to the input of another: `grep 'INT' filename | wc `
- Removing files or folders: `rm`
- Editing text files on the server: `vim filename`       


### Handy [UNIX cheat sheet](https://files.fosswire.com/2007/08/fwunixref.pdf) for helping to remember some of these commonly used commands (and others) {-}

### Here's another useful [UNIX cheatsheet](http://cheatsheetworld.com/programming/unix-linux-cheat-sheet/) {-}


## Github Tutorial #1{-}

  Before getting started, make sure you have a GitHub account [here](https://github.com/) and download github desktop [here](https://desktop.github.com/).

##### In the browser {-}

### Making a Github Repository {-}

1.  Navigate to your profile on GitHub
2.  Click the Repositories link at the top of the page
3.  Select the green *New* button in the top right corner of your screen
4.  Name your repository, select **Add a README file** and select the green *Create Repository* button at the bottom of the screen

### Modifying files {-}

In order to edit a file in the browser, for example your README.md, click on the little pencil button in the top right corner, then when you are done, click the green *Commit changes* button.

### Uploading files {-}

In order to upload a file, click on the *Add file* button just left of the green *Code* button.

### To review commit history {-}

To review all the changes you have made to the repo, click on the little clock icon followed by your number of commits just under the green *Code* button. You can click on each commit to see what you have changed.

### Using the GitHub desktop app {-}

### Clone existing repos {-}

1.  Open the GitHub desktop app.
2.  Click on *Clone a repository*. You will have to sign in to your GitHub account.
3.  Choose a repository you would like to clone and specify where your computer should store it. This step might take a few minutes to load.
4.  Once the repo has been cloned, find the new folder on your computer. This is where your files will be living - you can edit, remove or add files here and GitHub desktop will track these changes.

### Editing and adding files {-}

1.  Open the README.md file with any markdown editor you like (perhaps RStudio is the best ;) )
2.  Make any kind of change and then save your changes (like you always do when you want to save changes to a file you are working on).
3.  Navigate back to the GitHub desktop app. On the left, you should see your changes appear!
  4.  To add a file to your repo, you can simply drag and drop your file on your computer into the above mentioned folder. Again, you should see the fact that you have added a file on the left in your GitHub desktop app.
5.  Once you are done modifying your repo, make some notes in the bottom left corner about what you have done, and click *Commit to main*. All your changes should disappear from the left bar and a new line should appear in the bottom left corner under *Committed just now*.
6.  Now, you can click on *Push origin* in order to upload your changes.
7.  You can head to your GitHub repo page in the browser, and after refreshing the page, you should see your changes appear.

NOTE: The git commit command captures a snapshot of the projects changes. The git push command is used to upload local repository content to a remote repository. Thus, you can make several commits before pushing all the commits at once, however, it is generally advisable to push as you go.

### What if you have modified your repo somewhere else? {-}

1.  Go to your GitHub repo in the browser and make a change to your README.md file. Make sure to commit your changes.
2.  Navigate back to your GitHub desktop app. As you will see, there are no changes in the left bar because you havent changed anything locally. If you click on the *History* bar, you will see your past changes, but to the most recent one you made in the browser.
3.  In order to retrive the changes you made in the browser, first click *Fetch origin* (this will change if there has been any remote changes to your repo), then click on *Pull origin* to download the changes you have made to your local computer.
4.  You should now be able to see your changes in the folder on your computer, as well as in the *History* bar on the left.

### Merge conflict before committing {-}

What if you have made some changes to your repo locally, before pulling the changes you have made remotely? Lets test it out!

1.  Make another change to your README.md file in the browser. Commit.
2.  Now open the README.md file on your computer. As you see, the changes you made in the browser are not present in your version of the file (as we havent pulled yet, on purpose). Make a change to the file (different from what you have done in the browser) and save it.
3.  Navigate back to your GitHub desktop app.
4.  Click on *Fetch origin* and then *Pull origin*.
5.  This will produce and ERROR message, which is exactly what we wanted. Click *stash changes and continue*. You will see that the history bar only shows the changes you have made online, it is missing the changes you have made locally. Also, there will be a new bar on the bottom that says *Stashed Changes*.
6.  Click on *Stashed Changes* then click on *Restore*.
7.  You will see that some lines are highlighted with blue on the left. Click on the lines that you dont want to keep (this will unselect them and they will no longer appear blue). Then, click on commit and push. The changes should appear in the History tab, as well as in the browser.
8.  Finally, you can discard the changes that still appear in the left tab by right clicking on the name of the file (e.g. README.md) and selecting *Discard changes*. You are done!

### Merge conflict after committing {-}

1.  Make a change to your README.md file in the browser. Commit.
2.  Now open the README.md file on your computer. As you see, the changes you made in the browser are not present in your version of the file (as we havent pulled yet, on purpose). Make a change to the file (different from what you have done in the browser) and save it.
3.  Navigate back to your GitHub desktop app. Commit your local change. Click on the push button.
4.  You will get an error message saying "Newer commits on remote". It is saying that there have been some remote changes that you need to pull first, before you could push your new changes. Click on Fetch.
5.  The button on the top should change to *Pull origin*. Click on it. Since you have modified the same file, git wont be able to automatically resolve your merge, and it tells you to open the file the has the merge conflict. GitHub desktop will suggest an editor app to open you file in (you can change this, lets say to RStudio by hitting *Abort*, GitHub Desktop in the top left corner, *Integrations* \> *External editor*). Click on "Open in" followed be the name of your editor of choise.
6.  Delete the lines you want to discard (typically, lines with \>,\< or = symbols). Save your changes.
7.  Go back to you GitHub Desktop. The marge conflict message should change to *No conflicts remaining*. Click *Continue merge*.
8.  We have essentially made a new commit by clicking on *Continue merge*, so you should see a new commit appear in the bottom left corner of the page, beginning with "Merge branch 'main'". Now we just have to click on *Push origin*. Resolved!
  9.  To double check what we have done, navigate to the *History* tab. You should be able to see both of the changes that you have made: the one you made locally, and the one you made remotely. And, if you navigate back to your browser and look at the file you have been modifying, you should see the final version of the file.

### Using git in the terminal {-}

1.  In order to be able to access your repos remotely, e.g. from the class server, you will need to generate a personal access token on GitHub. To do that, navigate to GitHub, click on your little icon, click Settings \> Developer settings \> Personal access tokens \> Tokens (classic) \> Generate new token (classic) \> Name and select repo. Copy and save the personal access token!
  2.  Open your terminal and type ssh NETID\@ecogen.uvm.edu. Enter your UVM password when prompted.
3.  Go to the browser and find the page for your GitHub repo and youd like to use. Click on the green *Code* button. Copy the HTTPS link, it will be something like: [https://github.com/Cpetak/your_repo_name.git](https://github.com/Cpetak/test.git)
4.  Type `git clone https://github.com/Cpetak/test.git` to clone your git repo. Enter your GitHub username and when prompted for a password, paste your personal access token.
5.  Now if you type `ls`, you should see you repo! Try to `cd`, and ls to view the content. Everything should be there!
6.  Type `echo "hello from the class server" > my_new_file.txt` to make a new file.
7.  Type `git status`. This will show you if you have made any changes to the repo, just like GitHub desktop does.
8.  You will see that the new file we created my_new_file.txt is listed under *Untracked files*. This is because we have to tell git that this is a file/change that we would like git to keep track of. To that, type `git add my_new_file.txt`.
9.  Type `git status` again. You will see that my_new_file.txt has moved to *Changes to be committed*. So lets commit this change! Type `git commit -m"testing class server"`.
10. Now you have made a commit titled "testing class server", so we have created a snapshot of our project. Just like before, we need to push the commit in order for us to be able to see the changes outside of the remote server we are using right now. Type `git push origin main` (origin means remote repo, main means main branch). You will need to enter your github username and personal access code. `git status` should tell you that there is nothing to commit.
11. Navigate back to your GitHub desktop and click *Fetch origin* and then *Pull origin*. You should be able to see the commit you made on the class server in your History tab. Yeyy!
  
  ### Merge conflict in the terminal {-}
  
  1.  Go to the git folder on your computer, open the my_new_file.txt file and add a line at the end. Save, commit and push through GitHub desktop.

2.  Go to your terminal and locate your git repo on the class server.

3.  Type `git pull origin main`. You will need to enter your github username and personal access code. It should show you that a file has been modified. `cat my_new_file.txt` should show you the updated version of the file.

4.  Repeat step 1 and step 2. Now, instead of pulling first (like you should to avoid a merge conflict) type `echo "here is a new line from the server" >> my_new_file.txt`. This will append a new line to the file called my_new_file.txt. Note, if you type `cat my_new_file.txt`, you will only see the modification you made on the server, but not the one you made on your computer, because you havent pulled.

5.  Type `git pull origin main`. It will complain and tell you "Please commit your changes or stash them before you merge. Aborting". To resolve this issue, type `git add my_new_file.txt`, then `git commit -m'changed my file for reasons'`.

6.  Now type `git pull origin main`. This time, it will tell you "You have divergent branches and need to specify how to reconcile them" and "Need to specify how to reconcile divergent branches". Since we will want to merge in the future, type `git config pull.rebase false`. Then, type `git pull origin main` again.

7.  This will give you "CONFLICT (content): Merge conflict in my_new_file.txt". Type `nano my_new_file.txt` to fix the conflict (i.e. remove lines you dont want.)

8.  Finish by doing the usual `git add my_new_file.txt`, `git commit -m'resolved conflict'`, `git push origin main`. Pull in the GitHub app and check what you have done in the History tab. You should see both the local and the server commits, as well as your conflict resolution as separate commits.


## Github Tutorial #2 {-}
  
  Note: you can choose to use github on your browser, or your desktop through the app. You do not need to do both.
If youve already made a repository you can skip *Making a Github Repository* and go to the last step of *Adding additional files to your repo*

Make a github account [here](https://github.com/) and download github desktop [here](https://desktop.github.com/)

### Making a Github Repository {-}


### On your browser {-}
1. Navigate to your profile
2. Click the Repositories link at the top of the page
3. Select the green *New* button in the top right corner of your screen
4. Name your repository and select the green *Create Repository* button at the bottom of the screen

### Add to the desktop app {-}
1. In the blue box titled "Quick setup" click the *setup in desktop* button
2. This should prompt your Github Desktop to open. Select the button on the pop-up that says *clone repository*
3. The repository should show up in the left panel on the desktop app. To find the path to where this repository was made on your local machine, right click and select *show in finder*.
4. You may want to move your repository to a more convenient location (possibly where all your Ecological Genomics files live)


### Uploading initial files {-}

### On your browser
1. In the blue quick setup box, select the *uploading an existing file* link
2. Select files to add to your repository <br /> 
3.Click the green *commit changes* button

### On the desktop app  {-}
1. Move files to your github repository folder on your desktop (The file should show up in the left panel of your app)
2. Add a description of the file youre adding next to your github icon
3. Select the *commit to master* button
4. Click the *publish branch* button
5. Adding additional files is done the same way, but the button will say *push to origin*
  
  ### Adding additional files to your repo {-}
  
  ### On your browser {-}
  1. At the top of your repository page, select the *add file* button
2. Select files to add to your repository <br /> 
  3.Click the green *commit changes* button

### On the desktop app  {-}
1. Move files to your github repository folder on your desktop (The file should show up in the left panel of your app)
2. Add a description of the file youre adding next to your github icon
3. Select the blue *commit to master* button <br />
4.Select the *push to origin* button at the top of your screen


## Population Genomics #1 {-}

### Population & Landscape Genomics {-}
 #### Learning Objectives for 09/11/23 {-}
  
  1.  To get background on the study system (Red spruce, *Picea rubens*), and the experimental design of the exome capture data
2.  To understand the general work flow or "pipeline" for processing and analyzing the exome capture sequence data
3.  To visualize and interpret Illumina data quality (what is a fastq file; what are Phred (Q) scores?).
4.  To learn how to make/write a bash script, and how to use bash commands to process files in batches
5.  To trim the reads based on base quality scores in preparation for mapping to the reference genome

#### 1. Red spruce, *Picea rubens* {-}

(https://raw.githubusercontent.com/stephenrkeller/Ecological_Genomics/master/Fall_2023/pics/mmf.jpg?token=GHSAT0AAAAAACGK5AYTT2AWDSWLG36MEWMGZH6T54A)

Red spruce is a coniferous tree that plays a prominent role in montane communities throughout the Appalachians. It thrives in the cool, moist climates of the high elevation mountains of the Appalachians and northward along the coastal areas of Atlantic Canada. In the low-latitude trailing edge of the range, populations are highly fragmented and isolated on mountaintops. These "island" populations are remnants of spruce forests that covered the southern U.S. glaciers extended as far south as Long Island, NY. As the climate warmed at the end of the Pleistocene (\~20K years ago), red spruce retreated upward in elevation to these mountaintop refugia, where they are now highly isolated from other such stands and from the core of the range further north.

(https://upload.wikimedia.org/wikipedia/commons/4/42/Picea_rubens_range_map.png)

Because of its preference for cool, moist climates, red spruce shows climate sensitivities that may make it especially vulnerable to climate change. This makes assessing the amount and distribution of genetic diversity across the landscape of red spruce an important conservation issue! Ultimately, we want to use genomic insights to help inform conservation biologists working to restore red spruce and evaluate the potential for assisted migration (a form of human-mediated dispersal) to offset the loss of adaptation under climate change. A close partner in this effort is the [Nature Conservancy](https://www.nature.org/en-us/about-us/where-we-work/united-states/west-virginia/stories-in-west-virginia/sprucing-things-up-a-bit/) and the Central Appalachian Spruce Restoration Initiative ([CASRI](http://restoreredspruce.org)) -- a multi-partner group dedicated to restoring and enhancing red spruce populations to promote their resilience under climate change.

(https://restoreredspruce.org/wp-content/uploads/2023/07/cropped-cropped-CASRI-Website-Header-1536x142.png)

Some videos of the collaboration between UVM and CASRI on red spruce restoration:
  
  -   [Building Resilience](https://youtu.be/Ld1EvG9cZN8?si=f7kRufegkeJNWsLJ)

-   [Seeds of Hope](https://youtu.be/FYKbjXB4cHs)

Since 2015, the Keller Lab has been studying the genetic basis of climate adaptation across the entire distribution of *P. rubens*. Our main goal is to use genomics to aid conservation of red spruce under climate change through a better understanding of **(1) how genetic diversity diversity is distributed across the range and how that reflects the demographic history of population expansions, bottlenecks, gene flow, and divergence** and **(2) identify regions of the genome that show evidence of adaptation in response to abiotic climate gradients.** We hope to use this information to inform areas of the range most likely to experience climate mal-adaptation, and to help guide mitigation strategies such as sourcing seed for restoration and assisted migration.

### Summary of prior population genomics research on red spruce {-}

Our recent work funded by NSF (2017-2022) focused on early-life fitness of seedlings in response to population genomic variation and climate adaptation. That work sampled seeds and needle tissue from 340 mother trees at 65 populations spread throughout the range and generated population genomic data through exome capture sequencing. Seedlings from each mother were grown in multiple common gardens and measured for fitness traits. Based on these data, some of the insights we gleaned were:
  
  -   Red spruce has very low genetic diversity and has an Ne that has been declining for thousands of years [Capblancq et al. 2020](https://onlinelibrary.wiley.com/doi/full/10.1111/eva.12985)
-   Populations are differentiated into 3 geographically separated clusters of genetic ancestry in the north (core) mid-latitude (margin) and southern (edge) regions of its range [Capblancq et al. 2020](https://onlinelibrary.wiley.com/doi/full/10.1111/eva.12985))
-   Local populations level of genetic diversity and frequency of deleterious mutations (aka, "genetic load") were related to how well seedlings survived and grew under greenhouse conditions [Capblancq et al. 2021](https://link.springer.com/article/10.1007/s10592-021-01378-7)
-   Seedling growth traits in common garden experiments showed heritable genetic variation and genetically-based trait divergence among the 3 ancestry groups [Prakash et al. 2022](https://royalsocietypublishing.org/doi/full/10.1098/rstb.2021.0008)
-   Certain genomic regions showed strong allele frequency clines along climatic gradients indicative of selection, with gene functions often related to abiotic stress (heat and drought) [Capblancq et al. 2023](https://nph.onlinelibrary.wiley.com/doi/full/10.1111/nph.18465)
-   There was a hint in some of the results that hybridization with black spruce might be playing a role in some of the above results, but we lacked genomic data from black spruce to nail that down.

**Lets brainstorm about some issues these data couldnt address, or where the inference of population history or climate adaptation were constrained by aspects of the experimental design?** 
  
  *What questions would we want to ask next?*
  
  ## A new dataset for analysis
  
  The data well be analyzing here consist of exome capture data for adult red spruce growing in a [provenance trial in northern New Hampshire near the town of Colebrook](https://goo.gl/maps/4LSPvpN2RdVbUuH78). Here are the details:

-   95 individuals sampled from 12 populations across the range (N=190 red spruce)
-   Individuals were grown from seed and planted out into the provenance trial as 2 year old seedlings in 1960
-   Multiple studies have assessed survival, growth (height DBH), and cold tolerance of these individuals at multiple time points over the last 60 years
-   Needle tissue was sampled from surviving red spruce in the trial in May 2020 for genomic DNA
-   We also sampled 18 black spruce individuals from natural stands in 2 locations distant from red spruces range (MN and MI). 
-  These will be useful for detecting black spruce ancestry in the red spruce populations, if it exists
-   We used the same exome-capture probe set as Capblancq et al. (2020). 
-  *Why exome capture instead of alternatives (WGS, or RAD/GBS)??*
  -   Exome-capture was designed based on transcriptomes from multiple tissues and developmental stages in the related species, white spruce (*P. glauca*).
-   Bait design used 2 transcriptomes previously assembled by [Rigault et al. (2011)](https://academic.oup.com/plphys/article/157/1/14/6108725) and [Yeaman et al. (2014)](https://nph.onlinelibrary.wiley.com/doi/full/10.1111/nph.12819).
-   A total of 80,000 120 bp probes were designed, including 75,732 probes within or overlapping exomic regions, and an additional 4,268 probes in intergenic regions.
-   Each probe was required to represent a single blast hit to the *P. glauca* reference genome of at least 90bp long and 85% identity, covering **38,570 unigenes**.
-   Libraries were made by random mechanical shearing of DNA (250 ng -1ug) to an average size of 400 bp followed by ligation of barcoded adapters, and PCR-amplification of the library. SureSelect probes (Agilent Technologies: Santa Clara, CA) were used for targeted enrichment following the SureSelect Target Enrichment System for Illumina Paired-End Multiplexed Sequencing Library protocol.
-   Libraries were sequenced on an Illumina HiSeq X to generate paired-end 150-bp reads.

Heres the table of sample population codes used in file naming and their source localities

```
library(data.table)
dat = read.csv("~/Documents/GitHub/Ecological-Genomics/Fall2023/data/RS_sampleSites.csv", header=F)
names(dat) = c("PopCode","PopName","State/Province","Country","Latitude","Longitude","Elevation")
knitr::kable(dat)
```

### 2. Here's our "pipeline" {-}

-   Visualize the quality of raw data (Program: FastQC)

-   Clean raw data (Program: Trimmomatic)

-   Visualize the quality of cleaned data (Program: FastQC)

-   Calculate \#'s of cleaned, high quality reads going into mapping

Well then use these cleaned reads to align to the reference genome next time so that we can start estimating genomic diversity and population structure.

### 3.-5. Visualize, Clean, and Visualize again {-}

Whenever you get a new batch of NGS data, the first step is to look at the data quality of coming off the sequencer and see if we notice any problems with base quality, sequence length, PCR duplicates, or adapter contamination. A lot of this info is stored in the raw data files you get from the core lab after sequencing, which are in *"fastq" format*.

The fastq files for our project are stored in this path: `/netfiles/ecogen/PopulationGenomics/fastq/red_spruce`

`cd` over there and `ll` to see the files. There should be 190 fastq files -- 2 for each of the 95 samples (2 files/sample because these are paired-end reads, and each sample gets a file with the forward reads (R1) and another with the reverse reads (R2)).

The naming convention for our data is: `<PopCode>_<RowID>_<ColumnID>_<ReadDirection>.fast.gz`

Together, `<PopCode>_<RowID>_<ColumnID>` define the unique individual ID for each DNA sample, and there should be 2 files per sample (and R1 and an R2)

### So...what is a .fastq file anyway? {-}

[A fastq file is the standard sequence data format for NGS](https://en.wikipedia.org/wiki/FASTQ_format). It contains the sequence of the read itself, the corresponding quality scores for each base, and some meta-data about the read.

The files are big (typically many Gb compressed), so we cant open them completely. Instead, we can peek inside the file using `head`. But size these files are compressed (note the .gz ending in the filenames), and we want them to stay compressed while we peek. Bash has a solution to that called `zcat`. This lets us look at the .gz file without uncompressing it all the way. Lets peek inside a file:

```         
zcat 2505_9_C_R2.fastq.gz | head -n 4

@A00354:455:HYG3FDSXY:1:1101:3893:1031 2:N:0:CATCAAGT+TACTCCTT
GTGGAAAATCAAAACCCTAATGCTGAAAGGAATCCAAATCAAATAAATATTTTCACCGACCTGTTTCGATGCCAGAATTGTCTGCGCAGAAGACTCGTGAAATTTCGCCAGCAGGTAAAATTAAAAGGCTAGAATTAACCGCTGAAATGGA
+
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:F:F
```

*Note:* `zcat` lets us open a .gz (gzipped) file; we then "pipe" `|` this output from `zcat` to the `head` command and print just the top 4 lines `-n4`

The fastq file format has 4 lines for each read:

| Line | Description        |
|--------------------------|----------------------------------------------|
| 1    | Always begins with '\@' and then information about the read            |
| 2    | The actual DNA sequence         |
| 3    | Always begins with a '+' and sometimes the same info in line 1         |
| 4    | A string of characters which represent the **quality** scores; always has same number of characters as line 2 |

[Heres a useful reference for understanding Quality (Phred) scores](http://www.drive5.com/usearch/manual/quality_score.html). If P is the probability that a base call is an error, then:
  
  Q = -10\*log10(P)
  
  So:
    
    | Phred Quality Score | Probability of incorrect base call | Base call accuracy |
    |--------------------|---------------------------------|-------------------|
    | 10     | 1 in 10  | 90%   |
    | 20     | 1 in 100 | 99%   |
    | 30     | 1 in 1000| 99.9% |
    | 40     | 1 in 10,000           | 99.99%|
    
*The Phred Q score is translated to ASCII characters so that a two digit number can be represented by a single character.*

```         
  Quality encoding: !"#$%&'()*"+,-./0123456789:;<=>?@ABCDEFGHI
      |         |         |         |         |
    Quality score: 0........10........20........30........40   
```

*What kind of characters do you want to see in your quality score?*

### Visualize using FastQC {-}

Were going to use [the program FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) (already installed on our server). FastQC looks at the quality collectively across all reads in a sample.

First, lets `cd` back to our home directories `~/` and set up some new folders to store our work. Well make 3 directories to store our data, scripts, and results:

```         
mkdir mydata/
mkdir myscripts/
mkdir myresults/
```

Then lets cd into the `myresults/` folder then use `pwd` to prove to yourself that youre in the `myresults/` folder within your home directory. It should look like this (but with your home directory info instead of mine):

```         
[kellrlab@ecogen myresults]$ pwd
/users/k/e/kellrlab/myresults
```

Now within `myresults/` lets make another folder called `fastqc/` to hold the outputs from our QC analysis. Do that on your own, just like we did above, then cd into the `fastqc/` folder and type `pwd` again to prove to yourself you did it right.

Now, were ready to run FastQC to look at the quality of our sequencing. The basic command is like this:

```         
fastqc filename.fastq.gz -o outputdirectory/
```

This will generate an .html output file for each input file youve run.

Youll each be in charge of cleaning and visualizing the left (R1) and right (R2) files from **one population**. *(Lets choose these populations now)*.

Since were doing multiple samples from the same population, we want to be clever and process in a batch instead of manually one at a time. We can do this by writing a bash script that contains a loop. This is the first taste of how bash scripting is powerful!

The basic syntax of a bash loop is like this:

```         
cd <path to the input data>

for name in somefilelist
do
  command1 -options ${name} -moreOptions
  command2 -options ${name} -moreOptions
done
```

Note the use of variable assignment using \${}. We define the word `name` in the "for loop" as the variable of interest, and then call the iterations of it using \${name}. For example, we could use the wildcard character (\*) in a loop to call all files that include the ID code for your population and then pass those filenames in a loop to FastQC. Something like:

```         
cd /netfiles/ecogen/PopulationGenomics/fastq/red_spruce/

for file in <mypop>*fastq.gz

do

 fastqc ${file} -o ~/myresults/fastqc/

done
```

Lets write the above into a script using the Vim text editor at the command line. Type `vim` to get into the editor, then type "i" to enter INSERT mode. You can then type your script (remember to make necessary changes to the "mypop" population code). Lastly, to save the file and quit Vim, hit the ESCAPE key to get out of INSERT mode, followed by `:wq ~/myscripts/fastqc.sh`. This will save the file into your `myscripts/` folder in your home directory.

Back at the command line, you should be able to cd into the `myscripts` folder and `ll` to see your new script!

#### You may find that you need to change the permission on your script to make it executable. Do this using chmod u+x, which changes the permissions to give the user (you) permission to execute (x). Then give it a run!

```         
chmod u+x fastqc.sh    # makes the script "executable" by the "user"
bash fastqc.sh         # executes the script
```

Itll take just a couple of minutes per fastq file. Once youve got results, lets use Filezilla to transfer the folder `~/myresults/fastqc/` over to your laptop and into the `results` folder in your github repo. Once the file transfer is complete, go to where you saved your files *on your laptop* and try double-clicking one of the html outputs. It should open with a web browser.

*How does the quality look?*

Since we made some changes (added files) to our github repo, we should practice committing these and then pushing to GitHub!

### Clean using Trimmomatic{-}

Now that we have a sense of the quality of our data, and where we might need some trimming, our next step is to clean the reads by removing low quality bases and leftover Illumina sequence adapters from the sequences.

[Well use the Trimmomatic program](http://www.usadellab.org/cms/index.php?page=trimmomatic) to clean the reads for each file. The program is already installed on our server.

Since this job is a bit more complicated, weve provided a partially completed example script here: `/netfiles/ecogen/PopulationGenomics/scripts/trim_loop_Fa23.sh`

1.  `cd` over to the directory where the example script lives
2.  Open and edit the bash script using the program vim.
3.  Edit the file so that youre trimming the fastq files for the population assigned to you; remember to add annotations as notes to yourself!
4.  Save the file in vim to your home directory. To do this, hit <Esc> when youre finished making edits, then `:wq ~/myscripts/trim_loop_Fa23.sh` This will write (w) a new copy of the file over to your \~/myscripts folder and the quit (q) vim.
5.  Change the permissions on your script to make it executable, then run it! (examples below)

Trimmomatic needs both read pairs (i.e., the R1 and R2 files) given to it at the same time. This makes the use of name variables in the loop a bit tougher since we cant rely just on the population name (example, `2505`) but also need each individual name too (example, `2505_9_C`).

Well use a couple of tricks of variable coding and substitution to call the name of the R1 file, use its naming convention to define the name for the second (R2) file for that individual, and then create a "basename" that contains the unique identifier for that individual.

```         
    R2=${R1/_R1.fastq.gz/_R2.fastq.gz}   # defines the name for the second read in the pair (R2) based on knowing the R1 name (the file names are identical except for the R1 vs. R2 designation)
    f=${R1/_R1_fastq.gz/}   # creates a new variable from R1 that has the "_R1.fastq.gz" stripped off
    name=`basename ${f}`   # calls the handy "basename" function to define a new variable containing only the very last part of the filename while stripping off all the path information. This gets us the "2505_9_C" bit we want.
```

Trimmomatic performs the cleaning steps in the order they are presented. Its recommended to clip adapter early in the process and clean for length at the end.

The steps and options are [from the Trimmomatic website](http://www.usadellab.org/cms/index.php?page=trimmomatic):

```         
ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read.
LEADING: Cut bases off the start of a read, if below a threshold quality
TRAILING: Cut bases off the end of a read, if below a threshold quality
SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.
MINLEN: Drop the read if it is below a specified length
```

Once youre ready, go ahead and execute your bash script that you saved into your `~/myscripts/` folder:

```         
cd ~/myscripts/
bash trim_loop_Fa23.sh
```

Note that the output (the trimmed and cleaned reads) are going to get saved to a new folder your script will make for you: `~/mydata/cleanreads/`

### Visualize again using FastQC {-}

To check the quality of one of your cleaned files, you can use FastQC again. Try that on 1 or 2 files to see how things looks (no need to do them all).

You fill in the blank!  Don't forget to change your input path to the cleanreads/ directory!

Take notes on your workflow today so you can remember what you did for the future!

An important final step is taking good notes on your workflow so you can remebber what you did down the road (your "future self") and share your process with others (reproducible science!). Its also really important once you start making detailed decisions that will affect the analysis outcome of your data, so you can recreate the results and explore the effect of different assumptions/decisions.

We want you to keep such a notebook for each module in the course (kind of like you would for each experiment, or each thesis chapter you will work on). Weve provided you with a notebook template based on the markdown (md) language.

[Notebook template in markdown (md) format](https://github.com/PespeniLab/Ecological-Genomics/blob/0708da3bdd3a0bf27d80bdedade51d4aba12c933/Fall2023/GithubNotebookTemplate.md)

You should save a copy of this template to your github repo. You can then open it up and edit this file directly within RStudio, taking notes using either the `Soource` interface if you know the markdown language (or want to learn) or you can use RStudios built-in markdown GUI editor under the `Visual` tab. This works very similar to Word.

Heres a cheatsheet for markdown language if you want to write using `Source` code:

[Markdown cheatsheet](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)


## Population Genomics #2  {-}

### Population & Landscape Genomics {-}

### Learning Objectives for 09/13/23 {-}
1.  Review visualizing QC on our sequence reads
2.  Trim reads using `fastp` and assess pre- and post trimming
3.  Start mapping (a.k.a. aligning) each set of cleaned reads to a reference genome
4.  Visualize sequence alignment files
5.  Calculate mapping statistics to assess quality of the result

### Clean using `fastp`  {-}

Now that we have a sense of the quality of our data, and where we might need some trimming, our next step is to clean the reads by removing low quality bases and leftover Illumina sequence adapters from the sequences.

[Well use the `fastp` program](https://github.com/OpenGene/fastp) to clean the reads for each file. See also the [paper published by Chen in May 2023](https://onlinelibrary.wiley.com/doi/10.1002/imt2.107). Another very similar program for trimming is [Trimmomatic](http://www.usadellab.org/cms/index.php?page=trimmomatic). It works great, but `fastp` does everything Trimmomatic does but in only a faction of the time! The program is already installed on our server:

`/data/popgen/fastp`

This job will be bit more complicated, since we want you each to process all the samples for a given population.

Since were doing multiple samples from the same population, we want to be clever and process in a batch instead of manually one at a time. We can do this by writing a bash script that contains a loop. This is the first taste of how bash scripting is powerful!
    
    The basic syntax of a bash loop is like this:
    
    ```         
  cd <path to the input data>
    
    for FILE in somelist
  
  do
  
  command1 -options ${FILE} -moreOptions
  command2 -options ${FILE} -moreOptions
  
  done
  ```
  
  Note the use of variable assignment using \${}. We define the word `FILE` in the "for loop" as the variable of interest, and then call the iterations of it using \${FILE}. For example, we could use the wildcard character (\*) in a loop to call all files that include the ID code for your population and then pass those filenames in a loop to the commands. For example, if your pop was "2XXX", then you could write a loop that would process all the R1 fastq files in that population like this:
    
    ```         
  MYPOP="2XXX"
  
  cd <path to the input data>
    
    for FILE in ${MYPOP}*R1.fastq.gz
  
  do
  
  command1 -options ${FILE} -moreOptions
  command2 -options ${FILE} -moreOptions
  
  done
  ```
  
  Thats the basic idea! Lets do it...
  
  Weve provided a partially completed example script here:

`/netfiles/ecogen/PopulationGenomics/scripts/fastp.sh`

1.  Make a copy of the script `cp` and put it into your `~/myscripts` directory
2.  Open and edit your copied script using the program vim `vim ~/myscripts/fastp.sh`.
3.  Edit the file so that youre trimming the fastq files for the population code assigned to you; you can add annotations as notes to yourself using the hashtag (\#)
       4.  Save the file after youre done making changes. To do this, hit <Esc> to get out of "Insert" mode, then `:wq` This will write (w) the changes to your file and then quit (q) vim.

**NOTE:** `fastp` needs both read pairs (i.e., the R1 and R2 files) given to it at the same time. This makes the use of name variables in the loop a bit tougher since we cant rely just on the population name (example, `2505`) but also need each individual name too (example, `2505_9_C`).
Well use a couple of tricks to recycle the R1 file name to quickly create a mathcing R2 file name within the loop. Well do a simialr trick to quickly name the output files adding "\_clean" to the end. Well talk about this in class together, but there are also annotations in the script itself explaining whats happening.
Once youre ready, go ahead and execute your bash script that you saved into your `~/myscripts/` folder:

```         
cd ~/myscripts/
bash fastp.sh
```

Note that the output (the trimmed and cleaned reads) are going to get saved to a folder on the network drive that youve been given write access to: `/netfiles/ecogen/PopulationGenomics/fastq_red_spruce/cleanreads`
### Visualize pre- and post-trimming read quality with the `fastp` html output  {-}
Conveniently, `fastp` is not only fast, but also produces a summary of the change in quality pre- and post-trimming. Just like FastQC, the output is in an html file that we can use FileZilla to transfer back to our laptops and look at in a browser. Pretty cool!
## Mapping cleaned and trimmed reads against the reference genome {-}
Now that we have cleaned and trimmed read pairs, were ready to map them against the reference genome.

The first step of read mapping is downloading the reference genome, freely available from the developers at [plantgenie.org](https://plantgenie.org/FTP?dir=Data%2FPlantGenIE%2FPicea_abies): (https://plantgenie.org/themes/plantgenie/images/plantgeine_logo.png)

The *Picea abies* reference genome is based on Norway spruce (*P. abies*) and [published by Nystedt et al. (2013)](https://www.nature.com/articles/nature12211).

(https://media.springernature.com/lw750/springer-static/image/art%3A10.1038%2Fnature12211/MediaObjects/41586_2013_Article_BFnature12211_Figa_HTML.jpg)

You dont actually need to download the genome because we already have the file in the directory below. But for future reference `wget` is a useful command to ownload files from the web.
```         
cd /netfiles/ecogen/PopulationGenomics/ref_genome
wget "ftp://plantgenie.org:980/Data/PlantGenIE/Picea_abies/v1.0/fasta/GenomeAssemblies/Pabies01-genome.fa.gz"
```
Rather than trying to map to the entire 19.6 Gbp reference (yikes!), we first subsetted the *P. abies* reference to include **just the contigs that contain one or more probes** from our exome capture experiment. For this, we did a BLAST search of each probe against the *P. abies* reference genome, and then retained all scaffolds that had a best hit.
 -   This reduced reference contains:
 -   1,376,182,454 bp (\~1.38 Gbp) in 33,679 contigs
-   The mean (median) contig size is 10.5 (12.9) kbp
-   The N50 of the reduced reference is 101,375 bp
-   The indexed reduced reference genome to use for your mapping is on our server here:
 `/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa`
### To help make our scripting approach efficient, were going to write several short scripts, optimizing each one at a time, then put them together at the end  {-}
-   First, we want to specify the population of interest and the paths to the input and output directories. We can do this by defining variables in bash, like so:
-   Each student gets assigned a population to work with:
-   `MYPOP="XXXX"`
-   Directory with the cleaned fastq files
-   `INPUT="/netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads"`
-   Output dir to store mapping files (bam)
-   `OUT="/netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam"`
-   For mapping, well use the program [bwa](https://github.com/lh3/bwa), which is a very efficient and very well vetted read mapper. Lots of others exist and can be useful to explore for future datasets. We tried several, and for our exome data, bwa seems to be the best.

-   Lets write a bash script called `mapping.sh` that calls the R1 and R2 reads for each individual in your population, and uses the [`bwa-mem2`](https://github.com/bwa-mem2/bwa-mem2) algorithm to map reads to the reference genome. We can test this out using one sample (individual) at a time, and then once the syntax is good and the bugs all worked out, we can scale this up to all the inds in our populations.

The basic `bwa-mem2` command well use is below. Think about how we should write this into a loop to call all the fastq files for our population of interest...(hint, look back at the `fastp.sh` script)

```         
/data/popgen/bwa-mem2/bwa-mem2 mem -t 1 ${REF} ${READ1} ${READ2} > ${OUT}${NAME}.sam
```

where

```         
-t 1 is the number of threads, or computer cpus to use (in this case, just 1)
-${REF} specifies the path and filename for the reference genome
${READ1} specifies the path and filename for the cleaned and trimmed R1 reads 
${READ2} specifies the path and filename for the cleaned and trimmed R2 reads 
>${OUT}/${NAME}.sam  specifies the path and filenam for the .sam file to be saved into a new directory
```

-   Other bwa options detailed here: [bwa manual page](https://bio-bwa.sourceforge.net/bwa.shtml)

Because youre each mapping sequences from multiple samples (N=8/pop), its going to take a little while.

Whenever you have a job that will take a long time, youre going to want to start a "screen session" using the `tmux` command. `tmux` initiates a new shell window that wont interrupt or stop your work if you close your computer, log off the server, or leave the UVM network. Anytime youre running long jobs, you definitely want to use `tmux`.

Using it is easy. Heres what you need to do:

-   Type `tmux` and hit `<return>`

-   You are now in a "screen" --- meaning you can start a job and once you exit the screen it will keep running.

-   Start your program or run your bash script like so: `bash mapping.sh`

-   Youll see the program start running. - Assuming everything is working and youre not seeing errors, you want to now detach from the screen. To do so, hold the `<control>` key down while you hit the `b` key. Then release the `<control>` key and hit just the `d` key.

-   If you do it right, then you should exit your screen session and go back to a "regular" terminal. If you dont do this, youll lose your work!

-   You can recover your screen by typing `tmux attach`. Thatll re-attach you back to your program!
When your script is ready, do:
```         
tmux
bash mapping.sh
```
Dont forget to detach from your screen!

While thats running, lets take a look at a Sequence AlignMent (SAM) file already available in `/netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam/`

-   First, try looking at a SAM file using `head` and `tail`.

```         
tail -n 100 FILENAME.sam
```

A SAM file is a tab delimited text file that stores information about the alignment of reads in a FASTQ file to a reference genome or transcriptome. For each read in a FASTQ file, theres a line in the SAM file that includes

-   the read, aka. query, name,
-   a FLAG (number with information about mapping success and orientation and whether the read is the left or right read),
-   the reference sequence name to which the read mapped
-   the leftmost position in the reference where the read mapped
-   the mapping quality (Phred-scaled)
-   a CIGAR string that gives alignment information (how many bases Match (M), where theres an Insertion (I) or Deletion (D))
-   an =, mate position, inferred insert size (columns 7,8,9),
-   the query sequence and Phred-scaled quality from the FASTQ file (columns 10 and 11),
-   then Lots of good information in TAGS at the end, if the read mapped, including whether it is a unique read (XT:A:U), the number of best hits (X0:i:1), the number of suboptimal hits (X1:i:0).

The left (R1) and right (R2) reads alternate through the file. SAM files usually have a header section with general information where each line starts with the \@ symbol. SAM and BAM files contain the same information; SAM is human readable and BAM is in binary code and therefore has a smaller file size.

Find the official Sequence AlignMent file documentation can be found [here](https://en.wikipedia.org/wiki/SAM_(file_format)) or [more officially](https://samtools.github.io/hts-specs/SAMtags.pdf).

-   [Some useful FLAGs to know](http://seqanswers.com/forums/showthread.php?t=17314) - for example what do the numbers in the second column of data mean?

-   [Heres a SAM FLAG decoder](https://broadinstitute.github.io/picard/explain-flags.html) by the Broad Institute.

#### How can we get a summary of how well our reads mapped to the reference? {-}

-   We can use the program [sambamba](https://lomereiter.github.io/sambamba/) for manipulating sam/bam files. [sambamba](https://lomereiter.github.io/sambamba/). Sambamba is closely related to its progenitor program `samtools` which is written by the same scientist who develop `bwa`, Heng Li. `sambamba` has been re-coded to increase efficiency (speed).

-   First we have to convert the sam file to bam format:
  
  -   `sambamba view -S -f bam IN.sam -o OUT.bam`

-   Then we can use the command `flagstats` gets us some basic info on how well the mapping worked:
  
  -   `sambamba flagstat FILENAME.bam`

Lets talk about how our mapping went!


## Population Genomics #3 {-}
### Population & Landscape Genomics {-}

### Learning Objectives for 09/18/23 {-}
  
  1. Introduce lab notebooks
2. Visualize sequence alignment files (\*.sam)
3. Process the mapping file \*sam to binary (\*.bam), sort, and remove duplicate reads
4. Calculate mapping statistics to assess quality of the result

### 1. Lab Notebooks: Take notes on your workflow today so you can remember what you did for the future! {-}

An important step is taking good notes on your workflow so you can remember what you did down the road (your "future self") and share your process with others (reproducible science!). Its also really important once you start making detailed decisions that will affect the analysis outcome of your data, so you can recreate the results and explore the effect of different assumptions/decisions.

We want you to keep such a notebook for each module in the course (kind of like you would for each experiment, or each thesis chapter you will work on). Weve provided you with a notebook template based on the markdown (md) language.

[Notebook template in markdown (md) format](https://github.com/PespeniLab/Ecological-Genomics/blob/0708da3bdd3a0bf27d80bdedade51d4aba12c933/Fall2023/GithubNotebookTemplate.md)

You should save a copy of this template to your github repo and rename it `PopulationGenomics_Notebook.md` in your main repo folder. You can then open it up and edit this file directly within RStudio, taking notes using either the `Source` interface if you know the markdown language (or want to learn) or you can use RStudios built-in markdown GUI editor under the `Visual` tab. This works very similar to Word.

Heres a cheatsheet for markdown language if you want to write using `Source` code:
  
  [Markdown cheatsheet](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)

When youre done your entries, save and quit RStudio, then commit your changes and push to Github.  You can check out your notebook on your repos site, and Github will automatically render the markdown code into a nice format!
  
  ### 2. Visualize the mapping: By now, you should all have Sequence AlignMent (SAM) files for the inds in your populations! {-}
  
  -   Lets take a peek at one of the non-binary (sam) alignment files

`/netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam/`

-   First, try looking at a SAM file using `head` and `tail`. Pick one of your files (just one) to play with below:

```         
tail -n 2 YOURFILENAME.sam
```

A SAM file is a tab delimited text file that stores information about the alignment of reads in a FASTQ file to a reference genome or transcriptome. For each read in a FASTQ file, theres a line in the SAM file that includes

-   the read, aka. query, name,
-   a FLAG (number with information about mapping success and orientation and whether the read is the left or right read),
-   the reference sequence name to which the read mapped
-   the leftmost position in the reference where the read mapped
-   the mapping quality (Phred-scaled)
-   a CIGAR string that gives alignment information (how many bases Match (M), where theres an Insertion (I) or Deletion (D))
-   an '=', mate position, inferred insert size (columns 7,8,9),
-   the query sequence and Phred-scaled quality from the FASTQ file (columns 10 and 11),
-   then Lots of good information in TAGS at the end, if the read mapped, including whether it is a unique read (XT:A:U), the number of best hits (X0:i:1), the number of suboptimal hits (X1:i:0).

The left (R1) and right (R2) reads alternate through the file. SAM files usually have a header section with general information where each line starts with the '\@' symbol. SAM and BAM files contain the same information; SAM is human readable and BAM is in binary code and therefore has a smaller file size.

Find the official Sequence AlignMent file documentation can be found [here](https://en.wikipedia.org/wiki/SAM_(file_format)) or [more officially](https://samtools.github.io/hts-specs/SAMtags.pdf).

-   [Heres a SAM FLAG decoder](https://broadinstitute.github.io/picard/explain-flags.html) by the Broad Institute.

-   [You can look up TAGs specific to `bwa mem` mapping here](https://bio-bwa.sourceforge.net/bwa.shtml)


### 3. Process our mapping files using samtools and sambamba {-}

-   We can use the program [sambamba](https://lomereiter.github.io/sambamba/) for manipulating alignment (sam/bam) files. [sambamba](https://lomereiter.github.io/sambamba/). Sambamba is closely related to its progenitor program `samtools` which is written by the same scientist who develop `bwa`, Heng Li. `sambamba` has been re-coded to increase efficiency (speed).

- There are several steps we need to do: 
  
  -  convert sam alignment file to (binary) bam format
-  sort the bam file by its read coordinates
-  mark and remove PCR duplicate reads
-  index the sorted, duplicate removed alignment for quick lookup

- Heres a script that we can customize for the above jobs: `/netfiles/ecogen/PopulationGenomics/scripts/process_bams.sh` 

- Copy that script into your `~/myscripts` folder and open it in `vim` to edit

- When youre ready, enter a screen session using `tmux` then execute your script `bash process_bams.sh`. If you get an error that the script doesnt exist, then either cd into the `~/myscripts` directory before running your script, or incorporate the path into the file name when you give your bash command.

- Detach from the screen using `<CTRL>`+`b` then `d`. You can always reattach by `tmux attach-session`


### 4. Calculate mapping stats: How can we get a summary of how well our reads mapped to the reference?  {-}

- We can use the program [samtools](https://github.com/samtools/samtools) Written by Heng Li, the same person who wrote bwa. It is a powerful tool for manipulating sam/bam files.  

- The samtools command `flagstat` gets us some basic info on how well the mapping worked

- We can also estimate depth of coverage (avg. number of reads/site) using the samtools command `depth`

- Well use both of these commands in loops to assess the mapping stats on each sample in our population.

- Well also use the `awk` tool to help format the output.  

- Heres a script to get us started:  `/netfiles/ecogen/PopulationGenomics/scripts/bam_stats.sh`


If theres time while thats running, we can take a look at one of our alignment files (sam or bam) using an integrated viewed in samtools called `tview`.  To use it, simply call the program and command, followed by the sam/bam file you want to view and the path to the reference genome.  For example:

```
samtools tview /netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam/XXXX_XX_X.sorted.rmdup.bam /netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa
```

## Population Genomics #4 {-}


### Population & Landscape Genomics {-}
  ##### Learning Objectives for 09/20/23  {-}
  
1.  Finish calculating mapping statistics to assess quality of the result
2.  Introduce use of genotype-likelihoods for analyzing diversity in low coverage sequences
3.  Use the 'ANGSD' program to calculate nucleotide diversity (thetas) and neutrality stats

### 1. Calculate mapping stats: How can we get a summary of how well our reads mapped to the reference?  {-}

- We can use the program [samtools](https://www.htslib.org/doc/samtools.html) Written by Heng Li, the same person who wrote bwa. It is a powerful tool for manipulating sam/bam files.  

- The samtools command `flagstat` gets us some basic info on how well the mapping worked

- We can also estimate depth of coverage (avg. number of reads/site) using the samtools command `depth`

- Well use both of these commands in loops to assess the mapping stats on each sample in our population.

- Well also use the `awk` tool to help format the output. [(awk cheatsheet here)](https://quickref.me/awk.html)

- Heres a script to get us started:  `/netfiles/ecogen/PopulationGenomics/scripts/bam_stats.sh`


### 2. Inference of population genomics from the aligned sequence data: should we call genotypes? {-}

Many of the papers youll read that do popgen on NGS data have a SNP calling step that results in a specific gneotype being called for each SNP site for each individual. For example,

| SNP | Ind1 | Ind 2 |
  |-----|------|-------|
  | 1   | CC   | CT    |
  | 2   | AG   | AA    |
  | 3   | GT   | TT    |
  
  But how do we know that Ind1 is homozygous at SNP-1 (CC) -- couldnt it be CT and we just didnt have enough coverage to observe the second allele?
  
  The basic problem is that read data are counts that produce a binomial distribution of allele calls at a given site, and if you have few reads, you might by chance not observe the true genotype. So, whats the right thing to do?

As with almost anything in statistics, the right thing to do is not throw away that uncertainty, but instead incorporate it into your analysis. That's what we're going to do...

### Genotype-free population genetics using genotype likelihoods {-}

A growing movement in popgen analysis of NGS data is embracing the use of genotype likelihoods to calculate stats based on each individual having a likelihood (probability) of being each genotype.

**A genotype likelihood (GL) is essentially the probability of observing the sequencing data (reads containing a particular base), given the genotype of the individual at that site.**

These probabilities are modeled explicitly in the calculation of population diversty stats like Theta-pi, Tajimas D, Fst, PCA, etc...; thus not throwing out any precious data, but also making fewer assumptions about the true (unknown) genotype at each locus

-   Were going to use this approach with the program 'ANGSD', which stands for 'Analysis of Next Generation Sequence Data'

-   This approach was pioneered by Rasmus Nielsen, published originally in [Korneliussen et al. 2014](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-014-0356-4).

-   [ANGSD has a user's manual (it's a work in progress...)](http://www.popgen.dk/angsd/index.php/ANGSD)


#### The basic work flow of ANGSD goes like this: {-}

1.  Create a list of bam files for the samples you want to analyze
2.  Estimate genotype likelihoods (GL's) and allele frequencies after filtering to minimize noise
3.  Use GL's to:
    (a) estimate the site frequency spectrum (SFS)

    (b) estimate nucleotide diversities and neutrality stats (Thetas, Tajimas D, ...)


#### 1. In your `~/myscripts` folder, enter vim and create a file called `ANGSD.sh`  {-}

Create the header for your inputs and outputs

```         
mkdir ~/myresults/ANGSD

INPUT=""

OUTPUT=~/myresults/ANGSD

REF="/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa"

MYPOP=""

ls ${INPUT}/${MYPOP}*sorted.rmdup.bam >${OUTPUT}/${MYPOP}_bam.list

```
Write (w) and quit(q) your file and try running it at the command line.

*Check your output bamlist to see it was made properly!* 
  
  -  Where would you look for this file? (Hint, refer back to the `ls` command that makes it). 
-  How would you verify its contents? (hint: use `head` or `cat` or even `vim`)


#### 2. Open your `ANGSD.sh` script back up in `vim` {-}

Estimate your GLs and allele freqs, optionally filtering for base and mapping quality, sequencing depth, SNP probability, minor allele frequency, etc.

Add the following code chunk at the bottom of your script:

```         
# File suffix to distinguish analysis choices
SUFFIX=""

# Estimating GL's and allele frequencies for all sites with ANGSD

######################

ANGSD -b ${OUTPUT}/${MYPOP}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${OUTPUT}/${MYPOP}_${SUFFIX} \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-GL 1 \
-doSaf 1 \
##### below filters require `do-Counts`
#-doCounts 1 \
#-minInd 4 \
#-setMinDepthInd 1 \
#-setMaxDepthInd 40 \
#-setMinDepth 10 \
#-skipTriallelic 1 \
#-doMajorMinor 1 \
##### below filters require `doMaf`
#-doMaf 1 \
#-SNP_pval 1e-6 \
#-minMaf 0.01
```

What do all these options mean?
  
  | Option| Description     |
  |--------------------|---------------------------------------------------------------------|
  | -nThreads 1        | how many cpus to use -- be conservative   |
  | -remove_bads 1     | remove reads flagged as 'bad' by samtools |
  | -C 50 | enforce downgrading of map quality if contains excessive mismatches |
  | -baq 1| estimates base alignment qualities for bases around indels          |
  | -minMapQ 20        | threshold for minimum read mapping quality (Phred)     |
  | -minQ 20           | threshold for minimum base quality (Phred)|
  | -GL 1 | calculate genotype likelihoods (GL) using the Samtools formula      |
  | -doSaf 1           | output allele frequency likelihoods for each site      |
  | -doCounts 1        | output allele counts for each site        |
  | -minInd 4          | min number of individuals to keep a site (see also ext 2 filters)   |
  | -setMinDepthInd 1  | min read depth for an individual to count towards a site            |
  | -setMaxDepthInd 40 | max read depth for an individual to count towards a site            |
  | -setMinDepth 10    | min read depth across ALL individual to keep a site    |
  | -skipTriallelic 1  | dont use sites with \>2 alleles          |
| -doMajorMinor 1    | fix major and minor alleles the same across all samples|
| -doMaf 1           | calculate minor allele frequencies        |
| -SNP_pval 1e-6     | Keep only site highly likely to be polymorphic (SNPs)  |
| -minMaf 0.01       | Keep only sites with minor allele freq > some proportion.           |

**NOTES**

-   If you want to restrict the estimation of the genotype likelihoods to a particular set of sites youre interested in, add the option `-sites selected_sites.txt` (tab delimited file with the position of the site in column 1 and the chromosome in column 2) or use `-rf selected_chromosome.chrs` (if listing just the unique "chromosomes" or contigs you want to anlayze)
-   Some popgen stats you want to estimate only the polymorphic sites; for this you should include the `-SNP_pval 1e-6` option to eliminate monomorphic sites when calculating your GLs
-   There are good reasons to do it BOTH ways, with and without the `-SNP_pval 1e-6` option. Keeping the monomorphic sites is essential for getting proper estimates of nucleotide diversity and Tajimas D. But other analyses such as PCA or GWAS want only the SNPs.

**Write (q) and quit (q) your script. Then run at the command line**
  
  #### 3a. In your `~/myscripts` folder, create `ANGSD_doTheta.sh` to estimate the SFS and nucleotide diversity stats for your pop  {-}
  
  Based on the saf.idx files from ANGSD GL calls, you can estimate the Site Frequency Spectrum (SFS), which is the precursor to many other analyses such as nucleotide diversities (as well as Fst, demographic history analysis, etc.)

```         
REF="/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa"

OUT=~/myresults/ANGSD

MYPOP=""

SUFFIX=""

#Estimation of the SFS for all sites using the FOLDED SFS
realSFS ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
-maxIter 1000 \
-tole 1e-6 \
-P 1 \
> ${OUT}/${MYPOP}_${SUFFIX}.sfs

```

#### 3b. After the SFS, estimate the theta diversity stats:  {-}

Once you have the SFS, you can estimate the thetas and neutrality stats by adding the following code chunk at the end of your `ANGSD_doTheta.sh` script:
  
  ```         
# Estimate thetas and stats using the SFS

realSFS saf2theta ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
-sfs ${OUT}/${MYPOP}_${SUFFIX}.sfs \
-outname ${OUT}/${MYPOP}_${SUFFIX}

thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx

```
If we wanted to analyze this on sliding windows, we could instead replace the above code chunk with the following:
  
  ```
# For sliding window analysis:

thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx \
-win 50000 \
-step 10000 \
-outnames ${OUT}/${MYPOP}_${SUFFIX}.thetasWindow.gz
```


For either of the results files above, the first column of the results file (*.thetas.idx.pestPG) is formatted a bit funny and we dont really need it. We can use the `cut` command to get rid of it:

```         
cut -f2- ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx.pestPG > ${OUT}/${MYPOP}_${SUFFIX}.thetas
```

This is now ready to bring into R to look at the mean and variability in nucleotide diversity for our pop. How does it compare to others?

We can compare the diversity in our different pops by entering your diversity stats in this google doc:

https://docs.google.com/spreadsheets/d/1y3GMvnGP65fYfBoJsdrixGLkGCe_TcDlo_7GFyPe1QM/edit?usp=sharing

## Population Genomics #5 {-}

### Population & Landscape Genomics {-}
#### Learning Objectives for 09/25/23 {-}

1. Calculate diversity stats for our focal pops (SFS, theta-W, theta-Pi, Tajimas D)
2. Summarize the results in R and share to google doc
3. Introduce Fst in ANGSD using genotype probabilities


### 1. Calculate SFS and diversity stats {-}

At the end of our last session, we used ANGSD to estimate genotype likelihoods for our red spruce populations. We wrote the script `ANGSD.sh` to work on these, which should have the following output files in your `~/myresults/ANGSD` directory:
  
  ```
-rw-r--r--. 1 kellrlab users 1417463962 Sep 20 11:44 9999_.saf.gz
-rw-r--r--. 1 kellrlab users    1086161 Sep 20 11:44 9999_.saf.idx
-rw-r--r--. 1 kellrlab users   79501025 Sep 20 11:44 9999_.saf.pos.gz
```

These "saf" files contain "site allele frequency" likelihoods, and are the info needed to estimate stats that depend on population allele frequencies, like nucleotide diversities, neutrality stats like Tajimas D, and population divergence stats like Fst. Each of these stats depends on the *SFS -- the Site Frequency Spectrum*
  So, our workflow will be to use the .saf files to estimate the SFS, and then use the SFS to estimate our diversity stats and Fst.

In your `~/myscripts` folder, create `ANGSD_doTheta.sh` to estimate the SFS and nucleotide diversity stats for your pop
Based on the saf.idx files from ANGSD GL calls, first estimate the Site Frequency Spectrum (SFS)

```         
REF="/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa"
OUT=~/myresults/ANGSD
MYPOP=""
SUFFIX=""
#Estimation of the SFS for all sites using the FOLDED SFS
realSFS ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
-maxIter 1000 \
-tole 1e-6 \
-P 1 \
-fold 1 \
> ${OUT}/${MYPOP}_${SUFFIX}.sfs
```

#### After the SFS, estimate the theta diversity stats:  {-}

Once you have the SFS, you can estimate the thetas and neutrality stats by adding the following code chunk at the end of your `ANGSD_doTheta.sh` script:
  ```         
# Estimate thetas and stats using the SFS

realSFS saf2theta ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
-sfs ${OUT}/${MYPOP}_${SUFFIX}.sfs \
-outname ${OUT}/${MYPOP}_${SUFFIX}

thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx

```
If we wanted to analyze this on sliding windows, we could instead replace the above code chunk with the following:
  For sliding window analysis:
  ```


thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx \
-win 50000 \
-step 10000 \
-outnames ${OUT}/${MYPOP}_${SUFFIX}.thetasWindow.gz
```
An important distinction! **The unfolded vs. folded SFS**
  
  The big difference here is whether we are confident in the ancestral state of each variable site (SNP) in our dataset
If we know the ancestral state, then the best info is contained in the **unfolded** SFS, which shows the frequency histogram of how many derived loci are rare vs. common
* bins in the **unfolded** SFS go from 0 to 2N -- why?
  When you dont know the ancestral state confidently, you can make the SFS based on the minor allele (the less frequent allele; always < 0.5 in the population).  

* bins in the **folded** SFS go from 0 to 1N -- why?
  
  Essentially, the folded spectra wraps the SFS around such that high frequency "derived" alleles are put in the small bins (low minor allele freq).

(https://i2.wp.com/evomics.org/wp-content/uploads/2018/01/fig5.png?resize=800%2C571)

#### Now we have some diversity results! {-}

For either of the results files above, the first column of the results file (*.thetas.idx.pestPG) is formatted a bit funny and we dont really need it. There are also a bunch of other neutrality stats that we dont need right now. We can use the `cut` command to get rid of these extra columns. The below `cut` command retains columns 2-5, 9 and 14 


```         
cut -f2-5,9,14 ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx.pestPG > ${OUT}/${MYPOP}_${SUFFIX}.thetas
```


The columns correspond to the following stats:
  
  | Col | Statistic    | Description        |
  |-----|--------------|-------------------------------------------------------------------------------------|
  | 2   | Chr          | The chromosome or (more appropriately) contig being analyzed           |
  | 3   | WinCenter    | The center of the contig, in bp |
  | 4   | tW           | Wattersons Theta -- an estimate of nucleotide diversity based on segregating sites |
  | 5   | tP           | Theta Pi -- estimate of nucleotide diversity based on pairwise divergence           |
  | 9   | Tajima       | Tajimas D -- a neutrality stat that tests for the difference in tW-tP |
  | 14  | nSites       | The number of base pairs being analyzed along this stretch of cont.    |
  
  
  ### 2. Summarize diversity stats in R {-}
  
  Were now ready to use `Filezilla` to download these 2 files:
  1. .thetas.idx.pestPG 
2. .sfs
Save these to your `results` folder in your githuib repo so we can import into R to look at the mean and variability in nucleotide diversity for our pop. Heres some basic R code to help you along:
  
  
  
  ```
setwd("") # set your path to your results folder in your repo where you saved your diversity stats file

list.files() # list out the files in this folder to make sure youre in the right spot.
```
First lets read in the diversity stats
```
theta <- read.table("_.thetas",sep="\t",header=T)
```

scales the theta-W by the number of sites
```
theta$tWsite = theta$tW/theta$nSites 
```
scales the theta-Pi by the number of sites
```
theta$tPsite = theta$tP/theta$nSites 
summary(theta)
```
You can order the contig list to show you the contigs with the highest values of Tajimas D, or the lowest
top 10 Tajimas D values
bottom 10 Tajimas D values
You can also look for contigs that have combinations of high Tajimas D and low diversity. these could represent outliers for selection

Be sure to replace "9999" with your pop code in the "main" legend below

Put the nucleotide diversities, Tajimas D, and SFS into a 4-panel figure

To reset the panel plotting, execute the line below:



```
<!-- head(theta[order(theta$Tajima, decreasing = TRUE),])  -->
  <!--  head(theta[order(theta$Tajima, decreasing = FALSE),])  -->
  
  
  <!-- #theta[which(theta$Tajima>1.5 & theta$tPsite<0.001),] -->
  <!-- sfs<-scan(9999_.sfs) -->
  
  <!-- sfs<-sfs[-c(1,which(sfs==0))] -->
  <!-- sfs<-sfs/sum(sfs) -->
  
  <!-- barplot(sfs,xlab="Chromosomes", -->
    <!-- names=1:length(sfs), -->
    <!-- ylab="Proportions", -->
    <!-- main="Pop 9999 Site Frequency Spectrum", -->
    <!-- col=blue) -->
  
  <!-- par(mfrow=c(2,2)) -->
  <!-- hist(theta$tWsite, xlab="theta-W", main="Wattersons theta") -->
  <!-- hist(theta$tPsite, xlab="theta-Pi", main="Pairwise Nucleotide Diversity") -->
  <!-- hist(theta$Tajima, xlab="D", main="Tajimas D") -->
  <!-- barplot(sfs,names=1:length(sfs),main=Site Frequency Spectrum) -->
  <!-- dev.off() -->
  ```
We can compare the diversity in our different pops by entering your diversity stats in this google doc:
  https://docs.google.com/spreadsheets/d/1y3GMvnGP65fYfBoJsdrixGLkGCe_TcDlo_7GFyPe1QM/edit?usp=sharing
### 3. Use ANGSD and the SFS for multiple pops to calculate genetic divergence between pops (Fst)  {-}

We can calculate Fst between any pair of populations by comparing their SFS to each other.  For this, well need to estimate the SFS for pairs of populations; we can each contribute to the overall analysis by looking at how our focal pop is divergent from the others.

* For this analysis, lets calculate Fst between our focal red spruce population (MYPOP) and the black spruce samples...this could tell us which of our pops might be hybridizing.  What would we expect for Fst in this case?
  * Lets write a bash script called ANGSD_Fst.sh that includes the following code:
  
  ```
Start with the usual bash script header

Give yourself some notes

Path to Black Spruce (BS) input saf.idx data:
  
  BLKSPR="/netfiles/ecogen/PopulationGenomics/fastq/black_spruce/cleanreads/bam/ANGSD"

#Path to save your output:

OUTPUT=
  
  MYPOP=""

SUFFIX=""

Estimate Fst between my red spruce pop and black spruce:
  
  realSFS ${MYPOP}_.saf.idx ${BLKSPR}/BS_all.saf.idx -P 1 >${MYPOP}_BS.sfs
realSFS fst index ${MYPOP}_.saf.idx ${BLKSPR}/BS_all.saf.idx -sfs ${MYPOP}_BS.sfs -fstout ${MYPOP}_BS -whichFst 1
realSFS fst stats ${MYPOP}_BS.fst.idx

```


## Population Genomics #6 {-}


### Population & Landscape Genomics  {-}

#### Learning Objectives for 09/27/23 {-}

1. Review the diversity stats for our focal pops on the google doc
2. Estimate genetic differentiation (Fst) in ANGSD between our focal red spruce pops and black spruce
3. Visualize population structure using PCA and Admixture


### 1. Review the diversity stats {-}

Lets compare the diversity in our different pops on the google doc:
  
  https://docs.google.com/spreadsheets/d/1y3GMvnGP65fYfBoJsdrixGLkGCe_TcDlo_7GFyPe1QM/edit?usp=sharing

I also put map in there for reference so you can see where different pops are located within the range.

-  *What do you notice about the diversities?*
  -  *Where is Ne the highest/lowest?*
  -  *What do the average Tajimas D values suggest about demographic history in these pops?*
  
  ### 2. Use ANGSD and the SFS for multiple pops to calculate genetic divergence between pops (Fst)
  
  We can calculate Fst between any pair of populations by comparing their SFS to each other.  For this, well need to estimate the SFS for pairs of populations; we can each contribute to the overall analysis by looking at how our focal pop is divergent from the others.

* For this analysis, lets calculate Fst between our focal red spruce population (MYPOP) and the black spruce samples...this could tell us which of our pops might be hybridizing.  What would we expect for Fst in this case?
  
  * Lets write a bash script called ANGSD_Fst.sh that includes the following code:
  
  ```
Start with the usual bash script header

Give yourself some notes

Path to Black Spruce (BS) input saf.idx data:
  
  BLKSPR="/netfiles/ecogen/PopulationGenomics/fastq/black_spruce/cleanreads/bam/ANGSD"

OUTPUT=
  
  MYPOP=""

cd ${OUTPUT}

Estimate Fst between my red spruce pop and black spruce:
  
  realSFS ${MYPOP}_.saf.idx \
${BLKSPR}/BS_all.saf.idx \
-P 1 \
>${MYPOP}_BS.sfs

realSFS fst index \
${MYPOP}_.saf.idx \
${BLKSPR}/BS_all.saf.idx \
-sfs ${MYPOP}_BS.sfs \
-fstout ${MYPOP}_BS \
-whichFst 1

realSFS fst stats ${MYPOP}_BS.fst.idx 

```

- Enter the *weighted* Fst value into the google sheet for your pop. Whats the trend?
  
  ### 3. Visualize popualtion structure across the landscape using PCA and Admixture 
  
  We often want to visualize differences in the genetic structure or genetic ancestry in our sample, and lots of papers weve read approach this using PCA or Admixture analysis. We can do each of these approaches on genotype likelihoods in ANGSD using a special routine called `pcANGSD`. 

`pcANGSD` uses a really cool iterative approach where it refines the estimation of allele frequencies for each individual *at the same time* that it finds the clusters that individual may have ancestry within.

(http://www.popgen.dk/software/images/d/d9/Pcangsd_admix.gif)

Here are some resources to understand the program options:
  
  * The [manual page](http://www.popgen.dk/software/index.php/PCAngsd) 

* A nice [pcANGSD tutorial](http://www.popgen.dk/software/index.php/PCAngsdTutorial) that walks through most of the routines

* The original paper describing the application to PCA and admixture are here: [Meisner & Albrechtsen 2019, *Genetics*](https://academic.oup.com/genetics/article/210/2/719/5931101?login=true)

Since we want to run `pcANGSD` for the entire set of samples -- not just your focal pop -- we need the genotype likelihoods from ANGSD for all 95 red spruce samples. That would take a long time to run (about 24 hrs) and would be redundant for each of you to do, so I ran these once for the class.

For your reference (and future work), the code I used to estimate the genotype likelihoods is here: (you dont have to run this now!)

`/netfiles/ecogen/PopulationGenomics/scripts/ANGSD_allRS_poly.sh`

and exported the genotype likelihoods in "beagle" format here:
  
  `/netfiles/ecogen/PopulationGenomics/ANGSD/allRS_poly.beagle.gz`

We can use the beagle file containing the genotype likelihoods for all 95 red spruce samples as input to `pcANGSD`.  The script is actually not too bad...lets give it a go:
  
  ```
Start with the usual bash script header

Give yourself some notes

Path to your input data (where the beagle file lives)

INPUT=
  
  Path to save your output (in your home directory):
  
  OUTPUT=
  
  SUFFIX="allRS_poly"

Make a copy of the list of bam files for all the red spruce samples and place in your home directory. Youll need this later for making figures.

cp ${INPUT}/allRS_bam.list ${OUTPUT}


To run pcANGSD, you need to activate a "virtual environment" on the server by including the line below:
  
  source /data/popgen/pcangsd/venv/bin/activate  


Then, run PCA and admixture scores with pcangsd:
  
  pcangsd -b ${INPUT}/${SUFFIX}.beagle.gz \
-o ${OUTPUT}/${SUFFIX} \
-e 1 \
--admix \
--admix_alpha 50 \
--threads 1 

```

This will run pcANGSD assuming it fits a single "eigenvalue" to split your samples into K=2 clusters. If you want to explore higher levels of clustering in the future, you can include the `-e <int>` flag, where <int> is a number that is K-1 number of clusters you want to fit.

Once the run is finished, use `FileZilla` to transfer the following files over to your repo on your laptop:
  
  * allRS_bam.list
* allRS_poly.cov
* allRS_poly.admix.2.Q

When you have these files transferred (dont forget where you saved them to on your laptop!), open up RStudio and lets start making some figures!
  
  Just a reminder, the following is R code, not bash. ;)

```
library(ggplot2) # plotting
library(ggpubr) # plotting

setwd("") # set the path to where you saved the pcANGSD results on your laptop

First, lets work on the genetic PCA:
  
  COV <- as.matrix(read.table("allRS_poly.cov")) # read in the genetic covariance matrix

PCA <- eigen(COV) # extract the principal components from the COV matrix

How much variance is explained by the first few PCs?
  
  var <- round(PCA$values/sum(PCA$values),3)

var[1:3]

A "screeplot" of the eigenvalues of the PCA:
  
  barplot(var, 
          xlab="Eigenvalues of the PCA", 
          ylab="Proportion of variance explained")

Bring in the bam.list file and extract the sample info:
  
  names <- read.table("allRS_bam.list")
names <- unlist(strsplit(basename(as.character(names[,1])), split = ".sorted.rmdup.bam"))
split = strsplit(names, "_")
pops <- data.frame(names[1:95], do.call(rbind, split[1:95]))
names(pops) = c("Ind", "Pop", "Row", "Col")

A quick and humble PCA plot:
  
  plot(PCA$vectors[,1:2],
       col=as.factor(pops[,2]),
       xlab="PC1",ylab="PC2", 
       main="Genetic PCA")

A more beautiful PCA plot using ggplot :)

data=as.data.frame(PCA$vectors)
data=data[,c(1:3)]
data= cbind(data, pops)

cols=c("#377eB8","#EE9B00","#0A9396","#94D2BD","#FFCB69","#005f73","#E26D5C","#AE2012", "#6d597a", "#7EA16B","#d4e09b", "gray70")

ggscatter(data, x = "V1", y = "V2",
          color = "Pop",
          mean.point = TRUE,
          star.plot = TRUE) +
  theme_bw(base_size = 13, base_family = "Times") +
  theme(panel.background = element_blank(), 
        legend.background = element_blank(), 
        panel.grid = element_blank(), 
        plot.background = element_blank(), 
        legend.text=element_text(size=rel(.7)), 
        axis.text = element_text(size=13), 
        legend.position = "bottom") +
  labs(x = paste0("PC1: (",var[1]*100,"%)"), y = paste0("PC2: (",var[2]*100,"%)")) +
  scale_color_manual(values=c(cols), name="Source population") +
  guides(colour = guide_legend(nrow = 2))




Next, we can look at the admixture clustering:
  import the ancestry scores (these are the .Q files)

q <- read.table("allRS_poly.admix.2.Q", sep=" ", header=F)

K=dim(q)[2] #Find the level of K modeled

order according to population code
ord<-order(pops[,2])

make the plot:
  barplot(t(q)[,ord],
          col=cols[1:K],
          space=0,border=NA,
          xlab="Populations",ylab="Admixture proportions",
          main=paste0("Red spruce K=",K))
text(tapply(1:nrow(pops),pops[ord,2],mean),-0.05,unique(pops[ord,2]),xpd=T)
abline(v=cumsum(sapply(unique(pops[ord,2]),function(x){sum(pops[ord,2]==x)})),col=1,lwd=1.2)

```

### What have we learned about the genetic structure from the PCA and admixture plots?  {-}

- *What does the PCA seem to be telling us?*
  
  - *What different picture does the admixture plot reveal?  How does it relate to the PCA?*
  
  - *Would we want to look for higher levels of K in the admixture analysis?  How do we do that??*
  
  For reference, heres a map of the sample sites within the red spruce range:
  (https://raw.githubusercontent.com/stephenrkeller/Ecological_Genomics/master/Fall_2023/pics/colebrookSourcesMap.jpeg?token=GHSAT0AAAAAACGK5AYSCMT2SZDBWWK4JTZKZITJOWQ)





## Population Genomics #7 {-}

### Population & Landscape Genomics {-}
### Learning Objectives for 10/02/23 {-}

1. Review the population structure results
2. Perform a scan for selection and identify contigs with outlier loci
3. Identify and visualize outlier loci 


### 1. What have we learned about the genetic structure from the PCA and admixture plots? {-}

Lets take a look at the PCA and admixture figures posted to the Slack #coding channel.

- *What does the PCA seem to be telling us?*
  
  - *What different picture does the admixture plot reveal?  How does it relate to the PCA?*
  
  - *Would we want to look for higher levels of K in the admixture analysis?  How do we do that??*
  
  
  #### 2. How has selection acted to drive divergence of individual loci in excess of the population structure weve observed? {-}
  
  When selection acts in response to local environmental conditions, we observe an excess of population structure at certain loci. This can be thought of as Fst at a single locus exceeding some background level of divergence that exists across the genome as a whole. We call these "Fst outliers" and identifying these outliers is a major goal in ecological genomic studies. But, a major challenge is how best to control for background population structure -- in other words, how can we sift out the "outliers" from the rest of the population structure in the genome?
  
  * One approach to identifying Fst outliers is using genetic PCA to (a) identify the major axes of population structure, and then (b) find loci that "load" very strongly on these axes, indicating their exceptional divergence is probably driven by the action of selection in excess of genetic drift.

* We can run a scan for Fst outliers using `pcANGSD`, just like we did for the genetic PCA, but here we initiate a selection scan for loci that are exceptionally divergent along one or more of the inferred genetic PC axes.  This method is especially helpful when its hard to define what are genetic "populations", since it uses the genetic PCA to infer the genetic structure directly.  The method employed in `pcANGSD` follows the "FastPCA" selection scan method described by [Galinsky et al. 2015](https://www.cell.com/ajhg/fulltext/S0002-9297(16)00003-3) 

* The approach essentially uses the SNP weights or "loadings" on a given genetic PC axis to determine the strength of association, and if it exceeds the expectation due to neutral drift.

* Lets write a bash script called `pcANGSD_selection.sh` that includes the following code. Recall that `pcANGSD` uses genotype likelihoods in "beagle" format, which have been calculated for you [(see last tutorial)](https://pespenilab.github.io/Ecological-Genomics/Fall2023/tutorials/2023-09-27_PopGenomics_Day6.html) and can be found here:
  `/netfiles/ecogen/PopulationGenomics/ANGSD/`

Heres a start to our script:
  
  ```
INPUT=""

OUTPUT=
  
  SUFFIX="allRS_poly"

cp ${INPUT}/allRS_bam.list ${OUTPUT}

Activates the pcANGSD environment and run the selection scan:
  
  source /data/popgen/pcangsd/venv/bin/activate

pcangsd -b ${INPUT}/${SUFFIX}.beagle.gz \
-o ${OUTPUT}/${SUFFIX} \
-e <what number should we use here?>
  --selection \
--minMaf 0.05 \
--sites_save \
--snp_weights \
--threads 1

```
Note the `minMaf 0.05` option tests only those loci that have a minor allele frequency of 0.05 of greater. Thats because theres little statistical power for testing association of alleles that are very rare (<5% in the sample).

It should run quickly! Once finished, youll the see a new set of files, including:
  
  | File name | contents |
  |--------|-------------------|
  | allRS_poly.selection.npy  | selection scores for each locus on each tested PC axis |
  | allRS_poly.weights.npy | weights that show how strongly each locus "loads" on each PC axis |
  | allRS_poly.sites | for each locus shows whether it got tested (1) or not (0)  |
  
  Youll also want to get a file with minor allele frequencies ("maf") that ANGSD makes when first estimating genotype likelihoods. You have one of these files from when you worked on your focal pops, but we need one for the "allRS_poly" run of all red spruce individuals for just polymorphic sites.  Ive provided this here:
  
  `/netfiles/ecogen/PopulationGenomics/ANGSD/allRS_poly.mafs.gz`

We need to combine the "mafs" file and the "sites" file to create a file that has the contig and position info for each locus that got tested for selection.  Heres a few lines of bash code to create this file:
  
  1. First, we need to add a header to the "sites" file:
  
  `sed -i 1i kept_sites ~/myresults/ANGSD/allRS_poly.sites`

2. Next, we need to get the contig and position info out of the "mafs" file and combine it with the "sites" file:
  
  `zcat /netfiles/ecogen/PopulationGenomics/ANGSD/allRS_poly.mafs.gz | cut -f1-7 | paste - ~/myresults/ANGSD/allRS_poly.sites >allRS_poly_mafs.sites`

Now we have all the files we need!  Be sure to transfer the 3 listed above, plus the new `allRS_poly_mafs.sites` you just made, using **FileZilla** and save into your repo on your laptop.  When you have these files transferred (dont forget where you saved them to on your laptop!), open up **RStudio** and lets start making some figures!
  
  
  
  ### 3. Idenitfy and visualize outlier loci {-}
  
  Well use R to import the selection scan results and associated meta-data, assign p-values for each tested locus, and visualize the results!  
  
  Just a reminder, the following is R code, not bash. ;)

```

###################################
Selection scans for red spruce #
###################################

library(RcppCNPy) # for reading python numpy (.npy) files

setwd("~/Documents/Github/Ecological_Genomics/Fall_2023/pcangsd/")

list.files()

read in selection statistics (these are chi^2 distributed)

s<-npyLoad("allRS_poly.selection.npy")

convert test statistic to p-value
pval <- as.data.frame(1-pchisq(s,1))
names(pval) = "p_PC1"

read positions
p <- read.table("allRS_poly_mafs.sites",sep="\t",header=T, stringsAsFactors=T)
dim(p)

p_filtered = p[which(p$kept_sites==1),]
dim(p_filtered)

How many sites got filtered out when testing for selection? Why?
  
  make manhattan plot
plot(-log10(pval$p_PC1),
     col=p_filtered$chromo,
     xlab="Position",
     ylab="-log10(p-value)",
     main="Selection outliers: pcANGSD e=1 (K2)")

We can zoom in if theres something interesting near a position...

plot(-log10(pval$p_PC1[2e05:2.01e05]),
     col=p_filtered$chromo, 
     xlab="Position", 
     ylab="-log10(p-value)", 
     main="Selection outliers: pcANGSD e=1 (K2)")

get the contig with the lowest p-value for selection
sel_contig <- p_filtered[which(pval==min(pval$p_PC1)),c("chromo","position")]
sel_contig

get all the outliers with p-values below some cutoff
cutoff=1e-3   # equals a 1 in 5,000 probability
outlier_contigs <- p_filtered[which(pval<cutoff),c("chromo","position")]
outlier_contigs

how many outlier loci < the cutoff?
  dim(outlier_contigs)[1]

how many unique contigs harbor outlier loci?
  length(unique(outlier_contigs$chromo))

```

How do we find out what functional genes are contained on the unique contigs harboring outliers?
  
  We can use the *Picea abies* reference genome annotation to get the genes based on the outlier contigs, then test for enrichment of gene function using available public databases.

First, export your unique outlier contigs in R:
  
  ```
write.table(unique(outlier_contigs$chromo),
"allRS_poly_PC1_outlier_contigs.txt", 
sep="\t",
quote=F,
row.names=F,
col.names=F)
```

Transfer back to the server and then grep out the gene IDs. We can do this with a series of piped commands like so:
  
  - Define your paths to the ref genome annotation on the server and your newly made outlier loci file

- Use `zcat` to open the annotation without unzipping it, then pipe to `grep` and use the `-f` flag to take the contig names from the outliers file -- this is a handy trick, and saves us a lot of time from having to manually input each contig one at a time and search!
  
  - Pipe to a new `grep` to get just the contigs containing genes

- Pipe the list of gene IDs to take just the unique ones, cut out the 9th column (containing the gene IDs), and get rid of the annoying "ID= " portion of each gene ID

Just a reminder, the following is bach code, not R ;)

```
ANNOT="/netfiles/ecogen/PopulationGenomics/ref_genome/annotation/Pabies1.0-all-cds.gff3.gz"

OUTLIERS=~/myresults/ANGSD/allRS_poly_PC1_outlier_contigs.txt

zcat ${ANNOT} | grep -f ${OUTLIERS} | grep "gene" | uniq | cut -f9 | sed "s/ID=//g"
```

Voila! You should now have a list of gene IDs printed to your screen. You can either copy these by highlighting with your mouse and clicking <ctrl><c> or you can save to an external file for later (using the `>outfile.txt` command)

Take your gene IDs and go to the [plantgenie](https://plantgenie.org) website. Here, you can create a gene list using your copied IDs, determine which genes they correspond to (not all wil be annotated!), and test for functional enrichment:
  
  - In the upper left corner, see a circle with 3 red bars --> click it, and create a new gene list

- Paste your gene IDs into the list, and plantgenie will search the P. abies annotation and return all the known genes

- Click the "+" button to save these to the current gene list, then close out

- Go to the "Analysis Tools" tab on the top bar of the page, and choose "Enrichment"

- Plantgenie will now test for whether the annotation Gene Ontology (GO) and Protein Family (Pfam) functional descriptions assigned to each gene ID are over-represented compared to all other genes containing those GO and Pfam categories in the P. abies genome.

- Relate back to the biology!
  - *What sort of functional genes did you find are represented in your outliers?*
  - *Are any enriched for certain GO or Pfam functions? (focus on p-values << 0.01, or q-values < 0.05)*
  - *Remember the population structure axis (genetic PC) these loci are outliers along...are any of the enriched functions interesting in light of this structure? Any surprising?*
  
  
  
## Pop Genomics #8  {-}
  
  ### Transcriptomics {-}
  
  
  #### Objectives for today  {-}
  
1. Review Homework Assignment #2.

2. Perform Gene Ontology (GO) functional enrichment analyses using [GOMWU](https://github.com/z0on/GO_MWU/tree/master) with our DESeq2 results.

### 1. Homework 2 is live! 
Find details [here](https://pespenilab.github.io/Ecological-Genomics/Fall2023/assignments/Homework2.html). This assignment is a choose your own adventure (as I mentioned it would be in class). You’ll find 5 different options with specific tasks associated with each (or a 6th that you choose and float by me). Please engage with the options and let me know your choice by Friday. 

#### 2. Perform Gene Ontology (GO) functional enrichment analyses using [GOMWU](https://github.com/z0on/GO_MWU/tree/master) with our DESeq2 results.

There are two general types of functional enrichment analyses: those that use a DGE significance cutoff (e.g., dividing genes into differentially expressed, padj < 0.05, and not and running a Fishers exact test) and those that use the whole distribution (MWU rank-based correlation or Kolmorgorov-Smirnov test). I generally prefer using the whole distribution, more data, and doesnt depend on an arbitrary cutoff.

There are many ways to characterize the function of genes. One of the commonly used databases and classification systems is that of [Gene Ontology (GO)](https://geneontology.org/docs/ontology-documentation/), which is a knowledgebase that "provides a computational representation of our current scientific knowledge about the functions of genes (or, more properly, the protein and non-coding RNA molecules produced by genes) from many different organisms, from humans to bacteria." There are three types of GO categories, those that describe: Molecular Function, Cellular Component, and Biological Process. I find the Biological Process to be most informative.

There are also many ways to test for the non-random distribution of genes with specific functions in a list of scores, such as p-values, logFoldChange, FST, etc. The package we will use for this tutorial is called [GO Mann-Whitney U or GOMWU](https://github.com/z0on/GO_MWU/tree/master) and was created by Dr. Misha Matz of UT Austin.

GOMWU requires all of the following to be in one directory: 
  
  * 2 user provided files: 
  * 1) a table of measure of interest: two columns of comma-separated values: gene id, continuous measure of change such as log(fold-change).  
* 2) table of GO annotations for your sequences: two-column (gene id - GO terms), tab-delimited, one line per gene, multiple GO terms separated by semicolon. 
* a set of scripts: GO_MWU.R, gomwu_a.pl, gomwu_b.pl, gomwu.functions.R (we only need to edit the first one)
* a GO database, hierarchy file (go.obo, http://www.geneontology.org/GO.downloads.ontology.shtml) * Ive already downloaded the most recent version for us to use.

The scripts, the GO database, and the annotations table are already assembled and can be found in `/data/project_data/RNAseq/analyses/GOMWU`. You can copy all of those files to your personal machine or you can move the files to your home directory on the server and run the code in R on our class server. The only part we need to make is the measures of interest based on DESeq2 results. 

### DESeq2 to GOMWU measure/scores files {-}

We want to save the results files for our contrasts of interest for which wed like to test for functional enrichment. Well focus on F0 contrasts of AM vs OW, OWA, and OA. Well run three tests focused on BP and compare enrichment across these three contrasts. Well use LFC as our metric, but we could also choose p-value or stat.

```{ }
## Set your working directory
setwd("~/github/hudsonica")

## Import the libraries that were likely to need in this session
library(DESeq2)

# Try with new counts table from filtered transcriptome assembly
countsTable <- read.table("salmon.isoform.counts.matrix.filteredAssembly", header=TRUE, row.names=1)


head(countsTable)
dim(countsTable)
#[1] 130580     38 - genes
# [1] 349516     38 - isoforms
# [1] 67916    38 - filtered assembly

countsTableRound <- round(countsTable) # bc DESeq2 doesnt like decimals (and Salmon outputs data with decimals)
head(countsTableRound)

#import the sample discription table
conds <- read.delim("ahud_samples_R.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1)
head(conds)

#################### MODEL NUMBER 2 - subset to focus on effect of treatment for each generation

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData=conds, 
    design= ~ treatment)

dim(dds)
# [1] 130580     38

# Filter 
dds <- dds[rowSums(counts(dds) >= 30) >= 28,]
nrow(dds) 

# Subset the DESeqDataSet to the specific level of the "generation" factor
dds_F0 <- subset(dds, select = generation == F0)
dim(dds_F0)

# Perform DESeq2 analysis on the subset
dds_F0 <- DESeq(dds_F0)

resultsNames(dds_F0)
# [1] "Intercept"           "treatment_OA_vs_AM"  "treatment_OW_vs_AM"  "treatment_OWA_vs_AM"

res_F0_OWvAM <- results(dds_F0, name="treatment_OW_vs_AM", alpha=0.05)

res_F0_OWvAM <- res_F0_OWvAM[order(res_F0_OWvAM$padj),]
head(res_F0_OWvAM) 

summary(res_F0_OWvAM)


res_F0_OWAvAM <- results(dds_F0, name="treatment_OWA_vs_AM", alpha=0.05)

res_F0_OWAvAM <- res_F0_OWAvAM[order(res_F0_OWAvAM$padj),]
head(res_F0_OWAvAM) 

summary(res_F0_OWAvAM)


res_F0_OAvAM <- results(dds_F0, name="treatment_OA_vs_AM", alpha=0.05)

res_F0_OAvAM <- res_F0_OAvAM[order(res_F0_OAvAM$padj),]
head(res_F0_OAvAM)

summary(res_F0_OAvAM)


################## Save all the results as csv to go into GOMWU

library(tidyr)

# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OWvAM_df <- data.frame(transcriptID = rownames(res_F0_OWvAM), res_F0_OWvAM)

# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OWvAM_df <- separate(res_F0_OWvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE) 

# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OWvAM_df$transcriptID_trim <- paste(res_F0_OWvAM_df$part1, res_F0_OWvAM_df$part2, sep = "::")

# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OWvAM_df <- res_F0_OWvAM_df[, !(names(res_F0_OWvAM_df) %in% c("part1", "part2", "part3", "rest"))]

write.table(res_F0_OWvAM_df, file = "res_F0_OWvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records

# Select the two columns we want to save for the GOMWU analysis
selected_columns_OW <- res_F0_OWvAM_df[c("transcriptID_trim", "log2FoldChange")]

# Save the selected columns as a CSV file
write.csv(selected_columns_OW, file = "res_F0_OWvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU


############ Now for OWA

# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OWAvAM_df <- data.frame(transcriptID = rownames(res_F0_OWAvAM), res_F0_OWAvAM)

# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OWAvAM_df <- separate(res_F0_OWAvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE) 

# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OWAvAM_df$transcriptID_trim <- paste(res_F0_OWAvAM_df$part1, res_F0_OWAvAM_df$part2, sep = "::")

# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OWAvAM_df <- res_F0_OWAvAM_df[, !(names(res_F0_OWAvAM_df) %in% c("part1", "part2", "part3", "rest"))]
write.table(res_F0_OWAvAM_df, file = "res_F0_OWAvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records

# Select the two columns we want to save for the GOMWU analysis
selected_columns_OWA <- res_F0_OWAvAM_df[c("transcriptID_trim", "log2FoldChange")]

# Save the selected columns as a CSV file
write.csv(selected_columns_OWA, file = "res_F0_OWAvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU



############ Now for OA

# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OAvAM_df <- data.frame(transcriptID = rownames(res_F0_OAvAM), res_F0_OAvAM)

# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OAvAM_df <- separate(res_F0_OAvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE) 

# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OAvAM_df$transcriptID_trim <- paste(res_F0_OAvAM_df$part1, res_F0_OAvAM_df$part2, sep = "::")

# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OAvAM_df <- res_F0_OAvAM_df[, !(names(res_F0_OAvAM_df) %in% c("part1", "part2", "part3", "rest"))]
write.table(res_F0_OAvAM_df, file = "res_F0_OAvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records

# Select the two columns we want to save for the GOMWU analysis
selected_columns_OA <- res_F0_OAvAM_df[c("transcriptID_trim", "log2FoldChange")]

# Save the selected columns as a CSV file
write.csv(selected_columns_OA, file = "res_F0_OAvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU
```

Make sure your .csv, measures files are in the same directory with your GOMWU scripts.

Edit the `GO_MWU.R` script and run the program at least three times, one for each contrast!
  
  Open the `GO_MWU.R` in R studio. There we will edit the two input filenames and confirm the sizes of GO categories to which to limit the analysis (e.g., between 10 and 500 gene members in a GO category). Lets discuss why to do this!
  
  #### Biological interpretation?  {-}
  What do these results mean? How do enrichment results vary based on treatment contrast? Are there any reasons to be concerned about these results? What next steps would you want to take?
  
  
  
  
  
  
  
  
  
## Structural variation in purple sea urchins {-}

### Background {-}

#### What is structural variation? {-}

The types of genomic structural variations (SVs) are: 
  
  - deletions 

- insertions

- duplications

- inversions

- translocations

, of at least 50 bp in size. Small variants, variants that are less than 50bp in length (single nucleotide variants (SNVs) and short insertions, and deletions (indels)), are often considered separately because then are the result of other mechanisms. Invertions and translocations are called balanced forms, while deletions, insertions, and duplications are imbalanced forms that are commonly also referred to as copy number variations.

Structural variants can result from a range of mutational mechanisms, like DNA recombination-, replication- and repair-associated processes. Here is a figure summarising these processes^[Currall, B. B., Chiangmai, C., Talkowski, M. E., & Morton, C. C. (2013). Mechanisms for structural variation in the human genome. Current genetic medicine reports, 1, 81-90.]: https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=3665418_nihms456963f1.jpg

#### Why are structural variants interesting? {-}

More and more studies are being published showing that SVs are an important source of genetic diversity in humans and primates (species we have the most sequence data for), contributing to their evolution. Surprisingly, it was reported that genomic differences caused by SVs are 2 to 10 times higher than differences cause by single nucleotide variants (SNVs) between human individuals. It has also been hypothesized that SVs could have a higher impact on phenotypic changes than SNVs. Given these new discoveries it is unsurprising that a lot of human diseases, including neurodevelopmental disorders and cancers, have recently been associated with SVs.^[Soto, D. C., Uribe‐Salazar, J. M., Shew, C. J., Sekar, A., McGinty, S. P., & Dennis, M. Y. (2023). Genomic structural variation: A complex but important driver of human evolution. American Journal of Biological Anthropology.]

#### Methods for detecting SVs  {-}

There are two main methods for detecting SVs:^[Kosugi, S., Momozawa, Y., Liu, X., Terao, C., Kubo, M., & Kamatani, Y. (2019). Comprehensive evaluation of structural variation detection algorithms for whole genome sequencing. Genome biology, 20, 1-18.]

1. Array-based detection - such as microarray comparative genome hybridization (array CGH)
- Pros: 
  - high-throughput analysis
- time and money efficient
- Cons: 
  - they only detect certain types of SVs
- lower sensitivity for small SVs
- lower resolution for determining breakpoints (BPs)

2. Sequencing-based computational methods
- Pros:
  - can detect a range of SV types
- higher resolution
- Cons:
  - more expensive
- high rate of miscalling of SVs (errors in base call, alignment, or de novo assembly)
- especially in repetitive regions (though this can be overcome by long read sequencing)

Main computational approaches:
  
  a) Read pairs (PEM)

b) Read depth 

c) Split read

Studies investigating structural variation usually use a combination of the above mentioned approaches.

a) Uses paired-end sequencing. After the reads are mapped to the reference genome, "pairs mapping at a distance that is substantially different from the expected length, or with anomalous orientation, suggest structural variants."^[Medvedev, P., Stanciu, M., & Brudno, M. (2009). Computational methods for discovering structural variation with next-generation sequencing. Nature methods, 6(Suppl 11), S13-S20.] See image below.


b) More reads mapped -> duplication, less reads mapped -> deletion.

c) Only half of the read maps.

### Local PCA {-}

Today, we will be using a fourth method to find large inversions and to explore the structure of genetic variation in sea urchin populations in general. We are going to be using a package developed for this purpose; called local PCA. Essentially, what it does is that it creates a PCA for each genomic window (width specified by the user) in our dataset, then it creates a super PCA (well, multidimensional scaling (MDS) which is kind of the same thing) of those PCAs. Finally, it identifies a subset of windows that cluster together in the super PCA. Here is the paper describing the method: <https://academic.oup.com/genetics/article/211/1/289/5931130?login=true>. Using these results, we will identify some putative inversions and look at GO enrichment of genes that fall into those subsets of windows identified by local PCA.

Here is a visual representation of the workflow from the paper:
  
  
  
  Example results with known inversions:
  
  
  
  In a nutshell, this pattern comes to be because the rate of recombination in the inverted region is reduced. Therefore, SNPs that were together prior to inversion stay together, even as the genetic variation changes due to recombination outside of the inverted region.

Imagine a mother that gets an original inversion in a germline cell through some mutational mechanism. When that copy pairs up during recombination, the inverted region wont recombine. Thus, whatever combination of SNPs happened to be in that region gets "preserved". In the end, if this inversion spreads in the population, every individual will one of the 3 possible genotypes:
  
  1. Be a homozygote to the inversion and thus to that specific combination of SNPs.

2. Be homozygote without an inversion and thus have a diverse set of SNPs in that region.

3. Be a heterozygote and have a copy which is inverted and a copy that is not inverted.

Thus, if we were to make a PCA with the SNPs that are in that inverted region, we would get a figure such as this:^[Ma, J., & Amos, C. I. (2012). Investigation of inversion polymorphisms in the human genome using principal components analysis. PloS one, 7(7), e40224.]



Where the number of points in each group will depend on how many individuals there are in the population for each of the above categories.

On the other hand, if we make a PCA with SNPs outside of this region, the pattern will likely be different, as individuals will cluster differently on the PCA. That is why you can see windows that plot similarly on a PCA within the inverted region on the figure above (the second figure in this section).

The dataset

The paper describing the urchin dataset that we will be analyzing is available here: https://www.journals.uchicago.edu/doi/10.1086/726013

We collected, shipped, extracted DNA, and sequenced the whole genomes of 140 purple sea urchins (Strongylocentrotus purpuratus), 20 from each of seven sites (figure below). Coordinates for collection sites were chosen on the basis of pH data collected by autonomous pH sensors mounted submerged in the water at ecologically relevant depth for this sea urchin species. Paired-end sequencing using NovaSeq S2 Flow Cell 150 × 150 bp on a single lane resulted in high-quality reads, such that no trimming was necessary.

Relevant results:
  
  - High genetic diversity

- No overall population structure due to high gene flow


### Steps already completed {-}

The following steps are already completed for you, you will not need to run any of this (mapping and variant calling).

### Mapped reads to the reference {-}

#### The reference genome {-}

<https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000002235.5/>
  
  N50: 37.3 Mb

"Length of the shortest contig for which longer and equal length contigs cover at least 50 % of the assembly".

21_scaffolds file in the `/netfiles/ecogen/structural_variation/`

### The mapping algorythm {-}

Input: List of read files (R1 and R2)

```
while read line ; do
F1=$(cut -d   -f1 <<< $line)
F2=$(cut -d   -f2 <<< $line)
echo "$F1 -- $F2"
FILE=$(mktemp)
cat header.txt >> $FILE
echo "spack load samtools@1.10" >> $FILE
echo "spack load bwa@0.7.17" >> $FILE
ref="/users/c/p/cpetak/WGS/reference_genome/GCF_000002235.5_Spur_5.0_genomic.fna"
out_name=$(cut -d . -f1 <<< $F1)
echo "bwa mem -t 1 -M $ref /users/c/p/cpetak/WGS/all_fastqs/$F1 /users/c/p/cpetak/WGS/all_fastqs/$F2 | samtools view -S -b > /users/c/p/cpetak/WGS/BWA_out/$out_name.bam" >> $FILE
sbatch $FILE
sleep 0.5
rm $FILE
done < $1
```

The Burrows-Wheeler Alignment Tool (BWA) MEM algorithm was used for mapping the raw reads to the S. purpuratus reference genome (Spur ver. 5.0, scaffold N50 ∼37 Mbp). The average coverage for each individual was 6.42±0.78, with an average mapping rate of 81.6±0.01.

#### Called variants for each chromosome across all individuals {-}

Input:
  
  -   21_scaffolds

-   list_of_files.txt, 140 lines, line 1: `/users/c/p/cpetak/WGS/BWA_out/BOD_18170X61_200925_A00421_0244_AHKML5DSXY_S81_L002_R1_001.rmdup.bam`

```
while read line ; do
echo "$line"
FILE=$(mktemp)
cat header.txt >> $FILE
ref="/users/c/p/cpetak/WGS/reference_genome/GCF_000002235.5_Spur_5.0_genomic.fna"
echo "echo "${line}" " >> $FILE
echo "bcftools mpileup -r $line -f $ref --bam-list list_of_files.txt | bcftools call -mv -Ob -o multi_bam_${line}.bcf" >> $FILE
sbatch $FILE
sleep 0.5
rm $FILE
done < $1
```

Output: bcf files in the `/netfiles/ecogen/structural_variation/bcf_files directory`

### Filtering the bcf files {-}

Before we do anything else, lets filter these bcf files. In order to look at them we will convert them into a vcf format shortly.

#### List of chromosomes to choose from: {-}

```
1. NW_022145594.1
2. NW_022145595.1
3. NW_022145596.1
4. NW_022145597.1
5. NW_022145598.1
6. NW_022145601.1
7. NW_022145602.1
8. NW_022145603.1 # Csenge
9. NW_022145604.1
10. NW_022145605.1
11. NW_022145606.1
12. NW_022145610.1
13. NW_022145611.1
14. NW_022145612.1
15. NW_022145613.1
16. NW_022145614.1
17. NW_022145615.1
```

#### Copy files you will be working with {-}

-   Make a new directory in `~/mydata` called str_data (use the mkdir command)

-   Do the following in tmux as it could take some time for the copying to finish.

-   Copy your bcf file into that new directory. You can do this by typing the following, and replacing the chromosome number with YOURS: `cp /netfiles/ecogen/structural_variation/bcf_files/multi_bam_mychromosome.bcf ~/mydata/str_data`

-   Check that you have the data youll need in `~/mydata/str_data`, a multi_bam_mychromosome.bcf file.

### Write a bash script to do the filtering for your chromosome {-}

-   cd into your myscripts directory, `cd ~/myscripts`

-   type `vim filter_chromosome.sh`, then hit the "i" key to enter insert mode

-   copy the following lines:
  
  ```
#!/bin/sh

mychr="NW_022145603.1" # Replace this with your chromosome

# use bcftools to filter your chromosome. The output of this line will be a vcf file that we can look at

bcftools view -e QUAL <= 40 || DP < 560 || MQB < -3 || RPB < -3 || RPB > 3 || AN < 238 ~/mydata/str_data/multi_bam_${mychr}.bcf > ~/mydata/str_data/${mychr}_filtered.vcf

echo "Filtered bcf" # Some printing to keep track of progress

# Convert the filtered vcf into the bcf file type which is the type the R package will be expecting

bcftools view -Ob ~/mydata/str_data/${mychr}_filtered.vcf > ~/mydata/str_data/${mychr}_filtered.bcf

echo "Converted to bcf" # Some printing to keep track of progress

# Index the filtered bcf file. This will make the file more searchable by the algorythm reading it.

bcftools index ~/mydata/str_data/${mychr}_filtered.bcf
echo "Indexed bcf"
echo "Done!"
```

-   Hit `esc`, then type `:wq`

*A bit on tmux*
  
  Before running the script above in tmux, I wanted to go over how to use tmux in a few minutes.

Tmux is a great package that allows you to switch easily between several programs in one terminal, detach them (they keep running in the background) and reattach them when you want to look at it again.

When you type `tmux` and hit enter, you create a new session that you can detach from using `Ctrl B + D`. However, if you type tmux again in the future, you will again create a new session instead of opening the previous one. If you do this repeatedly, you will end up creating a lot of sessions in the background at the same time. In order to be able to navigate back to a specific session later, you should instead type:
  
  `tmux new -s mysession` where mysession is the name of your session. So, for example now you can type:
  
  `tmux new -s bcf_filtering`. Then you can list the sessions you are currently running with

`tmux ls`. If you see a lot of sessions being printed to the screen and you want to get rid of them all, type

`tmux kill-session`. Great! Now that your tmux is all clean, lets create a new session again named bcf_filtering:
  
  `tmux new -s bcf_filtering`. Hit Enter. Type `Ctrl B + D`. Since we named it, you can open the same session again:
  
  `tmux attach-session -t bcf_filtering`. Yay! Now run the filtering script by typing `bash filter_chromosome.sh`. The code should complete in 6 minutes. Hit `Ctrl B + D` to detach the session. If you want to look at the progress, you can type `tmux attach-session -t bcf_filtering` again!
  
  #### The vcf file format {-}
  
  While the filtering is running, lets look at the vcf file that we are creating in the process. I have an example for you ready in `/netfiles/ecogen/structural_variation/examples`. Go ahead and cd into it. DO NOT try to open the vcf file with vim, it WILL crush your computer :). Instead, type `head NW_022145602.1_filtered.vcf` to print the first 10 lines.

VCF, variant call format, is an extremely common, standardized file format to store genetic information in. It always starts with a few (or many in our case) header lines that start with `##`. These header lines contain information about how the vcf file was generated, followed by information about the reference. Go ahead and type `more NW_022145602.1_filtered.vcf` and hit Enter. The more command is similar to the head command, but it allows you to move down in the file by hitting Enter. You will see that we have a lot of scaffolds listed in this file, so instead of hitting and infinite amount of Enters, hit `Ctrl C` to exit the more command, and then type `more -900 NW_022145602.1_filtered.vcf` to jump to line 900. After all the scaffold information, you will see a description of what the different sections in the INFO column in the vcf file mean. I decided to filter on:
  
  -   DP, which is Raw read depth, across all individuals (so 560 reads is 4 depth on average among 140 individuals)

-   MQB, Mann-Whitney U test of Mapping Quality Bias, "If p \< 0.05, it suggests there is significant bias i.e. the reads supporting alternate allele have lower mapping quality than reads supporting reference allele. The bigger p value is better."

-   RPB, Mann-Whitney U test of Read Position Bias, "alleles present at the end of reads may not be right".

-   AN, Total number of alleles in called genotypes, we have a total of 2 (heterozygote) x 140 (number of individuals) alleles. 85% is my cut off.

I decided on these values based on similar literature I found, and also the percent data loss after applying different levels of filtering.

Now lets look further down in the file. You should see a header line that lists the column names:
  
  `#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT` followed by a 140 long names. This is a standard format. Keep pressing enter until you find the first line.

-   CHROM is the chromosome name

-   POS is the position in the chromosome

-   ID can be an ID assigned to the specific variant. In some model organisms, like humans, there are known named variants. That information would go here. If you are curious, you can browse those here: <https://www.ncbi.nlm.nih.gov/variation/view/>
  
  -   REF allele in the reference

-   ALT allele in your dataset

-   QUAL posterior genotype probability in Phred scale. QUAL = 20 means there is 99% probability that there is a variant at the site. I decided to filter out sites with QUAL \<= 40.

-   FILTER If all filters are passed, PASS is written in the filter column. These are default filters during the variant calling, takes depth of coverage, genotyping quality and variant quality into account.

-   INFO Some information about the variant, see description in the vcf file header lines I mentioned above.

-   FORMAT Specifies the format in which the genotype data is given. In this case, it is GT:PL. GT - Genotype, PL - List of Phred-scaled genotype likelihoods.

Lets look at an example!
  
  Example 1:
  
  ```
NW_022145602.1	1564	.	T	A	129	.	DP=599;VDB=0.259328;SGB=31.2596;RPB=0.70951;MQB=0.816143;MQSB=0.0194;BQB=0.892443;MQ0F=0.358932;ICB=0.000506821;HOB=0.000246914;AC=3;AN=270;DP4=385,98,11,1;MQ=21	GT:PL	./.:0,0,0	0/0:0,12,76	0/0:0,6,74	0/0:0,15,118	0/0:0,3,4	0/1:31,0,28	0/0:0,9,40	0/0:0,6,7 0/0:0,6,42	0/0:0,12,11	0/0:0,3,4	0/0:0,3,37	0/0:0,21,171	0/0:0,3,4	./.:0,0,0	0/0:0,15,69	0/0:0,15,41	0/0:0,24,99	0/0:0,6,67	0/
  0:0,12,77	0/0:0,15,120	0/0:0,15,117	0/0:0,3,37	0/0:0,9,42	0/0:0,9,13	0/0:0,21,98	0/0:0,9,9	0/0:0,3,37	0/0:0,3,40	0/0:0,15,69	0/
  0:0,15,16	0/0:0,12,43	0/0:0,3,4	0/0:0,9,75	0/0:0,18,119	0/0:0,6,36	0/0:0,6,39	0/0:0,3,37	0/0:0,15,98	0/0:0,6,39	./.:0,0,0	0/
  0:0,9,77	0/0:0,9,74	0/0:0,12,60	0/0:0,3,4	0/0:0,6,39	0/0:0,12,44	0/0:0,9,68	0/0:0,15,103	0/0:0,9,71	0/0:0,18,122	0/0:0,9,40	0/
  0:0,12,59	0/0:0,18,102	0/0:0,6,64	0/0:0,15,71	0/0:0,12,99	0/0:0,6,74	0/0:0,3,37	0/0:0,24,128	0/0:0,18,108	0/1:122,0,34 (...)
```

0/0: homozygote to the alternative allele 0/1: heterozygote. since we dont know which allele is coming from which parent, 0/1 just means heterozygote, and you wont see 1/0 anywhere. 1/1: homozygote for the alternative allele

The likelihood scores: "Scores for 0/0 (homozygous ref), 0/1 (heterozygous), and 1/1 (homozygous alt) genotypes. For a phred-scaled likelihood of P, the raw likelihood of that genotype L = 10\^(-P/10) (so the higher the number, the less likely it is that your sample is that genotype). The sum of likelihoods is not necessarily 1."

NOTE: in module 1, population genetics on red spruce data, we were looking at SNP information at biallelic sites. As you can see if you scroll down, this dataset also includes sites with duplications/deletions/insertions, as well as triallelic (or even quadrallelic) sites.

Example 2:
  
  ```
NW_022145602.1	1553	.	ag	a,aGg	999	.	INDEL;IDV=7;IMF=0.7;DP=583;VDB=0.0245597;SGB=-53.7492;MQSB=0.0144647;MQ0F=
  0.447684;ICB=0.473584;HOB=0.0514945;AC=79,2;AN=274;DP4=286,108,161,28;MQ=20	GT:PL	0/0:0,3,4,3,4,4	0/0:0,5,57,9,60,57	0/0:0,9,42
,9,42,42	0/1:26,0,1,44,10,35	0/0:0,3,4,3,4,4	0/0:0,12,69,12,69,69	0/1:25,5,0,28,9,25	0/1:7,5,0,10,9,7	1/1:39,6,0
,39,6,39	0/0:0,15,12,15,12,12 (...) 1/1:21,8,0,24,12,21	0/0:0,18,33,12,36,33	0/0:0,6,34
,6,34,34	0/1:6,0,69,15,72,81	0/1:3,0,25,18,31,35	0/1:55,5,0,61,12,56	0/0:0,9,87,9,87,87	0/0:0,15,60,15,60,60	0/
  0:0,3,3,6,6,4	0/0:0,3,4,3,4,4	1/1:48,9,0,57,21,49	0/1:16,0,3,22,9,24	1/1:52,6,0,52,6,52	0/0:0,21,81,21,81,81	0/1:0,0,16
,6,25,21	0/0:0,9,61,9,61,61	0/1:27,0,42,36,51,74	0/1:44,0,26,59,32,77	0/1:14,0,30,23,33,50	0/1:20,3,0,20,3,20	1/
  1:72,12,0,72,12,72	0/1:23,3,0,29,9,25	0/0:0,12,13,12,13,13	0/1:26,5,0,29,9,26	0/0:0,9,21,9,21,21	0/1:3,3,0,6,6,4	0/
  0:0,15,65,15,65,65	0/0:0,5,58,9,61,58	0/0:0,9,15,9,15,15	0/0:0,3,4,3,4,4	0/1:48,0,16,54,28,70	0/0:0,1,22,3,25,24	0/
  1:11,2,0,17,6,14	1/2:89,62,59,30,0,22	0/1:50,3,0,53,6,51	0/0:0,6,8,6,8,8	0/0:0,3,31,3,31,31	0/0:0,9,37,9,37,37	0/
  0:0,15,109,15,109,109	0/1:17,0,78,35,81,108	0/0:0,2,78,12,81,85	0/1:47,0,56,59,69,110	1/1:68,6,0,68,6,68	0/0:0,9,55,9,55,55
0/0:0,9,65,9,65,65	0/1:2,0,1,14,4,12	0/0:0,9,71,9,71,71	1/1:99,18,0,99,18,99	0/2:62,71,94,0,36,27 (...)
```

In this case, the reference is AG, the first alternative allele has a deletion of the G, and the second alternative allele has an addition of a G! So for example, 0/2 is a heterozygote where one copy has the insertion, and 1/1 is a homozygote for the deletion.

### Running local PCA {-}

Local PCA is an R package, here is the GitHub page: <https://github.com/petrelharp/local_pca>. I already installed this package on the class server following the instructions in the README.md. You should be able to use it without doing anything.

On the page, they also provide an Rscript to use the package and a separate one to visualise the results, which I copied to the server (and slightly modified): `/netfiles/ecogen/structural_variation`.

-   Copy the R scripts over to your myscripts directory: `cp /netfiles/ecogen/structural_variation/run_lostruct.R ~/myscripts` and `cp /netfiles/ecogen/structural_variation/summarize_run.Rmd ~/myscripts`

### Lets look at the scripts {-}

Type vim run_lostruct.R in the \~/myscripts directory.

### Run local PCA {-}

-   IMPORTANT: make sure to remove the original unfiltered bcf file from your `~/mydata/str_data` directory, `rm multi_bam_mychromosome.bcf`, because the package will read all .bcf files in the directory that you give it and we only want to run the package on the filtered bcf file.

-   `tmux new -s run_localPCA`

-   `cd ~/mydata/str_data`. This way the results will automatically end up in this directory.

-   `Rscript ~/myscripts/run_lostruct.R -i ~/mydata/str_data -t snp -s 1000 -I /netfiles/ecogen/structural_variation/sample_info.tsv` We are running the Rscript with the "Rscript" command, just like we run the bash scripts with the "bash" command. -i specifies the input file location, -t specifies the way we are calculating the windows, which is based on SNPs for now. -s specifies the window size. -I specifies the location of the file I created to link the individual IDs in the bcf file with the 3 letter population name. This should take 15 minutes to an hour to run.

-   Once the script finishes, you should have a folder called lostruct_results in `~/mydata/str_data`. cd into lostruct_results. Then, cd into a folder called something like type_snp_size_1000_weights_none_jobid_166584 (will be called different for you). Type `ll`.

-   Output files:
  
  -   config.json -\> check with vim

-   mds_coords.csv -\> wc -l -\> how many lines are there? each line corresponds to a window

-   mychromosome.pca.csv -\> wc -l to check the number of rows and `awk -F, {print NF} mychromosome.pca.csv | sort -nu | tail -n 1` to check the number of columns -\> should be 283. For each window (rows), PC1 and PC2 for each individual (140) -\> input variables for the MDS

-   mychromosome.regions.csv -\> windows information (will be important later)

### Visualise results  {-}

NOTE: Make sure to change the `type_snp_size_1000_weights_none_jobid_166584` directory name everywhere in this tutorial to your directory name!
  
  Now well use another script provided by the local PCA package to make an html with a series of plots.

-   `tmux attach-session -t run_localPCA`

-   cd into `~/mydata/str_data/lostruct_results/type_snp_size_1000_weights_none_jobid_166584/`

-   type `Rscript -e templater::render_template("~/myscripts/summarize_run.Rmd",output="~/mydata/str_data/lostruct_results/type_snp_size_1000_weights_none_jobid_166584/run_summary.html",change.rootdir=TRUE)`

-   This should only take a few minutes. Once done, you should have new files in your `type_snp_size_1000_weights_none_jobid_166584` directory.

-   Open FileZilla on your computer. Move run_summary.html in the `~/mydata/str_data/lostruct_results/type_snp_size_1000_weights_none_jobid_166584` directory to your computer.

-   Find that file on your computer and open it in your favorite browser (e.g. Chrome).

### Select regions of interest {-}

In order to be able to get the genomic regions corresponding to a specific corner in the MDS plot, we will need to modify the plotting script (summarize_run.Rmd) such that it writes a csv file with the coordinates.

-   `cd ~/myscripts`, `vim summarize_run.Rmd`, hit I to enter the insert mode.

-   Around line 195, the script creates a variable called corner.regions based on the mds corners. This variable contains a list of chromosome, start, end information for each window in each corners of the mds plot. So, all we need to do is insert a line `write.csv(corner.regions[[1]], "first_corner.csv", row.names=FALSE)` under the line 198 (`}`) to save the genomic regions in the first corner.

-   Hit `esc` and `:wq` to save your changes.

Repeat steps above to rerun the plotting.

-   `tmux attach-session -t run_localPCA`

-   cd into `~/mydata/str_data/lostruct_results/type_snp_size_1000_weights_none_jobid_166584/`

-   type `Rscript -e templater::render_template("~/myscripts/summarize_run.Rmd",output="~/mydata/str_data/lostruct_results/type_snp_size_1000_weights_none_jobid_166584/run_summary.html",change.rootdir=TRUE)`

-   Now you should have a new file in `type_snp_size_1000_weights_none_jobid_166584` called `first_corner.csv`. Type `head first_corner.csv` to see the region information. NOTE: if you are not seeing the new file appear, you might have to delete the cache directory (`rm -r cache`) first, and then rerun the Rscript again.

### Do GO Enrichment for the selected corner  {-}

In order to get a list of genes that are present in the selected windows, we will use an annotation file from ncbi: <https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000002235.5/>. GFF is a standard file format to store genomic annotation information in. I have already downloaded this file and moved it to `/netfiles/ecogen/structural_variation/`, and named it genome_annotation.gff. 

Copy this file into your directory, `cp /netfiles/ecogen/structural_variation/genome_annotation.gff ~/mydata/str_data/lostruct_results/type_snp_size_1000_weights_none_jobid_166584` Lets take a look!
  
  To find an intersect between genes specified in the GFF file and coordinates in our first_corner.csv, we will use a package called bedtools. I have already installed this package for you. The intersectBed algorithm requires the first_corner.csv to be in a specific format (tab delimited, chromosome, start, end), so well need to modify it.

-   `cd ~/mydata/str_data/lostruct_results/type_snp_size_1000_weights_none_jobid_166584` if you are not there alread.

-   `head first_corner.csv` -\>
  
  ```{}
"chrom","start","end","pos"
"NW_022145602.1",227538,246919,237228.5
"NW_022145602.1",246921,249982,248451.5
"NW_022145602.1",250053,258244,254148.5
"NW_022145602.1",268365,272067,270216
"NW_022145602.1",283027,289393,286210
```

-   Remove 4th column: `cut -d, -f1-3 first_corner.csv > first_corner_formatted.csv` -\>
  
  ```{}
"chrom","start","end"
"NW_022145602.1",227538,246919
"NW_022145602.1",246921,249982
"NW_022145602.1",250053,258244
"NW_022145602.1",268365,272067
```

-   Remove "s: `sed -i s/"//g first_corner_formatted.csv`

-   Replace commas with tabs: `sed -i s/,/\t/g first_corner_formatted.csv` -\>
  
  ```{}
chrom	start	end
NW_022145602.1	227538	246919
NW_022145602.1	246921	249982
NW_022145602.1	250053	258244
NW_022145602.1	268365	272067
```

Now that we have the correct file format, lets run the intersectBed algorythm: `/netfiles/ecogen/structural_variation/bedtools2/bin/intersectBed -a first_corner_formatted.csv -b genome_annotation.gff -wa -wb > genes_first_corner.bed` Look at the genes_first_corner.bed file.

To extract the gene names: `sed -n "s/^.*gene=\(LOC[0-9]\+\).*$/\1/p" genes_first_corner.bed > gene_names_first_corner.txt`

Youll see that a lot of the gene names are repeated. To discard repeats, type: `sort gene_names_first_corner.txt | uniq > uni_gene_names_first_corner.txt`

Go to FileZilla on your computer and download this file (mydata/str_data/lostruct_results/type_snp_size_1000_weights_none_jobid_166584/uni_gene_names_first_corner.txt).

Now we can run a gene ontology analysis on these genes!
  
  -   Open the txt file above, select all, copy (Ctrl C).

-   Go to: <https://geneontology.org/>, paste the LOC names into the window on the right.

-   Select Strongylocentrotus purpuratus (purple sea urchin) from the drop down menu instead of Homo sapiens. Click Launch.

-   If you dont get any significantly enriched biological processes, you can change the Annotation Data Set in the drop down menu and click Launch analysis. You can also play around with the Test type and the correction method. Dont worry if you still dont get anything, that is also a valid result!
  
  Congrats! You are all done with this tutorial! :)

References:
  
  
  
  
## Transcriptomics #1 {-}


### Learning Objectives for today  {-}

1. Review _Acartia hudsonica_ ecology and biogeography and the experimental evolution/transcriptomics experimental design.
2. Develop questions that can be addressed and hypotheses that can be tested with the _A. hudsonica_ experiment.
3. Understand the general work flow or "pipeline" for processing and analyzing RNAseq data.
4. Visualize and interpret the quality of our Illumina data.
5. Assess our previously assembled _de novo_ transcriptome assembly using [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki).
6. Start mapping reads and quantifying abundance simultaneously using [Salmon](https://www.nature.com/articles/nmeth.4197).


### 1. Copepod experimental evolution in global change conditions  {-}
Can organisms adapt to global climate change conditions and if so, how?
  [https://www.diark.org/diark/species_list/Acartia_tonsa](/Users/mpespeni/Documents/github/Ecological-Genomics/images/Atonsa.png)

Copepods are one of the most abundant animals on planet. Calenoid copepods, particularly in the genus Acartia, are critical for ecosystem functioning and biogeochemical cycling. They are primary consumers at the base of the oceanic food chain, eating phytoplankton and transferring the energy up the food chain starting with larval fish. _Acartia hudsonica_ is a "cold-adapted" estuarine and near-shore coastal species. It is most abundant along the New England coast (ME, NH, MA, RI, CT), generally not further south than the Chesapeake Bay and not further north than Laborador/Newfoundland, Canada. With the rapid changes in global conditions, specifically temperature and pH in the oceans, it is critical to understand if and how such ecologically important species will survive.

In a collaboration with colleagues at the University of Connecticutt and with funding from the National Science Foundation (2016-2019), we have carried out long-term experimental evolution studies in high temperature and low pH in these two Acartiid species to understand their capacity for and mechanisms of resilience. We have measured phenotypic, allelic, epi-allelic, and transcriptomic responses across the generations in full factorial and reciprocal transplant experiments. Below is a summary of what we have learned thus far. 

- _A. tonsa_ can rapidly adapt to high to high temperature, low pH, and the combination, but with long-term costs revealed after 25 generations ([Dam _et al._ 2021 _Nature Climate Change_](https://www.nature.com/articles/s41558-021-01131-5)).
- Looking at the allelic responses to selection in _A. tonsa_, warming was the dominant driver of evolution in the combined warming and acidification treatment. However, the combination was highly synergistic with 47% of the selection response being unique from either treatment alone ([Brennan _et al._ 2022 _PNAS_](https://www.pnas.org/doi/abs/10.1073/pnas.2201521119)). These results highlight the challenge that concurrent stressors impose on predictions of adaptation to complex environmental changes.
- In a reciprocal transplant study in _A. tonsa_, we found that transcriptional plasticity was lost after 15 generations of experimental evolution in global change conditions, but there was sustained genetic capacity to re-adapt to ancestral ambient conditions at the expense of genetic diversity ([Brennan _et al._ 2022 _Nature Communications_](https://www.nature.com/articles/s41467-022-28742-6)).
- _A. hudsonica_ can also rapidly adapt to high temperature, low pH, and the combination, but with reduced survival revealed after 11 generations (~ one year) ([deMayo _et al._ 2023 _Proc. Roy. Soc. B_](https://royalsocietypublishing.org/doi/abs/10.1098/rspb.2023.1033)).


### New Data! {-}
We measured gene expression of the experimentally evolved _Acartia hudsonica_ over 11 generations in four sets of conditions: 
  
  1. Ambient (AM; 13 degrees C, 400 micro-atm pCO2)
2. Ocean Warming (OW; 15 degrees C, 400 micro-atm pCO2) 
3. Ocean Acidification (OA; 13 degrees C, 1000 micro-atm pCO2)
4. Ocean Warming and Acidification (OWA; 15 degrees C; 1000 micro-atm pCO2)

(/Users/mpespeni/Documents/github/Ecological-Genomics/images/Ahud_exptalDesign.png)

#### Additional experimental details:  {-}

* Animals were collected from the Long Island Sound, CT and reared in the lab for 3 generations before the start of the experiment.
* For each treatment, there were three replicate vessels with ~4,000 individuals per vessel. 
* To sample for RNA, pools of 50 adults were taken at the end of the F0, F2, F4, and F11 generations. Water was removed and animals were flash frozen in liquid nitrogen. 
* RNA was extracted using a modified TRIzol extraction protocol. 
* Library preparation and sequencing was carried out by Novogene using standard Illumina RNAseq library prep protocols (TruSeq3).
* Samples were sequenced 150 base pair paired-end reads (2 x 150bp) using the Illumina Novoseq 6000 platform with >6 Gb (gigabase pairs = >6 billion base pairs) per sample.  


Realized sample replication after sequencing:  N=38 x 2 = 76 (left and right reads)

|Trt        | Generation  |Nreps  |
  |-----------|-------------|-------|
  |Ambient (AA)    |F0      |3      |
  |Ambient (AA)    |F2      |2      |
  |Ambient (AA)    |F4      |3      |
  |Ambient (AA)    |F11     |3      |
  |Acidification (AH)    |F0       |3      |
  |Acidification (AH)    |F2       |3      |
  |Acidification (AH)    |F4      |3      |
  |Warming (HA)       |F0      |3      |
  |Warming (HA)       |F2      |3      |
  |Warming (HA)       |F4      |3      |
  |Acidification+Warming (HH)    |F0      |3     |
  |Acidification+Warming (HH)    |F4      |3    |
  |Acidification+Warming (HH)    |F11     |3    |
  |-----------|-------------|-------|
  |Total      ||38     |
  
  In `/data/project_data/RNAseq/rawdata/`, there should be 76 files: N=38 x 2 = 76 (left and right reads, `_1.fq.gz`, `_2.fq.gz`)


2. What questions can we ask or hypotheses can we test with this experimental design, with these data? {-}
1.
2.
3.
4.
5.



#### 3. Our general workflow for analyzing gene expression data (Transcriptomics pipeline):  {-}

### Clean the raw sequence data {-} 

* [FastP](https://github.com/OpenGene/fastp) on raw reads --> cleaned reads


  Already complete: Generate and annotate a de novo reference transcriptome assembly

* Used [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki)
* Evaluated transcriptome assembly for quality (length and completeness) using [BUSCO](https://busco.ezlab.org/).

### Map the clean reads to the reference assembly  {-}
* Use [Salmon](https://salmon.readthedocs.io/en/latest/salmon.html) to simulateously map reads to reference transcriptome and quantify abundance.

### Test for differential expression among groups  {-}
* [Import](https://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html#3%E2%80%99_tagged_rna-seq) the data into [DESeq2](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) in R for data normalization, visualization, and statistical tests for differential gene expression.
           
           ### Perform more advanced analyses {-}
           * Identify clusters of genes with correlated expression using Weighted Gene Correlation Network Analysis ([WGCNA](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/)).
           * Test for functional enrichment among genes differentially expressed between groups using [TopGO](https://bioconductor.org/packages/release/bioc/html/topGO.html) or GO Mann-Whitney U  [GO_MWU](https://github.com/z0on/GO_MWU)
           
 ### 4. Choose samples to visualize for quality and clean (fastp) {-}
There are 13 groups of samples (based on the table above, 13 treatment x generation combinations). Every student takes one group to process, that should work out... 
### fastq  {-}
Recall that .fastq (or .fq) files are sequence data files that include quality scores for each base pair. We can check out the reads using `zcat FILENAME.fq.gz | head -n 4`. Recall that letters early in the alphabet indicate good quality on the ASCII score.
*The Phred Q score is translated to ASCII characters so that a two digit number can be represented by a single character.*

```         
           Quality encoding: !""#$%&()*+,-./0123456789:;<=>?@ABCDEFGHI
|         |         |         |         |
  Quality score: 0........10........20........30........40   
```

If P is the probability that a base call is an error, then: Q = -10\*log10(P)

So:
  
  | Phred Quality Score | Probability of incorrect base call | Base call accuracy |
  |--------------------|---------------------------------|-------------------|
  | 10     | 1 in 10  | 90%   |
  | 20     | 1 in 100 | 99%   |
  | 30     | 1 in 1000| 99.9% |
  | 40     | 1 in 10,000           | 99.99%|
  
  ### fastp {-}
  We will use [the program fastp](https://github.com/OpenGene/fastp) (also already installed in our `/data/popgen/` directory and available to run from any directory).

We will use a bash script to loop through the replicates from your treatment group `$MYSAMP`. 

* Theres an example script for you to edit in `/data/project_data/RNAseq/scripts/fastp_ahud.sh`. 
* Copy this to your `~/myscripts` directory. 
  * **Take a moment to meditate on working in linux. Can you copy this file from anywhere to anywhere? What is important?**
* Edit the script: All you need to do is define your samples with `$MYSAMP`. 
* Make sure you make a `~/myresults/fastp` directory.
* This should only take about ~12 minutes for each of your sets of samples to complete, but lets start it in `tmux` to be safe.

But lets talk through the code first:

```
#!/bin/bash   

# This script loops through a set of files defined by MYSAMP, matching left and right reads
# and cleans the raw data using fastp according to parameters set below

# cd to the location (path) to the fastq data:

cd /data/project_data/RNAseq/rawdata

# Define the sample code to anlayze
# Be sure to replace with your 5-6-digit sample code

MYSAMP="XXXXX"

# for each file that has "MYSAMP" and "_1.fq.gz" (read 1) in the name
# the wildcard here * allows for the different reps to be captured in the list
# start a loop with this file as the input:

for READ1 in ${MYSAMP}*_1.fq.gz
do

# the partner to this file (read 2) can be found by replacing the _1.fq.gz with _2.fq.gz
# second part of the input for PE reads

READ2=${READ1/_1.fq.gz/_2.fq.gz}

# make the output file names: print the fastq name, replace _# with _#_clean

NAME1=$(echo $READ1 | sed "s/_1/_1_clean/g")
NAME2=$(echo $READ2 | sed "s/_2/_2_clean/g")

# print the input and output to screen 

echo $READ1 $READ2
echo $NAME1 $NAME2

# call fastp
/data/popgen/fastp -i ${READ1} -I ${READ2} -o /data/project_data/RNAseq/cleandata/${NAME1} -O /data/project_data/RNAseq/cleandata/${NAME2} \
--detect_adapter_for_pe \
--trim_front1 24 \
--trim_poly_g \
--thread 1 \
--cut_right \
--cut_window_size 6 \
--qualified_quality_phred 20 \
--length_required 35 \
--html ~/myresults/fastqc/${NAME1}.html \
--json ~/myresults/fastqc/${NAME1}.json

done
```

* Now move the .html files to your local machine using FileZilla. 
* Lets record in a google sheet some important stats that we may want to report out in a manuscript! Total reads before filtering, total reads after filtering, % reads passed filters. 

 ###5. Assess the quality of the reference transcriptome {-}

I previously assembled the _de novo_ transcriptome with these data using [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki). Lets look at some basic statistics of the assembly.

```{}
/data/popgen/trinityrnaseq-v2.13.2/util/TrinityStats.pl  /data/project_data/RNAseq/assembly/ahud_Trinity.fasta
################################
## Counts of transcripts, etc.
################################
Total trinity genes:	130580
Total trinity transcripts:	349516
Percent GC: 35.57

########################################
Stats based on ALL transcript contigs:
########################################

	Contig N10: 4647
	Contig N20: 3149
	Contig N30: 2356
	Contig N40: 1791
	Contig N50: 1356

	Median contig length: 430
	Average contig: 801.91
	Total assembled bases: 280279107


#####################################################
## Stats based on ONLY LONGEST ISOFORM per GENE:
#####################################################

	Contig N10: 4679
	Contig N20: 3115
	Contig N30: 2247
	Contig N40: 1613
	Contig N50: 1057

	Median contig length: 320
	Average contig: 626.33
	Total assembled bases: 81786351
```

We can also assess the completeness of the assembly using a program called [BUSCO](https://busco.ezlab.org/)

```
busco -m transcriptome -i ahud_Trinity.fasta -o BUSCOarthropoda -l arthropoda_odb10

```


```
$ cat short_summary.specific.arthropoda_odb10.BUSCOarthropoda.txt 
# BUSCO version is: 5.2.2 
# The lineage dataset is: arthropoda_odb10 (Creation date: 2020-09-10, number of genomes: 90, number of BUSCOs: 1013)
# Summarized benchmarking in BUSCO notation for file /data/project_data/RNAseq/assembly/ahud_Trinity.fasta
# BUSCO was run in mode: transcriptome

	***** Results: *****

	C:96.9%[S:7.1%,D:89.8%],F:1.1%,M:2.0%,n:1013	   
	982	Complete BUSCOs (C)			   
	72	Complete and single-copy BUSCOs (S)	   
	910	Complete and duplicated BUSCOs (D)	   
	11	Fragmented BUSCOs (F)			   
	20	Missing BUSCOs (M)			   
	1013	Total BUSCO groups searched		   

Dependencies and versions:
	hmmsearch: 3.1
	metaeuk: 5.34c21f2
```

### 6. Map to the reference transcriptome {-}

#### This chunk makes and preps the reference, note the --prep_reference flag {-}
NOTE: Only one person needs to do this step. Whos the lucky person?


  ```{}
/data/popgen/trinityrnaseq-v2.13.2/util/align_and_estimate_abundance.pl --transcripts /data/project_data/RNAseq/assembly/ahud_Trinity.fasta \
--est_method salmon \
--trinity_mode \
--prep_reference
```


This should make two files:


  ```{}
ahud_Trinity.fasta.gene_trans_map
ahud_Trinity.fasta.salmon.idx
```
#### The chunk below maps to the reference using salmon {-}
* For this part you need to make the samples file `ahud_XXXXX.txt` with your set of samples.
* Copy the complete file from `/data/project_data/RNAseq/assembly/ahud_XXXXX.txt` to you local machine.
* Use a text editor to edit the file to only include your samples.
* Copy the file back to the server; Make sure you give the correct path to your samples file.
* NOTE: Run this with `tmux` and first navigate to the `/data/project_data/RNAseq/mapping` directory


```
cd /data/project_data/RNAseq/mapping

/data/popgen/trinityrnaseq-v2.13.2/util/align_and_estimate_abundance.pl --transcripts /data/project_data/RNAseq/assembly/ahud_Trinity.fasta \
--seqType fq \
--samples_file /data/project_data/RNAseq/assembly/ahud_XXXXX.txt \
--est_method salmon \
--output_dir /data/project_data/RNAseq/mapping \
--thread_count 1 \
--trinity_mode
```

Lets check the mapping rate of the clean reads to the trinity assembly.

```
grep -r --include \*.log -e Mapping rate
```

#### This chunk below assembles all the individually mapped reads into one data matrix {-}

We provide a file list to point to just the `quant.sf` files called `salmon_results_filelist.txt`.


```
/data/popgen/trinityrnaseq-v2.13.2/util/abundance_estimates_to_matrix.pl --est_method salmon \
  --gene_trans_map /data/project_data/RNAseq/assembly/ahud_Trinity.fasta.gene_trans_map \
  --quant_files /data/project_data/RNAseq/mapping/salmon_results_filelist.txt \
  --name_sample_by_basedir
```


Alright! With this matrix of number of reads that mapped to each contig/transcript, we can move to analyzing the data to test for differences in gene expression and more!




## Transcriptomics #2 {-}

### Learning Objectives for today  {-}

1. Review _Acartia hudsonica_ ecology and biogeography and the experimental evolution/transcriptomics experimental design.
2. Develop questions that can be addressed and hypotheses that can be tested with the _A. hudsonica_ experiment.
3. Understand the general work flow or "pipeline" for processing and analyzing RNAseq data.
4. Visualize and interpret the quality of our Illumina data.
5. Assess our previously assembled _de novo_ transcriptome assembly using [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki).
6. Start mapping reads and quantifying abundance simultaneously using [Salmon](https://www.nature.com/articles/nmeth.4197).


### 1. Copepod experimental evolution in global change conditions  {-}
Can organisms adapt to global climate change conditions and if so, how?
[https://www.diark.org/diark/species_list/Acartia_tonsa](/Users/mpespeni/Documents/github/Ecological-Genomics/images/Atonsa.png)

Copepods are one of the most abundant animals on planet. Calenoid copepods, particularly in the genus Acartia, are critical for ecosystem functioning and biogeochemical cycling. They are primary consumers at the base of the oceanic food chain, eating phytoplankton and transferring the energy up the food chain starting with larval fish. _Acartia hudsonica_ is a "cold-adapted" estuarine and near-shore coastal species. It is most abundant along the New England coast (ME, NH, MA, RI, CT), generally not further south than the Chesapeake Bay and not further north than Laborador/Newfoundland, Canada. With the rapid changes in global conditions, specifically temperature and pH in the oceans, it is critical to understand if and how such ecologically important species will survive.

In a collaboration with colleagues at the University of Connecticutt and with funding from the National Science Foundation (2016-2019), we have carried out long-term experimental evolution studies in high temperature and low pH in these two Acartiid species to understand their capacity for and mechanisms of resilience. We have measured phenotypic, allelic, epi-allelic, and transcriptomic responses across the generations in full factorial and reciprocal transplant experiments. Below is a summary of what we have learned thus far. 

- _A. tonsa_ can rapidly adapt to high to high temperature, low pH, and the combination, but with long-term costs revealed after 25 generations ([Dam _et al._ 2021 _Nature Climate Change_](https://www.nature.com/articles/s41558-021-01131-5)).
- Looking at the allelic responses to selection in _A. tonsa_, warming was the dominant driver of evolution in the combined warming and acidification treatment. However, the combination was highly synergistic with 47% of the selection response being unique from either treatment alone ([Brennan _et al._ 2022 _PNAS_](https://www.pnas.org/doi/abs/10.1073/pnas.2201521119)). These results highlight the challenge that concurrent stressors impose on predictions of adaptation to complex environmental changes.
- In a reciprocal transplant study in _A. tonsa_, we found that transcriptional plasticity was lost after 15 generations of experimental evolution in global change conditions, but there was sustained genetic capacity to re-adapt to ancestral ambient conditions at the expense of genetic diversity ([Brennan _et al._ 2022 _Nature Communications_](https://www.nature.com/articles/s41467-022-28742-6)).
- _A. hudsonica_ can also rapidly adapt to high temperature, low pH, and the combination, but with reduced survival revealed after 11 generations (~ one year) ([deMayo _et al._ 2023 _Proc. Roy. Soc. B_](https://royalsocietypublishing.org/doi/abs/10.1098/rspb.2023.1033)).


### New Data!  {-}
We measured gene expression of the experimentally evolved _Acartia hudsonica_ over 11 generations in four sets of conditions: 
  
  1. Ambient (AM; 13 degrees C, 400 micro-atm pCO2)
2. Ocean Warming (OW; 15 degrees C, 400 micro-atm pCO2) 
3. Ocean Acidification (OA; 13 degrees C, 1000 micro-atm pCO2)
4. Ocean Warming and Acidification (OWA; 15 degrees C; 1000 micro-atm pCO2)

(/Users/mpespeni/Documents/github/Ecological-Genomics/images/Ahud_exptalDesign.png)

#### Additional experimental details: {-}

* Animals were collected from the Long Island Sound, CT and reared in the lab for 3 generations before the start of the experiment.
* For each treatment, there were three replicate vessels with ~4,000 individuals per vessel. 
* To sample for RNA, pools of 50 adults were taken at the end of the F0, F2, F4, and F11 generations. Water was removed and animals were flash frozen in liquid nitrogen. 
* RNA was extracted using a modified TRIzol extraction protocol. 
* Library preparation and sequencing was carried out by Novogene using standard Illumina RNAseq library prep protocols (TruSeq3).
* Samples were sequenced 150 base pair paired-end reads (2 x 150bp) using the Illumina Novoseq 6000 platform with >6 Gb (gigabase pairs = >6 billion base pairs) per sample.  

Realized sample replication after sequencing:  N=38 x 2 = 76 (left and right reads)

|Trt        | Generation  |Nreps  |
  |-----------|-------------|-------|
  |Ambient (AA)    |F0      |3      |
  |Ambient (AA)    |F2      |2      |
  |Ambient (AA)    |F4      |3      |
  |Ambient (AA)    |F11     |3      |
  |Acidification (AH)    |F0       |3      |
  |Acidification (AH)    |F2       |3      |
  |Acidification (AH)    |F4      |3      |
  |Warming (HA)       |F0      |3      |
  |Warming (HA)       |F2      |3      |
  |Warming (HA)       |F4      |3      |
  |Acidification+Warming (HH)    |F0      |3     |
  |Acidification+Warming (HH)    |F4      |3    |
  |Acidification+Warming (HH)    |F11     |3    |
  |-----------|-------------|-------|
  |Total      ||38     |
  
  In `/data/project_data/RNAseq/rawdata/`, there should be 76 files: N=38 x 2 = 76 (left and right reads, `_1.fq.gz`, `_2.fq.gz`)


### 2. What questions can we ask or hypotheses can we test with this experimental design, with these data? {-}
1.
2.
3.
4.
5.



### 3. Our general workflow for analyzing gene expression data (Transcriptomics pipeline): {-}
### Clean the raw sequence data {-}
* [FastP](https://github.com/OpenGene/fastp) on raw reads --> cleaned reads

### Already complete: Generate and annotate a de novo reference {-} transcriptome assembly
* Used [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki)
* Evaluated transcriptome assembly for quality (length and completeness) using [BUSCO](https://busco.ezlab.org/).

### Map the clean reads to the reference assembly {-}
* Use [Salmon](https://salmon.readthedocs.io/en/latest/salmon.html) to simulateously map reads to reference transcriptome and quantify abundance.

### Test for differential expression among groups {-}
* [Import](https://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html#3%E2%80%99_tagged_rna-seq) the data into [DESeq2](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) in R for data normalization, visualization, and statistical tests for differential gene expression.
           
           ### Perform more advanced analyses {-}
           * Identify clusters of genes with correlated expression using Weighted Gene Correlation Network Analysis ([WGCNA](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/)).
           * Test for functional enrichment among genes differentially expressed between groups using [TopGO](https://bioconductor.org/packages/release/bioc/html/topGO.html) or GO Mann-Whitney U  [GO_MWU](https://github.com/z0on/GO_MWU)
           
           
           ### 4. Choose samples to visualize for quality and clean (fastp)   {-}
           
           There are 13 groups of samples (based on the table above, 13 treatment x generation combinations). Every student takes one group to process, that should work out... 
           
           ### fastq {-}
           Recall that .fastq (or .fq) files are sequence data files that include quality scores for each base pair. We can check out the reads using `zcat FILENAME.fq.gz | head -n 4`. Recall that letters early in the alphabet indicate good quality on the ASCII score.
           *The Phred Q score is translated to ASCII characters so that a two digit number can be represented by a single character.*

```         
           Quality encoding: !"#$%&()*+",-./0123456789:;<=>?@ABCDEFGHI
|         |         |         |         |
  Quality score: 0........10........20........30........40   
```

If P is the probability that a base call is an error, then: Q = -10\*log10(P)

So:
  
  | Phred Quality Score | Probability of incorrect base call | Base call accuracy |
  |--------------------|---------------------------------|-------------------|
  | 10     | 1 in 10  | 90%   |
  | 20     | 1 in 100 | 99%   |
  | 30     | 1 in 1000| 99.9% |
  | 40     | 1 in 10,000           | 99.99%|
  
  ### fastp {-}
  We will use [the program fastp](https://github.com/OpenGene/fastp) (also already installed in our `/data/popgen/` directory and available to run from any directory).

We will use a bash script to loop through the replicates from your treatment group `$MYSAMP`. 

* Theres an example script for you to edit in `/data/project_data/RNAseq/scripts/fastp_ahud.sh`. 
* Copy this to your `~/myscripts` directory. 
  * **Take a moment to meditate on working in linux. Can you copy this file from anywhere to anywhere? What is important?**
* Edit the script: All you need to do is define your samples with `$MYSAMP`. 
* Make sure you make a `~/myresults/fastp` directory first.
* This should only take about ~12 minutes for each of your sets of samples to complete, but lets start it in `tmux` to be safe.

First, lets talk through the code:
``` 

``` {}
#!/bin/bash   

# This script loops through a set of files defined by MYSAMP, matching left and right reads
# and cleans the raw data using fastp according to parameters set below

# cd to the location (path) to the fastq data:

cd /data/project_data/RNAseq/rawdata

# Define the sample code to anlayze
# Be sure to replace with your 5-6-digit sample code

MYSAMP="XXXXX"

# for each file that has "MYSAMP" and "_1.fq.gz" (read 1) in the name
# the wildcard here * allows for the different reps to be captured in the list
# start a loop with this file as the input:

for READ1 in ${MYSAMP}*_1.fq.gz
do

# the partner to this file (read 2) can be found by replacing the _1.fq.gz with _2.fq.gz
# second part of the input for PE reads

READ2=${READ1/_1.fq.gz/_2.fq.gz}

# make the output file names: print the fastq name, replace _# with _#_clean

NAME1=$(echo $READ1 | sed "s/_1/_1_clean/g")
NAME2=$(echo $READ2 | sed "s/_2/_2_clean/g")

# print the input and output to screen 

echo $READ1 $READ2
echo $NAME1 $NAME2

# call fastp
/data/popgen/fastp -i ${READ1} -I ${READ2} -o /data/project_data/RNAseq/cleandata/${NAME1} -O /data/project_data/RNAseq/cleandata/${NAME2} \
--detect_adapter_for_pe \
--trim_front1 24 \
--trim_poly_g \
--thread 1 \
--cut_right \
--cut_window_size 6 \
--qualified_quality_phred 20 \
--length_required 35 \
--html ~/myresults/fastqc/${NAME1}.html \
--json ~/myresults/fastqc/${NAME1}.json

done
```

* Now move the .html files to your local machine using FileZilla. 
* Lets record in a google sheet some important stats that we may want to report out in a manuscript! Total reads before filtering, total reads after filtering, % reads passed filters. 

### 5. Assess the quality of the reference transcriptome {-}

I previously assembled the _de novo_ transcriptome with these data using [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki). Lets look at some basic statistics of the assembly.

```
/data/popgen/trinityrnaseq-v2.13.2/util/TrinityStats.pl  /data/project_data/RNAseq/assembly/ahud_Trinity.fasta
```
Should yield:

```
################################
Counts of transcripts, etc.
################################
Total trinity genes:	130580
Total trinity transcripts:	349516
Percent GC: 35.57

########################################
Stats based on ALL transcript contigs:
########################################

	Contig N10: 4647
	Contig N20: 3149
	Contig N30: 2356
	Contig N40: 1791
	Contig N50: 1356

	Median contig length: 430
	Average contig: 801.91
	Total assembled bases: 280279107


#####################################################
Stats based on ONLY LONGEST ISOFORM per GENE:
#####################################################

	Contig N10: 4679
	Contig N20: 3115
	Contig N30: 2247
	Contig N40: 1613
	Contig N50: 1057

	Median contig length: 320
	Average contig: 626.33
	Total assembled bases: 81786351
```
We can also assess the completeness of the assembly using a program called [BUSCO](https://busco.ezlab.org/)

```
busco -m transcriptome -i ahud_Trinity.fasta -o BUSCOarthropoda -l arthropoda_odb10

```
```
$ cat short_summary.specific.arthropoda_odb10.BUSCOarthropoda.txt 
# BUSCO version is: 5.2.2 
# The lineage dataset is: arthropoda_odb10 (Creation date: 2020-09-10, number of genomes: 90, number of BUSCOs: 1013)
# Summarized benchmarking in BUSCO notation for file /data/project_data/RNAseq/assembly/ahud_Trinity.fasta
# BUSCO was run in mode: transcriptome

	***** Results: *****

	C:96.9%[S:7.1%,D:89.8%],F:1.1%,M:2.0%,n:1013	   
	982	Complete BUSCOs (C)			   
	72	Complete and single-copy BUSCOs (S)	   
	910	Complete and duplicated BUSCOs (D)	   
	11	Fragmented BUSCOs (F)			   
	20	Missing BUSCOs (M)			   
	1013	Total BUSCO groups searched		   

Dependencies and versions:
	hmmsearch: 3.1
	metaeuk: 5.34c21f2
```

### 6. Map to the reference transcriptome {-}
#### This chunk makes and preps the reference, note the {-} --prep_reference flag {-}
NOTE: Only one person needs to do this step. Whos the lucky person?
  ```
/data/popgen/trinityrnaseq-v2.13.2/util/align_and_estimate_abundance.pl --transcripts /data/project_data/RNAseq/assembly/ahud_Trinity.fasta \
--est_method salmon \
--trinity_mode \
--prep_reference
```
This should make two files:
  ```
ahud_Trinity.fasta.gene_trans_map
ahud_Trinity.fasta.salmon.idx
```
#### The chunk below maps to the reference using salmon {-}
* For this part you need to make the samples file `ahud_XXXXX.txt` with your set of samples.
* Copy the complete file from `/data/project_data/RNAseq/assembly/ahud_XXXXX.txt` to your local machine (or you could edit in `vim` but there will be a lot of deleting.
* Use a text editor to edit the file to only include your samples.
* Copy the file back to the server; Make sure you give the correct path to your samples file.
* NOTE: Run this with `tmux` and first navigate to the `/data/project_data/RNAseq/mapping` directory
```{}
cd /data/project_data/RNAseq/mapping
/data/popgen/trinityrnaseq-v2.13.2/util/align_and_estimate_abundance.pl --transcripts /data/project_data/RNAseq/assembly/ahud_Trinity.fasta \
--seqType fq \
--samples_file /data/project_data/RNAseq/assembly/ahud_XXXXX.txt \
--est_method salmon \
--output_dir /data/project_data/RNAseq/mapping \
--thread_count 1 \
--trinity_mode
```
Lets check the mapping rate of the clean reads to the trinity assembly.
```
grep -r --include \*.log -e Mapping rate
```
#### This chunk below assembles all the individually mapped reads into one data matrix {-}

We provide a file list to point to just the `quant.sf` files called `salmon_results_filelist.txt`.

```{}
/data/popgen/trinityrnaseq-v2.13.2/util/abundance_estimates_to_matrix.pl --est_method salmon \
  --gene_trans_map /data/project_data/RNAseq/assembly/ahud_Trinity.fasta.gene_trans_map \
  --quant_files /data/project_data/RNAseq/mapping/salmon_results_filelist.txt \
  --name_sample_by_basedir
```
Alright! With this matrix of number of reads that mapped to each contig/transcript, we can move to analyzing the data to test for differences in gene expression and more!



## Transcriptomics #3 {-}

### Objectives for today {-}
  
  1. Review the quality of our Illumina data from [our google sheet](https://docs.google.com/spreadsheets/d/1c2rOpUUuMiOiymu6foAX9n_mAH5re1pEtMNKLlXQ23w/edit?usp=sharing).
2. Assess our previously assembled _de novo_ transcriptome assembly using scripts provided through [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki).
3. Map reads and quantify abundance simultaneously using [Salmon](https://www.nature.com/articles/nmeth.4197).
4. Prepare data for import into DESeq2.
5. Start analyzing the gene expression data using [DESeq2](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html)



### 1. Review the quality of our Illumina data{-}

Lets check the amount and quality of our RNAseq data. We looked at some the fastp .html outputs before. Now lets just check [our google sheet](https://docs.google.com/spreadsheets/d/1c2rOpUUuMiOiymu6foAX9n_mAH5re1pEtMNKLlXQ23w/edit?usp=sharing). A good rule of thumb for RNAseq data when working with a diploid eukaryote is about ~20M reads per sample. 

### 2. Assess the quality of the reference transcriptome{-}

I previously assembled the _de novo_ transcriptome `ahud_Trinity.fasta` with these RNAseq data using [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki). It can be found in `/data/project_data/RNAseq/assembly/`. Lets look at some basic statistics of the assembly.

```
/data/popgen/trinityrnaseq-v2.13.2/util/TrinityStats.pl  /data/project_data/RNAseq/assembly/ahud_Trinity.fasta
```
Should yield:
  
```
################################
Counts of transcripts, etc.
################################
Total trinity 'genes':	130580
Total trinity transcripts:	349516
Percent GC: 35.57

########################################
Stats based on ALL transcript contigs:
########################################

	Contig N10: 4647
	Contig N20: 3149
	Contig N30: 2356
	Contig N40: 1791
	Contig N50: 1356

	Median contig length: 430
	Average contig: 801.91
	Total assembled bases: 280279107


#####################################################
Stats based on ONLY LONGEST ISOFORM per 'GENE':
#####################################################

	Contig N10: 4679
	Contig N20: 3115
	Contig N30: 2247
	Contig N40: 1613
	Contig N50: 1057

	Median contig length: 320
	Average contig: 626.33
	Total assembled bases: 81786351
```
We can also assess the completeness of the assembly using a program called [BUSCO](https://busco.ezlab.org/)

```
busco -m transcriptome -i /data/project_data/RNAseq/assembly/ahud_Trinity.fasta -o ~/myresults/BUSCO/BUSCOarthropoda -l arthropoda_odb10

```
* Then check your BUSCO results as shown below.
```
$ cat short_summary.specific.arthropoda_odb10.BUSCOarthropoda.txt 
# BUSCO version is: 5.2.2 
# The lineage dataset is: arthropoda_odb10 (Creation date: 2020-09-10, number of genomes: 90, number of BUSCOs: 1013)
# Summarized benchmarking in BUSCO notation for file /data/project_data/RNAseq/assembly/ahud_Trinity.fasta
# BUSCO was run in mode: transcriptome

	***** Results: *****

	C:96.9%[S:7.1%,D:89.8%],F:1.1%,M:2.0%,n:1013	   
	982	Complete BUSCOs (C)			   
	72	Complete and single-copy BUSCOs (S)	   
	910	Complete and duplicated BUSCOs (D)	   
	11	Fragmented BUSCOs (F)			   
	20	Missing BUSCOs (M)			   
	1013	Total BUSCO groups searched		   

Dependencies and versions:
	hmmsearch: 3.1
	metaeuk: 5.34c21f2
```

### 3. Map to the reference transcriptome  {-}
#### The code below makes and preps the reference, note the --prep_reference flag  {-}
NOTE: Only one person needs to do this step. Whos the lucky person?
* Navigate to the directory where the assembly lives: `/data/project_data/RNAseq/assembly`
```
/data/popgen/trinityrnaseq-v2.13.2/util/align_and_estimate_abundance.pl --transcripts /data/project_data/RNAseq/assembly/ahud_Trinity.fasta \
  --est_method salmon \
  --trinity_mode \
  --prep_reference
```
This should make two files:
```
ahud_Trinity.fasta.gene_trans_map
ahud_Trinity.fasta.salmon.idx
```
#### The code below maps to the reference using salmon {-}
* For this part you need to make the samples file `ahud_XXXXX.txt` with your set of samples.
* Copy the complete file from `/data/project_data/RNAseq/assembly/ahud_XXXXX.txt` to your local machine (or you could edit in `vim` but there will be a lot of deleting.
* Use a text editor to edit the file to only include your samples.
* Copy the file back to the server into your `~/mydata` directory; Update the script below to have your correct samples_file filename.
* **NOTE: Run this with `tmux` ** and first navigate to the `/data/project_data/RNAseq/mapping` directory
```
cd /data/project_data/RNAseq/mapping

/data/popgen/trinityrnaseq-v2.13.2/util/align_and_estimate_abundance.pl --transcripts /data/project_data/RNAseq/assembly/ahud_Trinity.fasta \
  --seqType fq \
  --samples_file ~/mydata/ahud_XXXXX.txt \
  --est_method salmon \
  --output_dir /data/project_data/RNAseq/mapping \
  --thread_count 1 \
  --trinity_mode
```

#### Explore mapping rate {-}
For each sample mapped, you now have a directory with several output files including a log of the run. In that log, the mapping rate (% of reads mapped with sufficient quality) is reported. We can view the contents of the file using `cat`. 
We can also use `grep` (i.e., regular expressions) to pull out the mapping rate for all the samples. Though theres probably a more elegant solution, here is one:
```
grep -r --include \*.log -e 'Mapping rate'
```
* How could we save this output?
* What is our mapping rate? Is this good/enough? What factors could affect mapping rate?


### 4. Prepare data for import into DESeq2. {-}

#### The code below assembles all the individually mapped reads into one data matrix {-}

We provide a file list to point to just the `quant.sf` files called `salmon_results_filelist.txt`.

* **NOTE: Only one person needs to run the code below. Whos the lucky person this time? **

```
cd /data/project_data/RNAseq/mapping

/data/popgen/trinityrnaseq-v2.13.2/util/abundance_estimates_to_matrix.pl --est_method salmon \
  --gene_trans_map /data/project_data/RNAseq/assembly/ahud_Trinity.fasta.gene_trans_map \
  --quant_files /data/project_data/RNAseq/mapping/salmon_results_filelist.txt \
  --name_sample_by_basedir
```
Alright! With this matrix of number of reads that mapped to each contig/transcript, we can move to analyzing the data to test for differences in gene expression and more!

* Move the counts data matrix to your individual machines using FileZilla. Also move the `ahud_samples_R.txt` file from `/data/project_data/RNAseq/mapping/` to your machine. This file is a table that associates each of our samples with their conditions (treatment, generation, replicate).

### 5.Analyze the gene expression data (a.k.a. counts data) using [DESeq2](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html)  {-}

Now we will work in R on our individual machines, each of us working with the complete data set (n=38, not just a subset of samples). There are [detailed tutorials available from the creators of DESeq2](https://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html).

Get set up; Load the packages/libraries we will likely need 
```{ }
Set your working directory
setwd("~/github/hudsonica")

Import the libraries that were likely to need in this session

if (!require("BiocManager", quietly = TRUE))
  install.packages("BiocManager")

BiocManager::install("DESeq2")

library(DESeq2)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(wesanderson)
library(vsn)  ### First: BiocManager::install("vsn") AND BiocManager::install("hexbin")
```
`Tools` -> `Install packages` -> Search for the library of interest; Install including dependencies.

 ### Import Counts Matrix and Sample ID tables into R and DESeq2 {-}
```{ }
# Import the counts matrix
countsTable <- read.table("data/salmon.isoform.counts.matrix", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)

countsTableRound <- round(countsTable) # bc DESeq2 doesn't like decimals (and Salmon outputs data with decimals)
head(countsTableRound)

#import the sample discription table
conds <- read.delim("ahud_samples_R.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1)
head(conds)
```

### Explore the counts data a bit {-}
```{ }
# Let's see how many reads we have from each sample
colSums(countsTableRound)
mean(colSums(countsTableRound))

barplot(colSums(countsTableRound), names.arg=colnames(countsTableRound),cex.names=0.5, las=3,ylim=c(0,20000000))
abline(h=mean(colSums(countsTableRound)), col="blue", lwd=2)

# the average number of counts per gene
rowSums(countsTableRound)
mean(rowSums(countsTableRound)) # [1] 11930.81 - tonsa, 6076.078 - hudsonica genes, 2269 - hudsonica isoform
median(rowSums(countsTableRound)) # [1] 2226 - tonsa, 582 - hudsonica, 109

apply(countsTableRound,2,mean) # 2 in the apply function does the action across columns
apply(countsTableRound,1,mean) # 1 in the apply function does the action across rows
hist(apply(countsTableRound,1,mean),xlim=c(0,1000), ylim=c(0,120000),breaks=10000)
```

### Create a DESeq object and define the experimental design {-}

```{ }
#### Create a DESeq object and define the experimental design here with the tilda

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData=conds, 
                              design= ~ treatment + generation)

dim(dds)

# Filter out genes with too few reads - remove all genes with counts < 15 in more than 75% of samples, so ~28)
## suggested by WGCNA on RNAseq FAQ

dds <- dds[rowSums(counts(dds) >= 30) >= 28,]
nrow(dds) 

# Run the DESeq model to test for differential gene expression
dds <- DESeq(dds)

# List the results you've generated
resultsNames(dds)
```



### Visualize the global gene expression patterns using PCA {-}
```{}
###############################################################
# PCA to visualize global gene expression patterns

# First normalize the data using variance stabilization
vsd <- vst(dds, blind=FALSE)

data <- plotPCA(vsd, intgroup=c("treatment","generation"), returnData=TRUE)
percentVar <- round(100 * attr(data,"percentVar"))

###########  

dataF0 <- subset(data, generation == 'F0')

F0 <- ggplot(dataF0, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,22,23,24), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A", "#CC3333"), labels = c("Ambient", "Acidification","Warming", "OWA"))+
##theme(legend.position = c(0.83,0.85), legend.background = element_blank(), legend.box.background = element_rect(colour = "black")) +
#guides(shape = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
#guides(fill = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
#guides(shape = guide_legend(override.aes = list(size = 5)))+
theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())

F0


png("PCA_F0.png", res=300, height=5, width=5, units="in")

ggarrange(F0, nrow = 1, ncol=1)

dev.off()

################# F2

dataF2 <- subset(data, generation == 'F2')

F2 <- ggplot(dataF2, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,22,23), labels = c("Ambient", "Acidification","Warming"))+
  # scale_color_manual(values = c('#6699CC',"#F2AD00","#00A08A", "#CC3333")) + 
  #scale_color_manual(values=c('black')) +
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A"), labels = c("Ambient", "Acidification","Warming"))+
  #theme(legend.position = c(0.83,0.85), legend.background = element_blank(), legend.box.background = element_rect(colour = "black")) +
  #scale_size(guide="none") +
  guides(shape = guide_legend(override.aes = list(shape = c( 21,22, 23))))+
  guides(fill = guide_legend(override.aes = list(shape = c( 21,22, 23))))+
  guides(shape = guide_legend(override.aes = list(size = 5)))+
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())
F2


png("PCA_F2.png", res=300, height=5, width=5, units="in")

ggarrange(F2, nrow = 1, ncol=1)

dev.off()

# Yes - F2 is missing one ambient replicate

################################ F4

dataF4 <- subset(data, generation == 'F4')

F4 <- ggplot(dataF4, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,22,23,24), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  # scale_color_manual(values = c('#6699CC',"#F2AD00","#00A08A", "#CC3333")) + 
  #scale_color_manual(values=c('black')) +
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A", "#CC3333"), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  #theme(legend.position = c(0.83,0.85), legend.background = element_blank(), legend.box.background = element_rect(colour = "black")) +
  #scale_size(guide="none") +
  guides(shape = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
  guides(fill = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
  guides(shape = guide_legend(override.aes = list(size = 5)))+
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())
F4


png("PCA_F4.png", res=300, height=5, width=5, units="in")

ggarrange(F4, nrow = 1, ncol=1)

dev.off()


################# F11

dataF11 <- subset(data, generation == 'F11')

F11 <- ggplot(dataF11, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,24), labels = c("Ambient", "OWA"))+
  # scale_color_manual(values = c('#6699CC',"#F2AD00","#00A08A", "#CC3333")) + 
  #scale_color_manual(values=c('black')) +
  scale_fill_manual(values=c('#6699CC', "#CC3333"), labels = c("Ambient", "OWA"))+
  #theme(legend.position = c(0.83,0.85), legend.background = element_blank(), legend.box.background = element_rect(colour = "black")) +
  #scale_size(guide="none") +
  guides(shape = guide_legend(override.aes = list(shape = c( 21, 24))))+
  guides(fill = guide_legend(override.aes = list(shape = c( 21, 24))))+
  guides(shape = guide_legend(override.aes = list(size = 5)))+
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())
F11


png("PCA_F11.png", res=300, height=5, width=5, units="in")

ggarrange(F11, nrow = 1, ncol=1)

dev.off()
```





## Transcriptomics #4 {-}

### Objectives for today  {-}
  
  1. Assemble the quant.sf files generated by salmon into a matrix to import into DESeq2.
2. Start analyzing the gene expression data using [DESeq2](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html)



### 1. Let's check our mapping rates {-}
For each sample mapped, you now have a directory with several output files including a log of the run. In that log, the mapping rate (% of reads mapped with sufficient quality) is reported. We can view the contents of the file using `cat`. 
We can also use `grep` (i.e., regular expressions) to pull out the mapping rate for all the samples. Though theres probably a more elegant solution, here is one:

```{}
grep -r --include \*.log -e 'Mapping rate'
```
* How could we save this output?
* What is our mapping rate? Is this good/enough? What factors could affect mapping rate?


### Prepare data for import into DESeq2. {-}

#### The code below assembles all the individually mapped reads into one data matrix {-}

We provide a file list to point to just the `quant.sf` files called `salmon_results_filelist.txt`.

* **NOTE: Only one person needs to run the code below. Whos the lucky person this time? **
  
  ```
cd /data/project_data/RNAseq/mapping

/data/popgen/trinityrnaseq-v2.13.2/util/abundance_estimates_to_matrix.pl --est_method salmon \
--gene_trans_map /data/project_data/RNAseq/assembly/ahud_Trinity.fasta.gene_trans_map \
--quant_files /data/project_data/RNAseq/mapping/salmon_results_filelist.txt \
--name_sample_by_basedir
```
Alright! With this matrix of number of reads that mapped to each contig/transcript, we can move to analyzing the data to test for differences in gene expression and more!
  
  * Move the counts data matrix to your individual machines using FileZilla. Also move the `ahud_samples_R.txt` file from `/data/project_data/RNAseq/mapping/` to your machine. This file is a table that associates each of our samples with their conditions (treatment, generation, replicate).

## 2. Analyze the gene expression data (a.k.a. counts data) using [DESeq2](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) {-}

Now we will work in R on our individual machines, each of us working with the complete data set (n=38, not just a subset of samples). There are [detailed tutorials available from the creators of DESeq2](https://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html).

### Get set up; Load the packages/libraries we will likely need  {-}
```{ }
## Set your working directory
setwd("~/github/hudsonica")

## Import the libraries that we're likely to need in this session

if (!require("BiocManager", quietly = TRUE))
  install.packages("BiocManager")

BiocManager::install("DESeq2")

library(DESeq2)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(wesanderson)
library(vsn)  ### First: BiocManager::install("vsn") AND BiocManager::install("hexbin")
```
`Tools` -> `Install packages` -> Search for the library of interest; Install including dependencies.

### Import Counts Matrix and Sample ID tables into R and DESeq2  {-}
```{ }
# Import the counts matrix
countsTable <- read.table("data/salmon.isoform.counts.matrix", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)

countsTableRound <- round(countsTable) # bc DESeq2 doesn't like decimals (and Salmon outputs data with decimals)
head(countsTableRound)

#import the sample discription table
conds <- read.delim("ahud_samples_R.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1)
head(conds)
```

### Explore the counts data a bit {-}
```{ }
# Let's see how many reads we have from each sample
colSums(countsTableRound)
mean(colSums(countsTableRound))

barplot(colSums(countsTableRound), names.arg=colnames(countsTableRound),cex.names=0.5, las=3,ylim=c(0,20000000))
abline(h=mean(colSums(countsTableRound)), col="blue", lwd=2)

# the average number of counts per gene
rowSums(countsTableRound)
mean(rowSums(countsTableRound)) # [1] 11930.81 - tonsa, 6076.078 - hudsonica genes, 2269 - hudsonica isoform
median(rowSums(countsTableRound)) # [1] 2226 - tonsa, 582 - hudsonica, 109

apply(countsTableRound,2,mean) # 2 in the apply function does the action across columns
apply(countsTableRound,1,mean) # 1 in the apply function does the action across rows
hist(apply(countsTableRound,1,mean),xlim=c(0,1000), ylim=c(0,120000),breaks=10000)
```

### Create a DESeq object and define the experimental design {-}
```{ }
#### Create a DESeq object and define the experimental design here with the tilda

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData=conds, 
                              design= ~ generation + treatment)

dim(dds)

# Filter out genes with too few reads - remove all genes with counts < 15 in more than 75% of samples, so ~28)
## suggested by WGCNA on RNAseq FAQ

dds <- dds[rowSums(counts(dds) >= 30) >= 28,]
nrow(dds) 

# Run the DESeq model to test for differential gene expression
dds <- DESeq(dds)

# List the results you've generated
resultsNames(dds)
```

### Check the quality of the data by sample clustering and visualization {-}
```{ }
###############################################################

# Check the quality of the data by sample clustering and visualization

library("pheatmap")
library("vsn")

# this gives log2(n + 1)
ntd <- normTransform(dds)
meanSdPlot(assay(ntd))

vsd <- vst(dds, blind=FALSE)
meanSdPlot(assay(vsd))


sampleDists <- dist(t(assay(vsd)))

library("RColorBrewer")
sampleDistMatrix <- as.matrix(sampleDists)
rownames(sampleDistMatrix) <- paste(vsd$treatment, vsd$generation, sep="-")
colnames(sampleDistMatrix) <- NULL
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
pheatmap(sampleDistMatrix,
         clustering_distance_rows=sampleDists,
         clustering_distance_cols=sampleDists,
         col=colors)
```

### Visualize the global gene expression patterns using PCA {-}
```{ }
###############################################################

# PCA to visualize global gene expression patterns

# first transform the data for plotting using variance stabilization
vsd <- vst(dds, blind=FALSE)

pcaData <- plotPCA(vsd, intgroup=c("treatment","generation"), returnData=TRUE)
percentVar <- round(100 * attr(data,"percentVar"))

ggplot(pcaData, aes(PC1, PC2, color=treatment, shape=generation)) +
  geom_point(size=3) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) + 
  coord_fixed()
```

### Let's plot the PCA by generation in four panels {-}
```{ }
###############################################################

# Let's plot the PCA by generation in four panels

data <- plotPCA(vsd, intgroup=c("treatment","generation"), returnData=TRUE)
percentVar <- round(100 * attr(data,"percentVar"))

###########  

dataF0 <- subset(data, generation == 'F0')

F0 <- ggplot(dataF0, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,22,23,24), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A", "#CC3333"), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  ##theme(legend.position = c(0.83,0.85), legend.background = element_blank(), legend.box.background = element_rect(colour = "black")) +
  #guides(shape = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
  #guides(fill = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
  #guides(shape = guide_legend(override.aes = list(size = 5)))+
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())

F0


#png("PCA_F0.png", res=300, height=5, width=5, units="in")

#ggarrange(F0, nrow = 1, ncol=1)

#dev.off()

################# F2

dataF2 <- subset(data, generation == 'F2')

F2 <- ggplot(dataF2, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,22,23), labels = c("Ambient", "Acidification","Warming"))+
  # scale_color_manual(values = c('#6699CC',"#F2AD00","#00A08A", "#CC3333")) + 
  #scale_color_manual(values=c('black')) +
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A"), labels = c("Ambient", "Acidification","Warming"))+
  theme(legend.position = c(0.83,0.85), legend.background = element_blank(), legend.box.background = element_rect(colour = "black")) +
  #scale_size(guide="none") +
  guides(shape = guide_legend(override.aes = list(shape = c( 21,22, 23))))+
  guides(fill = guide_legend(override.aes = list(shape = c( 21,22, 23))))+
  guides(shape = guide_legend(override.aes = list(size = 5)))+
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())
F2


# png("PCA_F2.png", res=300, height=5, width=5, units="in")
# 
# ggarrange(F2, nrow = 1, ncol=1)
# 
# dev.off()

# Yes - F2 is missing one ambient replicate

################################ F4

dataF4 <- subset(data, generation == 'F4')

F4 <- ggplot(dataF4, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,22,23,24), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  # scale_color_manual(values = c('#6699CC',"#F2AD00","#00A08A", "#CC3333")) + 
  #scale_color_manual(values=c('black')) +
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A", "#CC3333"), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  #theme(legend.position = c(0.83,0.85), legend.background = element_blank(), legend.box.background = element_rect(colour = "black")) +
  #scale_size(guide="none") +
  guides(shape = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
  guides(fill = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
  guides(shape = guide_legend(override.aes = list(size = 5)))+
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())
F4


# png("PCA_F4.png", res=300, height=5, width=5, units="in")
# 
# ggarrange(F4, nrow = 1, ncol=1)
# 
# dev.off()


################# F11

dataF11 <- subset(data, generation == 'F11')

F11 <- ggplot(dataF11, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,24), labels = c("Ambient", "OWA"))+
  scale_fill_manual(values=c('#6699CC', "#CC3333"), labels = c("Ambient", "OWA"))+
  guides(shape = guide_legend(override.aes = list(shape = c( 21, 24))))+
  guides(fill = guide_legend(override.aes = list(shape = c( 21, 24))))+
  guides(shape = guide_legend(override.aes = list(size = 5)))+
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())
F11


# png("PCA_F11.png", res=300, height=5, width=5, units="in")
# 
# ggarrange(F11, nrow = 1, ncol=1)
# 
# dev.off()

ggarrange(F0, F2, F4, F11, nrow = 2, ncol=2)
```


### Now that we have a sense of our data, let's explore models and contrasts for testing for differential expression {-}

```{ }
## Check on the DE results from the DESeq command way above ##

resAM_OWA <- results(dds, name="treatment_OWA_vs_AM", alpha=0.05)

resAM_OWA <- resAM_OWA[order(resAM_OWA$padj),]
head(resAM_OWA)  

summary(resAM_OWA)


resAM_OW <- results(dds, name="treatment_OW_vs_AM", alpha=0.05)

resAM_OW <- resAM_OW[order(resAM_OW$padj),]
head(resAM_OW)  

summary(resAM_OW)


### Plot Individual genes ### 

# Counts of specific top interaction gene! (important validatition that the normalization, model is working)
d <-plotCounts(dds, gene="TRINITY_DN2919_c0_g1_i12", intgroup = (c("treatment","generation")), returnData=TRUE)
d

p <-ggplot(d, aes(x=treatment, y=count, color=treatment, shape=generation)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.2,h=0), size=3)
p <- p + stat_summary(fun = mean, geom = "point", size=5, alpha=0.7) 
p
```

### Let's try another model for testing DGE  {-}

```{ }

#################### MODEL NUMBER 2 - subset to focus on effect of treatment for each generation

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData=conds, 
                              design= ~ treatment)

dim(dds)
# [1] 130580     38

# Filter 
dds <- dds[rowSums(counts(dds) >= 30) >= 28,]
nrow(dds) 

# Subset the DESeqDataSet to the specific level of the "generation" factor
dds_sub <- subset(dds, select = generation == 'F0')
dim(dds_sub)

# Perform DESeq2 analysis on the subset
dds_sub <- DESeq(dds_sub)

resultsNames(dds_sub)

res_F0_AMvOW <- results(dds_sub, name="treatment_OW_vs_AM", alpha=0.05)

res_F0_AMvOW <- res_F0_AMvOW[order(res_F0_AMvOW$padj),]
head(res_F0_AMvOW) 

summary(res_F0_AMvOW)


### Plot Individual genes ### 

# Counts of specific top interaction gene! (important validatition that the normalization, model is working)
d <-plotCounts(dds, gene="TRINITY_DN3821_c0_g1_i2", intgroup = (c("treatment","generation")), returnData=TRUE)
d

p <-ggplot(d, aes(x=treatment, y=count, color=treatment, shape=generation)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.2,h=0), size=3)
p <- p + stat_summary(fun = mean, geom = "point", size=5, alpha=0.7) 
p
```

### We can make an MA (or sideways volcano) plot now that we're contrasting just two groups {-}
```{ }
plotMA(res_F0_AMvOW, ylim=c(-4,4))
```

### We can also make a heatmap of the DEGs {-}
```{ }
########################################

# Heatmap of top 20 genes sorted by pvalue

library(pheatmap)

vsd <- vst(dds_sub, blind=FALSE)

topgenes <- head(rownames(res_F0_AMvOW),20)
mat <- assay(vsd)[topgenes,]
mat <- mat - rowMeans(mat)
df <- as.data.frame(colData(dds_sub)[,c("generation","treatment")])
pheatmap(mat, annotation_col=df)
```



## Transcriptomics #5 {-}


#### Objectives for today  {-}
  
  1. Review results from filtering of the de novo assembly and transfer the new gene expression matrix. 
2. Explore patterns in the gene expression data using [DESeq2](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html): explore data, plot PCA, test for DGE, make a Venn diagram.

### 1. Review de novo transcriptome assembly filtering results {-}
Our initial assembly has ~350,000 isoforms and ~180,000 genes due to the large amount of genetic diversity among the 100s of individuals sequenced and potential isoforms/splice variants among the different treatment conditions. I clustered the initial assembly based on 95% sequence similarity using CD-HIT-EST (v4.6.6-2016-0711) and ran Transdecoder to filter down to only open reading frames. The code I used is below for your reference.

```         
/data/popgen/cd-hit-v4.6.6-2016-0711/cd-hit-est -i /data/project_data/RNAseq/archive/assembly/ahud_Trinity.fasta -o ahud_Trinity_95.fa -c 0.95 -n 10 -d 0 -M 16000 -T 8
```
Where,
-   c % sequence similarity
-   n word size, -n 10, 11 for thresholds 0.95 \~ 1.0

```         
/data/popgen/trinityrnaseq-v2.13.2/util/TrinityStats.pl  /data/project_data/RNAseq/archive/assembly/ahud_Trinity_95.fa


################################
## Counts of transcripts, etc.
################################
Total trinity 'genes':  118421
Total trinity transcripts:  195476
Percent GC: 34.10

########################################
Stats based on ALL transcript contigs:
  ########################################

Contig N10: 4403
Contig N20: 2913
Contig N30: 2114
Contig N40: 1537
Contig N50: 1069

Median contig length: 371
Average contig: 679.44
Total assembled bases: 132813467


#####################################################
## Stats based on ONLY LONGEST ISOFORM per 'GENE':
#####################################################

Contig N10: 4778
Contig N20: 3209
Contig N30: 2338
Contig N40: 1708
Contig N50: 1166

Median contig length: 333
Average contig: 661.14
Total assembled bases: 78292692
```
The contig length in this filtered assembly is a little bit longer than before. Now run Transdecoder to get Open Reading Frames:
  ```         
/data/popgen/TransDecoder-3.0.1/TransDecoder.LongOrfs -t /data/project_data/RNAseq/archive/assembly/ahud_Trinity_95.fa
```

Completed! Now check the assembly stats:
  
  ```         
/data/popgen/trinityrnaseq-v2.13.2/util/TrinityStats.pl  /data/project_data/RNAseq/archive/assembly/ahud_Trinity_95.fa.transdecoder_dir/longest_orfs.cds


################################
## Counts of transcripts, etc.
################################
Total trinity 'genes':  27960
Total trinity transcripts:  67916
Percent GC: 41.50

########################################
Stats based on ALL transcript contigs:
  ########################################

Contig N10: 4446
Contig N20: 3027
Contig N30: 2247
Contig N40: 1749
Contig N50: 1386

Median contig length: 531
Average contig: 925.37
Total assembled bases: 62847294


#####################################################
## Stats based on ONLY LONGEST ISOFORM per 'GENE':
#####################################################

Contig N10: 5154
Contig N20: 3576
Contig N30: 2751
Contig N40: 2175
Contig N50: 1761

Median contig length: 792
Average contig: 1210.45
Total assembled bases: 33844206
```

Woo hoo!!! It looks a lot longer!
  
  Given BUSCO issues on our server, I found a way to submit the BUSCO job online: <https://gvolante.riken.jp/> Submitted at 10pm 10/20, finished at 8am 10/21

```         
SHORT SUMMARY OF THE PROJECT:
  Job ID  202310211036-Q2PWKD8FRLY1CHJY
Project name    Ahud_95_ORF
Selected program    BUSCO_v5
Selected ortholog set   Arthropoda

COMPLETENESS ASSESSMENT RESULTS:
  Total number of core genes queried  1013
Number of core genes detected
Complete  980 (96.74%)
Complete + Partial    992 (97.93%)
Number of missing core genes    21 (2.07%)
Average number of orthologs per core gene   1.59
% of detected core genes that have more than 1 ortholog 36.12
Scores in BUSCO format  C:96.7%[S:61.8%,D:34.9%],F:1.2%,M:2.1%
```

The percent single copy and duplicated dramatically improved,from **S:7.1%,D:89.8% to S:61.8%,D:34.9%!** Much better! So I remapped our cleaned reads to this new assembly using the salmon.


|                                   | Complete assembly | Filtered assembly |
  |-----------------------------------|-------------------|-------------------|
  | Number of transcripts             | 349,516           | 67,916            |
  | N50                               | 1,057             | 1,761             |
  | Mean length                       | 626.33            | 1210.45           |
  | BUSCO % complete                  | 96.9%             | 96.74%            |
  | BUSCO Single                      | 7.1%,             | 61.8%             |
  | BUSCO Duplicated                  | 89.8%             | 34.9%             |
  | \% mapping                        | 98%               | 66%               |
  | transcripts after depth filtering | 42,575            | 20,598            |
  
  * **Use Filezilla to copy over the new counts matrix `salmon.isoform.counts.matrix.filteredAssembly` from `/data/project_data/RNAseq/mapping`.**
  
  ### 2. Analyze the gene expression data (a.k.a. counts data) using [DESeq2](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) {-}
  
  Now we will work in R on our individual machines, each of us working with the complete data set (n=38, not just a subset of samples). There are [detailed tutorials available from the creators of DESeq2](https://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html).

Get set up; Load the packages/libraries we will likely need 

```{ }
## Set your working directory
setwd("YOUR/WORKING/DIRECTORY")

## Import the libraries that we're likely to need in this session

library(DESeq2)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(wesanderson)
library(vsn)  
```

### A. Import Counts Matrix and Sample ID tables into R and DESeq2 {-}
**Make sure to replace the counts matrix you are importing!**
  ```{ }
# Import the counts matrix
countsTable <- read.table("salmon.isoform.counts.matrix.filteredAssembly", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)

countsTableRound <- round(countsTable) # bc DESeq2 doesn't like decimals (and Salmon outputs data with decimals)
head(countsTableRound)

#import the sample discription table
conds <- read.delim("ahud_samples_R.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1)
head(conds)
```

## B. Explore the counts data with simple calculations and plots
```{ }
# Let's see how many reads we have from each sample
colSums(countsTableRound)
mean(colSums(countsTableRound))

barplot(colSums(countsTableRound), names.arg=colnames(countsTableRound),cex.names=0.5, las=3,ylim=c(0,20000000))
abline(h=mean(colSums(countsTableRound)), col="blue", lwd=2)

# the average number of counts per gene
rowSums(countsTableRound)
mean(rowSums(countsTableRound)) # [1] 6076.078 - hudsonica genes, 2269 - hudsonica isoform, 8218 - hudsonica filtered
median(rowSums(countsTableRound)) # [1] 582 - hudsonica, 109 - hudsonica isoforms, 377 - hudsonica filtered

apply(countsTableRound,2,mean) # 2 in the apply function does the action across columns
apply(countsTableRound,1,mean) # 1 in the apply function does the action across rows
hist(apply(countsTableRound,1,mean),xlim=c(0,1000), ylim=c(0,50000),breaks=10000)
```

### C. Create a DESeq object, define the experimental design, and filter by depth {-}
```{ }
#### Create a DESeq object and define the experimental design here with the tilda

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData=conds, 
                              design= ~ generation + treatment)

dim(dds)

# Filter out genes with too few reads - remove all genes with counts < 15 in more than 75% of samples, so ~28)
## suggested by WGCNA on RNAseq FAQ

dds <- dds[rowSums(counts(dds) >= 30) >= 28,]
nrow(dds) 

# Run the DESeq model to test for differential gene expression
dds <- DESeq(dds)

# List the results you've generated
resultsNames(dds)
```

#### D. Check the quality of the data by sample clustering and visualization  {-}
```{ }
###############################################################

# Check the quality of the data by sample clustering and visualization
# The goal of transformation "is to remove the dependence of the variance on the mean, particularly the high variance of the logarithm of count data when the mean is low."

library("pheatmap")
library("vsn")

# this gives log2(n + 1)
ntd <- normTransform(dds)
meanSdPlot(assay(ntd))

# Variance stabilizing transformation
vsd <- vst(dds, blind=FALSE)
meanSdPlot(assay(vsd))


sampleDists <- dist(t(assay(vsd)))

library("RColorBrewer")
sampleDistMatrix <- as.matrix(sampleDists)
rownames(sampleDistMatrix) <- paste(vsd$treatment, vsd$generation, sep="-")
colnames(sampleDistMatrix) <- NULL
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
pheatmap(sampleDistMatrix,
         clustering_distance_rows=sampleDists,
         clustering_distance_cols=sampleDists,
         col=colors)
```

#### Visualize the global gene expression patterns using PCA {-}
```{ }
###############################################################

# PCA to visualize global gene expression patterns

# first transform the data for plotting using variance stabilization
vsd <- vst(dds, blind=FALSE)

pcaData <- plotPCA(vsd, intgroup=c("treatment","generation"), returnData=TRUE)
percentVar <- round(100 * attr(data,"percentVar"))

ggplot(pcaData, aes(PC1, PC2, color=treatment, shape=generation)) +
  geom_point(size=3) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) + 
  coord_fixed()
```

#### Make a more advanced PCA plot and plot by generation in four panels {-}
```{ }
###############################################################

# Let's plot the PCA by generation in four panels

data <- plotPCA(vsd, intgroup=c("treatment","generation"), returnData=TRUE)
percentVar <- round(100 * attr(data,"percentVar"))

###########  

dataF0 <- subset(data, generation == 'F0')

F0 <- ggplot(dataF0, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-10, 25) + xlim(-40, 10)+ # zoom for F0 with new assembly
  #ylim(-40, 25) + xlim(-50, 50)+ # new assembly limits
  #ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,22,23,24), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A", "#CC3333"), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  ##theme(legend.position = c(0.83,0.85), legend.background = element_blank(), legend.box.background = element_rect(colour = "black")) +
  #guides(shape = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
  #guides(fill = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
  #guides(shape = guide_legend(override.aes = list(size = 5)))+
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())

F0


#png("PCA_F0.png", res=300, height=5, width=5, units="in")

#ggarrange(F0, nrow = 1, ncol=1)

#dev.off()

################# F2

dataF2 <- subset(data, generation == 'F2')

F2 <- ggplot(dataF2, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-40, 25) + xlim(-50, 55)+
  #ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,22,23), labels = c("Ambient", "Acidification","Warming"))+
  # scale_color_manual(values = c('#6699CC',"#F2AD00","#00A08A", "#CC3333")) + 
  #scale_color_manual(values=c('black')) +
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A"), labels = c("Ambient", "Acidification","Warming"))+
  theme(legend.position = c(0.83,0.85), legend.background = element_blank(), legend.box.background = element_rect(colour = "black")) +
  #scale_size(guide="none") +
  guides(shape = guide_legend(override.aes = list(shape = c( 21,22, 23))))+
  guides(fill = guide_legend(override.aes = list(shape = c( 21,22, 23))))+
  guides(shape = guide_legend(override.aes = list(size = 5)))+
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())
F2


# png("PCA_F2.png", res=300, height=5, width=5, units="in")
# 
# ggarrange(F2, nrow = 1, ncol=1)
# 
# dev.off()

# Yes - F2 is missing one ambient replicate

################################ F4

dataF4 <- subset(data, generation == 'F4')

F4 <- ggplot(dataF4, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-40, 25) + xlim(-50, 55)+ # limits with filtered assembly
  #ylim(-20, 10) + xlim(-40, 25)+  # zoom with filtered assembly
  #ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,22,23,24), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  # scale_color_manual(values = c('#6699CC',"#F2AD00","#00A08A", "#CC3333")) + 
  #scale_color_manual(values=c('black')) +
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A", "#CC3333"), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  #theme(legend.position = c(0.83,0.85), legend.background = element_blank(), legend.box.background = element_rect(colour = "black")) +
  #scale_size(guide="none") +
  guides(shape = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
  guides(fill = guide_legend(override.aes = list(shape = c( 21,22, 23, 24))))+
  guides(shape = guide_legend(override.aes = list(size = 5)))+
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())
F4


# png("PCA_F4.png", res=300, height=5, width=5, units="in")
# 
# ggarrange(F4, nrow = 1, ncol=1)
# 
# dev.off()


################# F11

dataF11 <- subset(data, generation == 'F11')

F11 <- ggplot(dataF11, aes(PC1, PC2)) +
  geom_point(size=10, stroke = 1.5, aes(fill=treatment, shape=treatment)) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  ylim(-40, 25) + xlim(-50, 55)+
  #ylim(-40, 20) + xlim(-50, 30)+
  scale_shape_manual(values=c(21,24), labels = c("Ambient", "OWA"))+
  scale_fill_manual(values=c('#6699CC', "#CC3333"), labels = c("Ambient", "OWA"))+
  guides(shape = guide_legend(override.aes = list(shape = c( 21, 24))))+
  guides(fill = guide_legend(override.aes = list(shape = c( 21, 24))))+
  guides(shape = guide_legend(override.aes = list(size = 5)))+
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(legend.title = element_blank())
F11


# png("PCA_F11.png", res=300, height=5, width=5, units="in")
# 
# ggarrange(F11, nrow = 1, ncol=1)
# 
# dev.off()

ggarrange(F0, F2, F4, F11, nrow = 2, ncol=2)
```


#### E. Explore differential expression with full model, GE ~ generation + treatment  {-}

Now that we have a sense of our data, lets explore models and contrasts for testing for differential expression

```{ }
## Check on the DE results from the DESeq command way above ##

resAM_OWA <- results(dds, name="treatment_OWA_vs_AM", alpha=0.05)

resAM_OWA <- resAM_OWA[order(resAM_OWA$padj),]
head(resAM_OWA)  

summary(resAM_OWA)


resAM_OW <- results(dds, name="treatment_OW_vs_AM", alpha=0.05)

resAM_OW <- resAM_OW[order(resAM_OW$padj),]
head(resAM_OW)  

summary(resAM_OW)

```
Its important to check the expression patterns of the top differentially genes to know that the model and tests are pulling out patterns you expect.

```{ }

### Plot Individual genes ### 

# Counts of specific top interaction gene! (important validatition that the normalization, model is working)
d <-plotCounts(dds, gene="TRINITY_DN3600_c0_g1::TRINITY_DN3600_c0_g1_i2::g.16079::m.16079", intgroup = (c("treatment","generation")), returnData=TRUE)
d

p <-ggplot(d, aes(x=treatment, y=count, color=treatment, shape=generation)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.2,h=0), size=3)
p <- p + stat_summary(fun = mean, geom = "line")
p <- p + stat_summary(fun = mean, geom = "point", size=5, alpha=0.7) 
p
```
So, the above model finds differences between treatment groups that are largely consistent across generations. We know based on our PCA that the responses to treatments are quite different across generations. So lets try subsetting our data by generation and testing for differential expression.

#### F. Run another model that focuses on treatment effects for each generation {-}

```{ }

#################### MODEL NUMBER 2 - subset to focus on effect of treatment for each generation

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData=conds, 
                              design= ~ treatment)

dim(dds)
# [1] 130580     38

# Filter 
dds <- dds[rowSums(counts(dds) >= 30) >= 28,]
nrow(dds) 

# Subset the DESeqDataSet to the specific level of the "generation" factor
dds_sub <- subset(dds, select = generation == 'F0')
dim(dds_sub)

# Perform DESeq2 analysis on the subset
dds_sub <- DESeq(dds_sub)

resultsNames(dds_sub)

res_F0_OWvAM <- results(dds_sub, name="treatment_OW_vs_AM", alpha=0.05)

res_F0_OWvAM <- res_F0_OWvAM[order(res_F0_OWvAM$padj),]
head(res_F0_OWvAM)

summary(res_F0_OWvAM)


### Plot Individual genes ### 

# Counts of specific top interaction gene! (important validatition that the normalization, model is working)
d <-plotCounts(dds_sub, gene="TRINITY_DN30_c0_g2::TRINITY_DN30_c0_g2_i1::g.130::m.130", intgroup = (c("treatment","generation")), returnData=TRUE)
d

p <-ggplot(d, aes(x=treatment, y=count, color=treatment, shape=generation)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.2,h=0), size=3)
p <- p + stat_summary(fun = mean, geom = "point", size=5, alpha=0.7) 
p
```

### We can make an MA (or sideways volcano) plot now that we're contrasting just two groups {-}
```{ }

### We can make an MA plot
plotMA(res_F0_OWvAM, ylim=c(-4,4))
```

### We can also make a heatmap of the DEGs {-}
```{ }
########################################

# Heatmap of top 20 genes sorted by pvalue

library(pheatmap)

# By environment
vsd <- vst(dds_sub, blind=FALSE)

topgenes <- head(rownames(res_F0_OWvAM),20)
mat <- assay(vsd)[topgenes,]
mat <- mat - rowMeans(mat)
df <- as.data.frame(colData(dds_sub)[,c("generation","treatment")])
pheatmap(mat, annotation_col=df)
pheatmap(mat, annotation_col=df, cluster_cols = F)
```

### Let's make a Venn (or Euler) Diagram {-}

Last but not least, since we have three contrasts, lets make a Venn (or Euler) Diagram to see how similar or different the DGE is from Ambient for OA, OW, and OWA at F0!

```{ }
#################################################################

#### PLOT OVERLAPPING DEGS IN VENN EULER DIAGRAM

#################################################################

# For OW vs AM
res_F0_OWvAM <- results(dds_sub, name="treatment_OW_vs_AM", alpha=0.05)
res_F0_OWvAM <- res_F0_OWvAM[order(res_F0_OWvAM$padj),]
head(res_F0_OWvAM)

summary(res_F0_OWvAM)
res_F0_OWvAM <- res_F0_OWvAM[!is.na(res_F0_OWvAM$padj),]
degs_F0_OWvAM <- row.names(res_F0_OWvAM[res_F0_OWvAM$padj < 0.05,])

# For OA vs AM
res_F0_OAvAM <- results(dds_sub, name="treatment_OA_vs_AM", alpha=0.05)
res_F0_OAvAM <- res_F0_OAvAM[order(res_F0_OAvAM$padj),]
head(res_F0_OAvAM)

summary(res_F0_OAvAM)
res_F0_OAvAM <- res_F0_OAvAM[!is.na(res_F0_OAvAM$padj),]
degs_F0_OAvAM <- row.names(res_F0_OAvAM[res_F0_OAvAM$padj < 0.05,])

# For OWA vs AM
res_F0_OWAvAM <- results(dds_sub, name="treatment_OWA_vs_AM", alpha=0.05)
res_F0_OWAvAM <- res_F0_OWAvAM[order(res_F0_OWAvAM$padj),]
head(res_F0_OWAvAM)

summary(res_F0_OWAvAM)
res_F0_OWAvAM <- res_F0_OWAvAM[!is.na(res_F0_OWAvAM$padj),]
degs_F0_OWAvAM <- row.names(res_F0_OWAvAM[res_F0_OWAvAM$padj < 0.05,])

library(eulerr)

# Total
length(degs_F0_OAvAM)  # 520
length(degs_F0_OWvAM)  # 4841 
length(degs_F0_OWAvAM)  # 3742

# Intersections
length(intersect(degs_F0_OAvAM,degs_F0_OWvAM))  # 387
length(intersect(degs_F0_OAvAM,degs_F0_OWAvAM))  # 340
length(intersect(degs_F0_OWAvAM,degs_F0_OWvAM))  # 2585

intWA <- intersect(degs_F0_OAvAM,degs_F0_OWvAM)
length(intersect(degs_F0_OWAvAM,intWA)) # 308

# Number unique

520-387-340+308 # 101 OA
4841-387-2585+308 # 2177 OW 
3742-340-2585+308 # 1125 OWA

387-308 # 79 OA & OW
340-308 # 32 OA & OWA
2585-308 # 2277 OWA & OW


# Note that the names are important and have to be specific to line up the diagram
fit1 <- euler(c("OA" = 101, "OW" = 2177, "OWA" = 1125, "OA&OW" = 79, "OA&OWA" = 32, "OW&OWA" = 2277, "OA&OW&OWA" = 308))


plot(fit1,  lty = 1:3, quantities = TRUE)
# lty changes the lines

plot(fit1, quantities = TRUE, fill = "transparent",
     lty = 1:3,
     labels = list(font = 4))


#cross check
2177+2277+308+79 # 4841, total OW
1125+2277+308+32 # 3742, total OWA
101+32+79+308    # 520, total OA

```



## Transcriptomics #6 {-}

### Objectives for today  {-}
  
  1. Transfer a data file that includes additional trait information.
2. Work step by step through correlation (or association) analyses using [WGCNA](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/Tutorials/index.html): install packages, import and filter the data, look for outliers, normalize, construct networks, explore module eigengenes, associate module eigengenes with traits, and visualize the data along the way!!!
  
  ### 1. Get the trait data {-}
  * **Use FileZilla to transfer `Ahud_trait_data.txt` to your machine. It can be found on `ecogen.uvm.edu:/data/project_data/RNAseq/traitdata`.**
  
  ### 2. Work through Weighted Gene Co-Expression Network Analysis (WGCNA)  {-}
  
  WGCNA is a useful tool for identifying correlated genes (modules) and testing if those modules (or eigengenes) are associated with specific traits of interest. Youll see from the link above that the WGCNA tutorials are out of data and difficult to followup. Many other people have made useful tutorials including ones on YouTube. One I used to refresh on WGCNA was by the cleverly named [Bioinformagician](https://www.youtube.com/watch?v=gYE59uEMXT4).

#### Get set up; Load the packages/libraries we will need  {-}

```{ }
## Set your working directory
setwd("YOUR/WORKING/DIRECTORY")

# Load the package
library(WGCNA);
# The following setting is important, do not omit.
options(stringsAsFactors = FALSE);

library(DESeq2)
library(ggplot2)

library(tidyverse)

# install.packages("remotes")
# remotes::install_github("kevinblighe/CorLevelPlot")

library(CorLevelPlot) 
library(gridExtra)

library(Rmisc) 
```

### A. Import the counts matrix and metadata and filter using DESeq2  {-}
```{ }
# 1. Import the counts matrix and metadata and filter using DESeq2

countsTable <- read.table("salmon.isoform.counts.matrix.filteredAssembly", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)

countsTableRound <- round(countsTable) # bc DESeq2 doesn't like decimals (and Salmon outputs data with decimals)
head(countsTableRound)

#import the sample description table
# conds <- read.delim("ahud_samples_R.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1)
# head(conds)

sample_metadata = read.table(file = "Ahud_trait_data.txt",header=T, row.names = 1)

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData=sample_metadata, 
                              design= ~ 1)

dim(dds)

# Filter out genes with too few reads - remove all genes with counts < 15 in more than 75% of samples, so ~28)
## suggested by WGCNA on RNAseq FAQ

dds <- dds[rowSums(counts(dds) >= 15) >= 28,]
nrow(dds) 
# [1] 25260, that have at least 15 reads (a.k.a counts) in 75% of the samples

# Run the DESeq model to test for differential gene expression
dds <- DESeq(dds)

```

### B. Look for outliers, particularly outlier samples {-}
```{ }
# 2. QC - outlier detection ------------------------------------------------
# detect outlier genes

gsg <- goodSamplesGenes(t(countsTable))
summary(gsg)
gsg$allOK

table(gsg$goodGenes)
table(gsg$goodSamples)


# detect outlier samples - hierarchical clustering - method 1
htree <- hclust(dist(t(countsTable)), method = "average")
plot(htree) 



# pca - method 2

pca <- prcomp(t(countsTable))
pca.dat <- pca$x

pca.var <- pca$sdev^2
pca.var.percent <- round(pca.var/sum(pca.var)*100, digits = 2)

pca.dat <- as.data.frame(pca.dat)

ggplot(pca.dat, aes(PC1, PC2)) +
  geom_point() +
  geom_text(label = rownames(pca.dat)) +
  labs(x = paste0('PC1: ', pca.var.percent[1], ' %'),
       y = paste0('PC2: ', pca.var.percent[2], ' %'))

```
Luckily, we dont have any, so we can move on. Otherwise we would have to filter our data to exclude those samples. 
 
### C. Normalize using DESeq2 {-}
```{ }
# 3. Normalization ----------------------------------------------------------------------

colData <- row.names(sample_metadata)

# making the rownames and column names identical
all(rownames(colData) %in% colnames(countsTableRound)) # to see if all samples are present in both
all(rownames(colData) == colnames(countsTableRound))  # to see if all samples are in the same order



# perform variance stabilization
dds_norm <- vst(dds)
# dds_norm <- vst(normalized_counts)

# get normalized counts
norm.counts <- assay(dds_norm) %>% 
  t()
```

### D. Check the quality of the data by sample clustering and visualization {-}
This is a critical step and one that can be played with iteratively to determine the "best" `soft_power` for clustering our genes in a biologically meaningful way.

```{ }
# 4. Network Construction  ---------------------------------------------------
# Choose a set of soft-thresholding powers
power <- c(c(1:10), seq(from = 12, to = 50, by = 2))

# Call the network topology analysis function; this step takes a couple minutes
sft <- pickSoftThreshold(norm.counts,
                         powerVector = power,
                         networkType = "signed",
                         verbose = 5)


sft.data <- sft$fitIndices

# visualization to pick power

a1 <- ggplot(sft.data, aes(Power, SFT.R.sq, label = Power)) +
  geom_point() +
  geom_text(nudge_y = 0.1) +
  geom_hline(yintercept = 0.8, color = 'red') +
  labs(x = 'Power', y = 'Scale free topology model fit, signed R^2') +
  theme_classic()


a2 <- ggplot(sft.data, aes(Power, mean.k., label = Power)) +
  geom_point() +
  geom_text(nudge_y = 0.1) +
  labs(x = 'Power', y = 'Mean Connectivity') +
  theme_classic()


grid.arrange(a1, a2, nrow = 2)
# based on this plot, choose a soft power to maximize R^2 (above 0.8) and minimize connectivity
# for these ahud data: 6-8; Higher R2 should yield more modules.


# convert matrix to numeric
norm.counts[] <- sapply(norm.counts, as.numeric)

soft_power <- 6
temp_cor <- cor
cor <- WGCNA::cor # use the 'cor' function from the WGCNA package


# this step also takes a few minutes; ideally your maxBlockSize is larger than your number of genes to run the memory-intensive network construction all at once.
bwnet <- blockwiseModules(norm.counts,
                          maxBlockSize = 26000,
                          minModuleSize = 30, 
                          reassignThreshold=0,
                          TOMType = "signed",
                          power = soft_power,
                          mergeCutHeight = 0.25,
                          numericLabels = F,
                          randomSeed = 1234,
                          verbose = 3)

# TOMtype (Topological Overlap Matrix type) parameter - unsigned - doesn't consider positive/negative co-expression
# signed - when you want to consider the direction of co-expression interaction, e.g., activating or inhibiting
# WGCNA often uses a dendrogram-based approach to identify modules. The choice of the 
# height cut in the dendrogram can determine the number of modules. Selecting a higher
# cut height results in fewer, larger modules, while a lower cut height leads to more, 
# smaller modules.

cor <- temp_cor

```

### E. Explore the eigengenes {-}

```{ }
# 5. Module Eigengenes ---------------------------------------------------------
module_eigengenes <- bwnet$MEs

head(module_eigengenes)


# get number of genes for each module
table(bwnet$colors)

# Plot the dendrogram and the module colors before and after merging underneath
plotDendroAndColors(bwnet$dendrograms[[1]], cbind(bwnet$unmergedColors, bwnet$colors),
                    c("unmerged", "merged"),
                    dendroLabels = FALSE,
                    addGuide = TRUE,
                    hang= 0.03,
                    guideHang = 0.05)

# grey module = all genes that doesn't fall into other modules were assigned to the grey module
# with higher soft power, more genes fall into the grey module

```


### F. Associate modules with traits {-}

```{ }
# 6A. Relate modules to traits --------------------------------------------------
# module trait associations

traits <- sample_metadata[, c(5,8,11,14,17)]


# Define numbers of genes and samples
nSamples <- nrow(norm.counts)
nGenes <- ncol(norm.counts)


module.trait.corr <- cor(module_eigengenes, traits, use = 'p')
module.trait.corr.pvals <- corPvalueStudent(module.trait.corr, nSamples)



# visualize module-trait association as a heatmap

heatmap.data <- merge(module_eigengenes, traits, by = 'row.names')

head(heatmap.data)

heatmap.data <- heatmap.data %>% 
  column_to_rownames(var = 'Row.names')


names(heatmap.data)

CorLevelPlot(heatmap.data,
             x = names(heatmap.data)[12:16],
             y = names(heatmap.data)[1:11],
             col = c("blue1", "skyblue", "white", "pink", "red"))



module.gene.mapping <- as.data.frame(bwnet$colors) # assigns module membership to each gene
module.gene.mapping %>% 
  filter(`bwnet$colors` == 'yellow') %>% 
  rownames()

groups <- sample_metadata[,c(3,1)]
module_eigengene.metadata <- merge(groups, heatmap.data, by = 'row.names')

#Create a summary data frame of a particular module eigengene information
MEyellow_summary <- summarySE(module_eigengene.metadata, measurevar="MEyellow", groupvars=c("Generation","treatment"))

#Plot a line interaction plot of a particular module eigengene
ggplot(MEyellow_summary, aes(x=as.factor(Generation), y=MEyellow, color=treatment, fill = treatment, shape = treatment)) +
  geom_point(size=5, stroke = 1.5 ) +
  geom_errorbar(aes(ymin=MEyellow-se, ymax=MEyellow+se), width=.15) +
  geom_line(aes(color=treatment, group=treatment, linetype = treatment)) +
  scale_color_manual(values = c('#6699CC',"#F2AD00","#00A08A", "#CC3333")) +
  scale_shape_manual(values=c(21,22,23,24), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A", "#CC3333"), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  xlab("Generation") +
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(panel.grid.minor.y = element_blank(), legend.position = "none", plot.margin = margin(0,6,0,6))

```

### Plot individual genes to see how eigengene patterns match gene expression patterns  {-}

```{ }

# 6B. Intramodular analysis: Identifying driver genes ---------------

# Get top hub genes (genes with highest connectivity in the network)
hubs  <-  chooseTopHubInEachModule(norm.counts, bwnet$colors, type = "signed", omitColors = "")
hubs

### Plot Individual genes  to check! ### 

d <-plotCounts(dds, gene="TRINITY_DN11845_c0_g1::TRINITY_DN11845_c0_g1_i9::g.36434::m.36434", intgroup = (c("treatment","Generation")), returnData=TRUE)
d_summary <- summarySE(d, measurevar = "count", groupvars=c("Generation","treatment"))

ggplot(d_summary, aes(x=Generation, y=count, color=treatment, fill = treatment, shape = treatment)) +
  geom_point(size=5, stroke = 1.5 ) +
  geom_errorbar(aes(ymin=count-se, ymax=count+se), width=.15) +
  geom_line(aes(color=treatment, group=treatment, linetype = treatment)) +
  scale_color_manual(values = c('#6699CC',"#F2AD00","#00A08A", "#CC3333")) +
  scale_shape_manual(values=c(21,22,23,24), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A", "#CC3333"), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  xlab("Generation") +
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(panel.grid.minor.y = element_blank(), legend.position = "none", plot.margin = margin(0,6,0,6))



# Calculate the module membership and the associated p-values

# The module membership/intramodular connectivity is calculated as the correlation of the eigengene and the gene expression profile. 
# This quantifies the similarity of all genes on the array to every module.

module.membership.measure <- cor(module_eigengenes, norm.counts, use = 'p')
module.membership.measure.pvals <- corPvalueStudent(module.membership.measure, nSamples)


module.membership.measure.pvals[1:10,1:10]

```

### Make a heat map of gene expressions within modules {-}

```{ }
# Make a heat map of gene expressions within modules.
# Use the norm.counts matrix, subset based on module membership
t_norm.counts <- norm.counts %>% t() %>% as.data.frame()

# Yellow module
yellow_transcripts <- module.gene.mapping %>% 
  filter(`bwnet$colors` == 'yellow') %>% 
  rownames()

t_norm.counts_yellow <- t_norm.counts %>% 
  filter(row.names(t_norm.counts) %in% yellow_transcripts)

t_norm.counts_yellow <- t_norm.counts_yellow - rowMeans(t_norm.counts_yellow)
df <- as.data.frame(colData(dds)[,c("eneration","treatment")])

#blue to yellow color scheme
paletteLength <- 50
myColor <- colorRampPalette(c("dodgerblue", "black", "yellow"))(paletteLength)
myBreaks <- c(seq(min(t_norm.counts_yellow), 0, length.out=ceiling(paletteLength/2) + 1), 
              seq(max(t_norm.counts_yellow)/paletteLength, max(t_norm.counts_yellow), length.out=floor(paletteLength/2)))
pheatmap(t_norm.counts_yellow, color = myColor, breaks = myBreaks,
         show_colnames = FALSE, show_rownames = FALSE, annotation_col = df, main = "Yellow")


```







## Transcriptomics #7 {-}

### Objectives for today  {-}
  
  1. Discuss "what's missing" in hands-on coding sessions - to better set you up for the future (so youre not surprised)
2. Continue working through the correlation (or association) analyses using [WGCNA](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/Tutorials/index.html): associate module eigengenes with traits and visualize the data!
3. Perform Gene Ontology (GO) functional enrichment analyses using [GOMWU](https://github.com/z0on/GO_MWU/tree/master) with our DESeq2 results.

### 1. "What's missing" in hands-on coding sessions {-}
1. Choosing which programs to use. But youre getting a sense for this through noting the programs used and carefully reading and discussing our discussion papers.
2. **Installing the programs for use. **
3. Troubleshooting code (though we certainly do get to experience some of that in class).
4. **Moving from one file type to the next, i.e., making your output from one program your input for the next. **
5. Exploring parameter space, e.g., all the micro-decisions in each program. This may be the Homework #2 assignment!

The bolded points above take a lot of time and troubleshooting "behind the scenes." We share this with you just for a bit of a "reality check" so youre not surprised when you go to do these sorts of analyses in your dissertation research. Remember: Google, ChatGPT, your instructors and committee members, and your friends are your friends!

### 2. Continue to work through Weighted Gene Co-Expression Network Analysis (WGCNA)  {-}

Recall that WGCNA is a useful tool for identifying correlated genes (modules) and testing if those modules (or eigengenes) are associated with specific traits of interest. 

Pick up where you left off in your previous R script.

* If you weren't able to make the blockwise modules (bwnet) object, I've put one on the class server that you can transfer to your machine and load. `/data/project_data/RNAseq/analyses/WGCNA/`

```{ }
# What I did to save it:
saveRDS(bwnet, file = "bwnet.rds")

# To load the object
bwnet <- readRDS("bwnet.rds")
```

### Picking up the code from where we left off last class (you can continue to add to your previous script)  {-}


### E. Explore the eigengenes {-}

```{}
# 5. Module Eigengenes ---------------------------------------------------------
module_eigengenes <- bwnet$MEs

head(module_eigengenes)


# get number of genes for each module
table(bwnet$colors)

# Plot the dendrogram and the module colors before and after merging underneath
plotDendroAndColors(bwnet$dendrograms[[1]], cbind(bwnet$unmergedColors, bwnet$colors),
                    c("unmerged", "merged"),
                    dendroLabels = FALSE,
                    addGuide = TRUE,
                    hang= 0.03,
                    guideHang = 0.05)

# grey module = all genes that doesn't fall into other modules were assigned to the grey module
# with higher soft power, more genes fall into the grey module

```


### F. Associate modules with traits  {-}

```{}
# 6A. Relate modules to traits --------------------------------------------------
# module trait associations

traits <- sample_metadata[, c(5,8,11,14,17)]


# Define numbers of genes and samples
nSamples <- nrow(norm.counts)
nGenes <- ncol(norm.counts)


module.trait.corr <- cor(module_eigengenes, traits, use = 'p')
module.trait.corr.pvals <- corPvalueStudent(module.trait.corr, nSamples)



# visualize module-trait association as a heatmap

heatmap.data <- merge(module_eigengenes, traits, by = 'row.names')

head(heatmap.data)

heatmap.data <- heatmap.data %>% 
  column_to_rownames(var = 'Row.names')


names(heatmap.data)

CorLevelPlot(heatmap.data,
             x = names(heatmap.data)[12:16],
             y = names(heatmap.data)[1:11],
             col = c("blue1", "skyblue", "white", "pink", "red"))



module.gene.mapping <- as.data.frame(bwnet$colors) # assigns module membership to each gene
module.gene.mapping %>% 
  filter(`bwnet$colors` == 'yellow') %>% 
  rownames()

groups <- sample_metadata[,c(3,1)]
module_eigengene.metadata <- merge(groups, heatmap.data, by = 'row.names')

#Create a summary data frame of a particular module eigengene information
MEyellow_summary <- summarySE(module_eigengene.metadata, measurevar="MEyellow", groupvars=c("Generation","treatment"))

#Plot a line interaction plot of a particular module eigengene
ggplot(MEyellow_summary, aes(x=as.factor(Generation), y=MEyellow, color=treatment, fill = treatment, shape = treatment)) +
  geom_point(size=5, stroke = 1.5 ) +
  geom_errorbar(aes(ymin=MEyellow-se, ymax=MEyellow+se), width=.15) +
  geom_line(aes(color=treatment, group=treatment, linetype = treatment)) +
  scale_color_manual(values = c('#6699CC',"#F2AD00","#00A08A", "#CC3333")) +
scale_shape_manual(values=c(21,22,23,24), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A", "#CC3333"), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  xlab("Generation") +
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
  theme(text = element_text(size = 20)) +
  theme(panel.grid.minor.y = element_blank(), legend.position = "none", plot.margin = margin(0,6,0,6))

```

### Plot individual genes to see how eigengene patterns match gene expression patterns {-}

```{}

# 6B. Intramodular analysis: Identifying driver genes ---------------

# Get top hub genes (genes with highest connectivity in the network)
hubs  <-  chooseTopHubInEachModule(norm.counts, bwnet$colors, type = "signed", omitColors = "")
hubs

### Plot Individual genes  to check! ### 

d <-plotCounts(dds, gene="TRINITY_DN11845_c0_g1::TRINITY_DN11845_c0_g1_i9::g.36434::m.36434", intgroup = (c("treatment","Generation")), returnData=TRUE)
d_summary <- summarySE(d, measurevar = "count", groupvars=c("Generation","treatment"))

ggplot(d_summary, aes(x=Generation, y=count, color=treatment, fill = treatment, shape = treatment)) +
  geom_point(size=5, stroke = 1.5 ) +
  geom_errorbar(aes(ymin=count-se, ymax=count+se), width=.15) +
  geom_line(aes(color=treatment, group=treatment, linetype = treatment)) +
  scale_color_manual(values = c('#6699CC',"#F2AD00","#00A08A", "#CC3333")) +
  scale_shape_manual(values=c(21,22,23,24), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  scale_fill_manual(values=c('#6699CC',"#F2AD00","#00A08A", "#CC3333"), labels = c("Ambient", "Acidification","Warming", "OWA"))+
  xlab("Generation") +
  theme_bw() +
  theme(legend.position = "none") +
theme(panel.border = element_rect(color = "black", fill = NA, size = 4))+
theme(text = element_text(size = 20)) +
theme(panel.grid.minor.y = element_blank(), legend.position = "none", plot.margin = margin(0,6,0,6))



# Calculate the module membership and the associated p-values

# The module membership/intramodular connectivity is calculated as the correlation of the eigengene and the gene expression profile. 
# This quantifies the similarity of all genes on the array to every module.

module.membership.measure <- cor(module_eigengenes, norm.counts, use = 'p')
module.membership.measure.pvals <- corPvalueStudent(module.membership.measure, nSamples)


module.membership.measure.pvals[1:10,1:10]

```

### Make a heat map of gene expressions within modules {-}

```{}
# Make a heat map of gene expressions within modules.
# Use the norm.counts matrix, subset based on module membership
t_norm.counts <- norm.counts %>% t() %>% as.data.frame()

# Yellow module
yellow_transcripts <- module.gene.mapping %>% 
  filter(`bwnet$colors` == 'yellow') %>% 
  rownames()

t_norm.counts_yellow <- t_norm.counts %>% 
filter(row.names(t_norm.counts) %in% yellow_transcripts)

t_norm.counts_yellow <- t_norm.counts_yellow - rowMeans(t_norm.counts_yellow)
df <- as.data.frame(colData(dds)[,c("eneration","treatment")])

#blue to yellow color scheme
paletteLength <- 50
myColor <- colorRampPalette(c("dodgerblue", "black", "yellow"))(paletteLength)
myBreaks <- c(seq(min(t_norm.counts_yellow), 0, length.out=ceiling(paletteLength/2) + 1), 
seq(max(t_norm.counts_yellow)/paletteLength, max(t_norm.counts_yellow), length.out=floor(paletteLength/2)))
pheatmap(t_norm.counts_yellow, color = myColor, breaks = myBreaks,
show_colnames = FALSE, show_rownames = FALSE, annotation_col = df, main = "Yellow")
 ```
### 3. Perform Gene Ontology (GO) functional enrichment analyses using [GOMWU](https://github.com/z0on/GO_MWU/tree/master) with our DESeq2 results. {-}
There are two general types of functional enrichment analyses: those that use a DGE significance cutoff (e.g., dividing genes into differentially expressed, padj < 0.05, and not and running a Fishers exact test) and those that use the whole distribution (MWU rank-based correlation or Kolmorgorov-Smirnov test). I generally prefer using the whole distribution, more data, and doesnt depend on an arbitrary cutoff.
There are many ways to characterize the function of genes. One of the commonly used databases and classification systems is that of [Gene Ontology (GO)](https://geneontology.org/docs/ontology-documentation/), which is a knowledgebase that "provides a computational representation of our current scientific knowledge about the functions of genes (or, more properly, the protein and non-coding RNA molecules produced by genes) from many different organisms, from humans to bacteria." There are three types of GO categories, those that describe: Molecular Function, Cellular Component, and Biological Process. I find the Biological Process to be most informative.
There are also many ways to test for the non-random distribution of genes with specific functions in a list of scores, such as p-values, logFoldChange, FST, etc. The package we will use for this tutorial is called [GO Mann-Whitney U or GOMWU](https://github.com/z0on/GO_MWU/tree/master) and was created by Dr. Misha Matz of UT Austin.
GOMWU requires all of the following to be in one directory: 
* 2 user provided files: 
* 1) a table of measure of interest: two columns of comma-separated values: gene id, continuous measure of change such as log(fold-change).  
* 2) table of GO annotations for your sequences: two-column (gene id - GO terms), tab-delimited, one line per gene, multiple GO terms separated by semicolon. 
* a set of scripts: GO_MWU.R, gomwu_a.pl, gomwu_b.pl, gomwu.functions.R (we only need to edit the first one)
* a GO database, hierarchy file (go.obo, http://www.geneontology.org/GO.downloads.ontology.shtml) * Ive already downloaded the most recent version for us to use.

The scripts, the GO database, and the annotations table are already assembled and can be found in `/data/project_data/RNAseq/analyses/GOMWU`. You can copy all of those files to your personal machine or you can move the files to your home directory on the server and run the code in R on our class server. The only part we need to make is the measures of interest based on DESeq2 results. 

### DESeq2 to GOMWU measure/scores files  {-}

We want to save the results files for our contrasts of interest for which wed like to test for functional enrichment. Well focus on F0 contrasts of AM vs OW, OWA, and OA. Well run three tests focused on BP and compare enrichment across these three contrasts. Well use LFC as our metric, but we could also choose p-value or stat.

```{}
## Set your working directory
setwd("~/github/hudsonica")

## Import the libraries that we're likely to need in this session
library(DESeq2)

# Try with new counts table from filtered transcriptome assembly
countsTable <- read.table("salmon.isoform.counts.matrix.filteredAssembly", header=TRUE, row.names=1)


head(countsTable)
dim(countsTable)
#[1] 130580     38 - genes
# [1] 349516     38 - isoforms
# [1] 67916    38 - filtered assembly

countsTableRound <- round(countsTable) # bc DESeq2 doesn't like decimals (and Salmon outputs data with decimals)
head(countsTableRound)

#import the sample discription table
conds <- read.delim("ahud_samples_R.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1)
head(conds)

#################### MODEL NUMBER 2 - subset to focus on effect of treatment for each generation

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData=conds, 
                              design= ~ treatment)

dim(dds)
# [1] 130580     38

# Filter 
dds <- dds[rowSums(counts(dds) >= 30) >= 28,]
nrow(dds) 

# Subset the DESeqDataSet to the specific level of the "generation" factor
dds_F0 <- subset(dds, select = generation == 'F0')
dim(dds_F0)

# Perform DESeq2 analysis on the subset
dds_F0 <- DESeq(dds_F0)

resultsNames(dds_F0)
# [1] "Intercept"           "treatment_OA_vs_AM"  "treatment_OW_vs_AM"  "treatment_OWA_vs_AM"

res_F0_OWvAM <- results(dds_F0, name="treatment_OW_vs_AM", alpha=0.05)

res_F0_OWvAM <- res_F0_OWvAM[order(res_F0_OWvAM$padj),]
head(res_F0_OWvAM) 

summary(res_F0_OWvAM)


res_F0_OWAvAM <- results(dds_F0, name="treatment_OWA_vs_AM", alpha=0.05)

res_F0_OWAvAM <- res_F0_OWAvAM[order(res_F0_OWAvAM$padj),]
head(res_F0_OWAvAM) 

summary(res_F0_OWAvAM)


res_F0_OAvAM <- results(dds_F0, name="treatment_OA_vs_AM", alpha=0.05)

res_F0_OAvAM <- res_F0_OAvAM[order(res_F0_OAvAM$padj),]
head(res_F0_OAvAM)

summary(res_F0_OAvAM)


################## Save all the results as csv to go into GOMWU

library(tidyr)

# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OWvAM_df <- data.frame(transcriptID = rownames(res_F0_OWvAM), res_F0_OWvAM)

# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OWvAM_df <- separate(res_F0_OWvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE) 

# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OWvAM_df$transcriptID_trim <- paste(res_F0_OWvAM_df$part1, res_F0_OWvAM_df$part2, sep = "::")

# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OWvAM_df <- res_F0_OWvAM_df[, !(names(res_F0_OWvAM_df) %in% c("part1", "part2", "part3", "rest"))]

write.table(res_F0_OWvAM_df, file = "res_F0_OWvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records

# Select the two columns we want to save for the GOMWU analysis
selected_columns_OW <- res_F0_OWvAM_df[c("transcriptID_trim", "log2FoldChange")]

# Save the selected columns as a CSV file
write.csv(selected_columns_OW, file = "res_F0_OWvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU


############ Now for OWA

# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OWAvAM_df <- data.frame(transcriptID = rownames(res_F0_OWAvAM), res_F0_OWAvAM)

# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OWAvAM_df <- separate(res_F0_OWAvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE) 

# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OWAvAM_df$transcriptID_trim <- paste(res_F0_OWAvAM_df$part1, res_F0_OWAvAM_df$part2, sep = "::")

# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OWAvAM_df <- res_F0_OWAvAM_df[, !(names(res_F0_OWAvAM_df) %in% c("part1", "part2", "part3", "rest"))]
write.table(res_F0_OWAvAM_df, file = "res_F0_OWAvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records

# Select the two columns we want to save for the GOMWU analysis
selected_columns_OWA <- res_F0_OWAvAM_df[c("transcriptID_trim", "log2FoldChange")]

# Save the selected columns as a CSV file
write.csv(selected_columns_OWA, file = "res_F0_OWAvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU



############ Now for OA

# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OAvAM_df <- data.frame(transcriptID = rownames(res_F0_OAvAM), res_F0_OAvAM)

# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OAvAM_df <- separate(res_F0_OAvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE) 

# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OAvAM_df$transcriptID_trim <- paste(res_F0_OAvAM_df$part1, res_F0_OAvAM_df$part2, sep = "::")

# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OAvAM_df <- res_F0_OAvAM_df[, !(names(res_F0_OAvAM_df) %in% c("part1", "part2", "part3", "rest"))]
write.table(res_F0_OAvAM_df, file = "res_F0_OAvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records

# Select the two columns we want to save for the GOMWU analysis
selected_columns_OA <- res_F0_OAvAM_df[c("transcriptID_trim", "log2FoldChange")]

# Save the selected columns as a CSV file
write.csv(selected_columns_OA, file = "res_F0_OAvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU
```

Make sure your .csv, measures files are in the same directory with your GOMWU scripts.

### Edit the `GO_MWU.R` script and run the program at least three times, one for each contrast!  {-}

Open the `GO_MWU.R` in R studio. There we will edit the two input filenames and confirm the sizes of GO categories to which to limit the analysis (e.g., between 10 and 500 gene members in a GO category). Let's discuss why to do this!




## Transcriptomics #8 {-}

### Objectives for today   {-}
  
  1. Review Homework Assignment #2.

2. Perform Gene Ontology (GO) functional enrichment analyses using [GOMWU](https://github.com/z0on/GO_MWU/tree/master) with our DESeq2 results.

1. Homework 2 is live! 
Find details [here](https://pespenilab.github.io/Ecological-Genomics/Fall2023/assignments/Homework2.html). This assignment is a choose your own adventure (as I mentioned it would be in class). You’ll find 5 different options with specific tasks associated with each (or a 6th that you choose and float by me). Please engage with the options and let me know your choice by Friday. 

### 2. Perform Gene Ontology (GO) functional enrichment analyses using [GOMWU](https://github.com/z0on/GO_MWU/tree/master) with our DESeq2 results. {-}

There are two general types of functional enrichment analyses: those that use a DGE significance cutoff (e.g., dividing genes into differentially expressed, padj < 0.05, and not and running a Fishers exact test) and those that use the whole distribution (MWU rank-based correlation or Kolmorgorov-Smirnov test). I generally prefer using the whole distribution, more data, and doesnt depend on an arbitrary cutoff.

There are many ways to characterize the function of genes. One of the commonly used databases and classification systems is that of [Gene Ontology (GO)](https://geneontology.org/docs/ontology-documentation/), which is a knowledgebase that "provides a computational representation of our current scientific knowledge about the functions of genes (or, more properly, the protein and non-coding RNA molecules produced by genes) from many different organisms, from humans to bacteria." There are three types of GO categories, those that describe: Molecular Function, Cellular Component, and Biological Process. I find the Biological Process to be most informative.

There are also many ways to test for the non-random distribution of genes with specific functions in a list of scores, such as p-values, logFoldChange, FST, etc. The package we will use for this tutorial is called [GO Mann-Whitney U or GOMWU](https://github.com/z0on/GO_MWU/tree/master) and was created by Dr. Misha Matz of UT Austin.

GOMWU requires all of the following to be in one directory: 
  
  * 2 user provided files: 
  * 1) a table of measure of interest: two columns of comma-separated values: gene id, continuous measure of change such as log(fold-change).  
* 2) table of GO annotations for your sequences: two-column (gene id - GO terms), tab-delimited, one line per gene, multiple GO terms separated by semicolon. 
* a set of scripts: GO_MWU.R, gomwu_a.pl, gomwu_b.pl, gomwu.functions.R (we only need to edit the first one)
* a GO database, hierarchy file (go.obo, http://www.geneontology.org/GO.downloads.ontology.shtml) * Ive already downloaded the most recent version for us to use.

The scripts, the GO database, and the annotations table are already assembled and can be found in `/data/project_data/RNAseq/analyses/GOMWU`. You can copy all of those files to your personal machine or you can move the files to your home directory on the server and run the code in R on our class server. The only part we need to make is the measures of interest based on DESeq2 results. 

### DESeq2 to GOMWU measure/scores files  {-}

We want to save the results files for our contrasts of interest for which we'd like to test for functional enrichment. We'll focus on F0 contrasts of AM vs OW, OWA, and OA. We'll run three tests focused on BP and compare enrichment across these three contrasts. We'll use LFC as our metric, but we could also choose p-value or stat.

```{ }
## Set your working directory
setwd("~/github/hudsonica")

## Import the libraries that we're likely to need in this session
library(DESeq2)

# Try with new counts table from filtered transcriptome assembly
countsTable <- read.table("salmon.isoform.counts.matrix.filteredAssembly", header=TRUE, row.names=1)


head(countsTable)
dim(countsTable)
#[1] 130580     38 - genes
# [1] 349516     38 - isoforms
# [1] 67916    38 - filtered assembly

countsTableRound <- round(countsTable) # bc DESeq2 doesn't like decimals (and Salmon outputs data with decimals)
head(countsTableRound)

#import the sample discription table
conds <- read.delim("ahud_samples_R.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1)
head(conds)

#################### MODEL NUMBER 2 - subset to focus on effect of treatment for each generation

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData=conds, 
                              design= ~ treatment)

dim(dds)
# [1] 130580     38

# Filter 
dds <- dds[rowSums(counts(dds) >= 30) >= 28,]
nrow(dds) 

# Subset the DESeqDataSet to the specific level of the "generation" factor
dds_F0 <- subset(dds, select = generation == 'F0')
dim(dds_F0)

# Perform DESeq2 analysis on the subset
dds_F0 <- DESeq(dds_F0)

resultsNames(dds_F0)
# [1] "Intercept"           "treatment_OA_vs_AM"  "treatment_OW_vs_AM"  "treatment_OWA_vs_AM"

res_F0_OWvAM <- results(dds_F0, name="treatment_OW_vs_AM", alpha=0.05)

res_F0_OWvAM <- res_F0_OWvAM[order(res_F0_OWvAM$padj),]
head(res_F0_OWvAM) 

summary(res_F0_OWvAM)


res_F0_OWAvAM <- results(dds_F0, name="treatment_OWA_vs_AM", alpha=0.05)

res_F0_OWAvAM <- res_F0_OWAvAM[order(res_F0_OWAvAM$padj),]
head(res_F0_OWAvAM) 

summary(res_F0_OWAvAM)


res_F0_OAvAM <- results(dds_F0, name="treatment_OA_vs_AM", alpha=0.05)

res_F0_OAvAM <- res_F0_OAvAM[order(res_F0_OAvAM$padj),]
head(res_F0_OAvAM)

summary(res_F0_OAvAM)


################## Save all the results as csv to go into GOMWU

library(tidyr)

# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OWvAM_df <- data.frame(transcriptID = rownames(res_F0_OWvAM), res_F0_OWvAM)

# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OWvAM_df <- separate(res_F0_OWvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE) 

# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OWvAM_df$transcriptID_trim <- paste(res_F0_OWvAM_df$part1, res_F0_OWvAM_df$part2, sep = "::")

# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OWvAM_df <- res_F0_OWvAM_df[, !(names(res_F0_OWvAM_df) %in% c("part1", "part2", "part3", "rest"))]

write.table(res_F0_OWvAM_df, file = "res_F0_OWvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records

# Select the two columns we want to save for the GOMWU analysis
selected_columns_OW <- res_F0_OWvAM_df[c("transcriptID_trim", "log2FoldChange")]

# Save the selected columns as a CSV file
write.csv(selected_columns_OW, file = "res_F0_OWvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU


############ Now for OWA

# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OWAvAM_df <- data.frame(transcriptID = rownames(res_F0_OWAvAM), res_F0_OWAvAM)

# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OWAvAM_df <- separate(res_F0_OWAvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE) 

# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OWAvAM_df$transcriptID_trim <- paste(res_F0_OWAvAM_df$part1, res_F0_OWAvAM_df$part2, sep = "::")

# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OWAvAM_df <- res_F0_OWAvAM_df[, !(names(res_F0_OWAvAM_df) %in% c("part1", "part2", "part3", "rest"))]
write.table(res_F0_OWAvAM_df, file = "res_F0_OWAvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records

# Select the two columns we want to save for the GOMWU analysis
selected_columns_OWA <- res_F0_OWAvAM_df[c("transcriptID_trim", "log2FoldChange")]

# Save the selected columns as a CSV file
write.csv(selected_columns_OWA, file = "res_F0_OWAvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU



############ Now for OA

# Make the rownames a separate column called transcriptID and make it all a dataframe
res_F0_OAvAM_df <- data.frame(transcriptID = rownames(res_F0_OAvAM), res_F0_OAvAM)

# Split the "transcriptID" column by double colons and create new columns of the parts
res_F0_OAvAM_df <- separate(res_F0_OAvAM_df, transcriptID, into = c("part1", "part2", "part3", "rest"), sep = "::", remove = FALSE) 

# Create a new column by concatenating "part1" and "part2" with double colons in between
res_F0_OAvAM_df$transcriptID_trim <- paste(res_F0_OAvAM_df$part1, res_F0_OAvAM_df$part2, sep = "::")

# Optional: Remove the "part1" and "part2" columns from the dataframe
res_F0_OAvAM_df <- res_F0_OAvAM_df[, !(names(res_F0_OAvAM_df) %in% c("part1", "part2", "part3", "rest"))]
write.table(res_F0_OAvAM_df, file = "res_F0_OAvAM.txt", sep = "\t", row.names = F)   # saves the full original for the records

# Select the two columns we want to save for the GOMWU analysis
selected_columns_OA <- res_F0_OAvAM_df[c("transcriptID_trim", "log2FoldChange")]

# Save the selected columns as a CSV file
write.csv(selected_columns_OA, file = "res_F0_OAvAM_LFC.csv", quote = FALSE, row.names = F) # saves the selected columns for GOMWU
```

Make sure your .csv, measures files are in the same directory with your GOMWU scripts.

### Edit the `GO_MWU.R` script and run the program at least three times, one for each contrast!  {-}

Open the `GO_MWU.R` in R studio. There we will edit the two input filenames and confirm the sizes of GO categories to which to limit the analysis (e.g., between 10 and 500 gene members in a GO category). Lets discuss why to do this!

### Biological interpretation?  {-}
What do these results mean? How do enrichment results vary based on treatment contrast? Are there any reasons to be concerned about these results? What next steps would you want to take?